{
    "epochs": 5000,
    "layers": 6,
    "activations": {
        "layer1": "relu",
        "layer2": "relu",
        "layer3": "relu",
        "layer4": "relu",
        "layer5": "relu",
        "layer6": "sigmoid"
    },
    "neurons": {
        "layer1": 20,
        "layer2": 16,
        "layer3": 12,
        "layer4": 7,
        "layer5": 5,
        "layer6": 1
    },
    "learning_rate": 0.0075,
    "batch_size": 64,
    "threshold": 0.0019
}
