{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Model Training and Prediction\n",
    "## Introduction\n",
    "__\"Leverage SageMaker from Notebook --> Production Testing\"__\n",
    "\n",
    "---\n",
    "## 1 - Model Training\n",
    "### Permissions and Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import sagemaker\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from sagemaker.mxnet import MXNet\n",
    "from mxnet import gluon\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Confifuree SageMaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local repository for Numpy Arrays\n",
    "if not os.path.exists('/tmp/data'):\n",
    "    os.mkdir('/tmp/data')\n",
    "\n",
    "# Load the Training and Testing dataset\n",
    "dataset = h5py.File('datasets/datasets.h5', 'r')\n",
    "\n",
    "# Save the Dataset as Numpy Arrays\n",
    "np.save('/tmp/data/train_X.npy', np.array(dataset['train_set_x'][:]))\n",
    "np.save('/tmp/data/train_Y.npy', np.array(dataset['train_set_y'][:]))\n",
    "np.save('/tmp/data/test_X.npy', np.array(dataset['test_set_x'][:]))\n",
    "np.save('/tmp/data/test_Y.npy', np.array(dataset['test_set_y'][:]))\n",
    "\n",
    "# Upload the Training and Testing Data to S3\n",
    "inputs = sagemaker_session.upload_data(path='/tmp/data', key_prefix='training_input')\n",
    "bucket = inputs.split('/')[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a MXNet Estimator\n",
    "mxnet_estimator = MXNet(\n",
    "    'model.py',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    output_path='s3://'+bucket,\n",
    "    hyperparameters={\n",
    "        'epochs': 3000,\n",
    "        'optmizer': 'sgd',\n",
    "        'learning_rate': 0.0075,\n",
    "        'batch_size': 64\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Job name for current training run\n",
    "job_name = 'itsacat-train-v0-000'\n",
    "mxnet_estimator.fit(inputs, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## 2 - Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and uncompress output results from model training\n",
    "import tarfile, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#job_name = 'sagemaker-mxnet-py2-cpu-2018-04-02-21-15-16-687'\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file(job_name+'/output/output.tar.gz', '/tmp/output.tar.gz')\n",
    "tarfile.open('/tmp/output.tar.gz').extractall()\n",
    "with open('results.json') as j:\n",
    "    data = json.load(j)#, object_pairs_hook=OrderedDict)\n",
    "\n",
    "# Format data for plotting\n",
    "costs = []\n",
    "val_acc = []\n",
    "train_acc = []\n",
    "for key, value in sorted(data.iteritems(), key=lambda (k,v): (v, k)):\n",
    "    if 'epoch' in key:\n",
    "        for k, v in value.iteritems():\n",
    "            if k == 'cost':\n",
    "                costs.append(v)\n",
    "            elif k == 'val_acc':\n",
    "                val_acc.append(v)\n",
    "            elif k == 'train_acc':\n",
    "                train_acc.append(v)\n",
    "    elif 'Start' in key:\n",
    "        start = datetime.datetime.strptime(value, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    elif 'End' in key:\n",
    "        end = datetime.datetime.strptime(value, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "val_acc.reverse()\n",
    "val_acc = np.array(val_acc)\n",
    "train_acc.reverse()\n",
    "train_acc = np.array(train_acc)\n",
    "costs.reverse()\n",
    "costs = np.array(costs)\n",
    "delta = end - start\n",
    "print(\"Model Training Time: {} Minute(s)\".format(int(delta.total_seconds() / 60)))\n",
    "\n",
    "# Plot the Learning Curve\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0)\n",
    "plt.plot(costs)\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.ylabel('Cost / Accuracy')\n",
    "plt.xlabel('Epochs (in Hundreds)')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(['Cost', 'Training Accuracy', 'Validation Accuracy'])\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## 3 - Prediciton Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor = mxnet_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import transform\n",
    "%matplotlib inline\n",
    "\n",
    "# Get Classes\n",
    "tmp_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "classes = np.array(tmp_dataset['list_classes'][:])\n",
    "\n",
    "# Get Image files\n",
    "images = []\n",
    "for img_path in glob.glob('./images/*'):\n",
    "    images.append(mpimg.imread(img_path))\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(20.0,20.0))\n",
    "columns = 2\n",
    "for i, image in enumerate(images):\n",
    "    img = transform.resize(image, (64, 64), mode='constant').reshape((1, 64 * 64 * 3))\n",
    "    prediction = int(predictor.predict(img.tolist()))\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.title('Prediction = \\\"'+classes[prediction]+'\\\" picture.')\n",
    "    plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
