{
    "epochs": 2500,
    "layers": 4,
    "activations": {
        "layer1": "relu",
        "layer2": "relu",
        "layer3": "relu",
        "layer4": "sigmoid"
    },
    "neurons": {
        "layer1": 20,
        "layer2": 7,
        "layer3": 5,
        "layer4": 1
    },
    "learning_rate": 0.0075,
    "beta1": 0.9,
    "beta2": 0.9,
    "epsilon": 1e-8,
    "optimizer": "adam",
    "batch_size": 64
}
