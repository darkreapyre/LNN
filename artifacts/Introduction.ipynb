{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Neural Network Introduction\n",
    "## Architecture Overview\n",
    "\n",
    "The **“Serverless Neural Network”** is a solution for generating a Cloud Native image classifier, using idempotent Amazon Web Services ([AWS](https://aws.amazon.com/what-is-aws/)) Lambda Functions. The **\"SNN\"** is used to train a model to predict whether a particular image is a **“cat”** vs. **“non-cat”** and fits within an overall prediction pipeline as the model training process. The primary objective of using Lambda Functions as opposed to other services like **SageMaker** or dedicated Machine Leaning Frameworks like **MXNet **or ** TensorFlow**, is to remove the abstraction layer that inevitably perpetuates the concept that Neural Networks or Deep Learning is a “black box” architecture and thus somewhat difficult to understand.\n",
    "\n",
    "<img src=\"images/Prediction_Architecture.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center>**Machine Learning Pipeline**</center></caption><br>\n",
    "\n",
    "By simulating the individual Neurons and the mathematical functions they perform, it's easier to learn the exactly what each neuron is doing and how they contribute to optimizing (or “learning”) the overall hyper-parameters used for the final prediction model. Additionally, leveraging this framework for model training will hopefully provide a more in-depth understanding of how each neuron deals with the \"vectorized\" matrix calculations during Forward Propagation process **AND** the gradient derivative calculations during the Backward Propagation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neural Network Overview\n",
    "\n",
    "<img src=\"images/2layerNN_kiank.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network Model (shown above) can be summarized as:\n",
    "  \n",
    "**INPUT --> LINEAR/RELU --> LINEAR/SIGMOID --> OUTPUT**  \n",
    "\n",
    "- The **Input** is a $(64, 64, 3)$ image what is flattened to a vector $(12288, 1)$. See the **Data Overview** Section.\n",
    "- The corresponding vector: $[x_{0}, x_{1}, \\dots, x_{12287}]^T$ is then multiplied by the **weight matrix** $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
    "- The **bias** term is then added to take the **Relu** (non-linear activation) to get a vector of size $[a^{[1]}_0, a^{[2]}_1, \\dots, a^{[1]}_{n^{[1]}-1}]^T$.\n",
    "- The process is then repeated for the next layer, by taking the resulting vector and multiplying it by the weight matrix $W^{[2]}$ and then adding the intercept (**bias**).\n",
    "- Lastly, the **sigmoid** activation is applied to the result. If the result is greater then $0.5$, it is classified as a **Cat**.\n",
    "\n",
    "Therefore, the **Network Model Parameters** (*parameters.json*) for the above process are as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"epochs\": 10,\n",
    "    \"layers\": 2,\n",
    "    \"activations\": {\n",
    "        \"layer1\": \"relu\",\n",
    "        \"layer2\": \"sigmoid\"\n",
    "    },\n",
    "    \"neurons\": {\n",
    "        \"layer1\": 3,\n",
    "        \"layer2\": 1\n",
    "    },\n",
    "    \"learning_rate\": 0.0075,\n",
    "    \"batch_size\": 64,\n",
    "    \"threshold\": 0.0019\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Overview\n",
    "### Training and Test Datasets\n",
    "It is **very important** in Neural Network programming (without the use of a Deep Learning Framework), to have a full understanding of the dimensions of the input data as well as how the dimensions are transformed at each layer, therefore to build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat, the following cells explain the datsets.\n",
    "\n",
    "To train the Neural Network, we are provided with a dataset (`datasets.h5`) containing:\n",
    "- a training set of $m$ images containing cats and non-cats as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- a test set of $m$ images containing cats and non-cat sas well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- classes list for cat and non-cat images.\n",
    "\n",
    ">**Note:** The original dataset was comprised of two separate files, `test_catvnoncat.h5` and `train_catvnoncat.h5`. For the sake of this implementation a single file is needed to upload to the *S3 Bucket*, `datasets.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_classes\n",
      "test_set_x\n",
      "test_set_y\n",
      "train_set_x\n",
      "train_set_y\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "# Load main dataset that's stored locally\n",
    "dataset = h5py.File('datasets/datasets.h5', \"r\")\n",
    "\n",
    "# Get the names of the unique datsets\n",
    "datasetNames = [n for n in dataset.keys()]\n",
    "for n in datasetNames:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create numpy arrays of the various unique datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "classes = np.array(dataset[\"list_classes\"][:]) # the list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_orig dimensions: (209, 64, 64, 3)\n",
      "train_set_y_orig dimension: (209,)\n",
      "test_set_x_orig dimensions: (50, 64, 64, 3)\n",
      "test_set_y_orig dimensions: (50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaye the dimensions of each unique data set\n",
    "print(\"train_set_x_orig dimensions: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_y_orig dimension: \" + str(train_set_y_orig.shape))\n",
    "print(\"test_set_x_orig dimensions: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_y_orig dimensions: \" + str(test_set_y_orig.shape))\n",
    "test_set_y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, the image data (`train_set_x_orig` and `test_set_x_orig`) are 4-dimensional arrays consiting of $209$ training examoples (**m_train**) and $50$ testing images (**m_test**) respectively. Each image is in turn of *height*, *width* and *depth* (**R**ed, **G**reen **B**lue values) of $64 \\times 64 \\times 3$.\n",
    "\n",
    "Additionally, the dimension for the labels (`train_set_y_orig` and `test_set_y_orig`) only show a $209$ and $50$ column structure. So it is recommended when coding new networks, don't use data structures where the shape is $5$, or $n$, rank 1 array. Instead, this is set to, `(1, 209)` and `(1, 50)`, to make them a **row vector**, and in essence add another dimension to the `Numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create row vectors for the labels.\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_y dimensions: (1, 209)\n",
      "test_set_y dimensions: (1, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_set_y dimensions: \" + str(train_set_y.shape))\n",
    "print(\"test_set_y dimensions: \" + str(test_set_y.shape))\n",
    "test_set_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Now that the additional dimension has been added to the label data, we can note the additional \"[[ ]]\" when displaying the array.\n",
    "\n",
    "Next we can see the label data and view the corresponding image, in this case, $index = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], and therefore it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfkTECiJO/IiIFUSc+BERK4j/H6VB4THAcFMnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0fc8eba780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Example of a cat picture\n",
    "index = 2\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \\\n",
    "       \", and therefore it's a '\" + \\\n",
    "       classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \\\n",
    "       \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE:** The `np.squeeze()` method extracts the \" inner dimension\" of the array, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_y[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_set_y[:, index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** The \"[ ]\" has been removed.\n",
    "\n",
    "### Data Preprocessing\n",
    "The final model analysis (see **Analysis** Notebook)is expecting a *training* set and a *test* set represented by a numpy array of shape (no. pixels $\\times$ no. pixels $\\times$ depth, data set size) respectively. In turn, the model is expecting the training set and test set labels represented as a numpy array (vector) of shape (1, data set size) respectively.\n",
    "\n",
    ">**Note:** It is not determined as yet wether the \"vectorization\" of the images should be performed by the `TrainerLambda` to set up the inputs for *Layer 0*. For the sake of Version 1.0, the preprocessing of the input data will be performed by `launch.py` as various helper functions.\n",
    "\n",
    "#### Vectorize\n",
    "The images are represented by a 3D array of shape $(length, height, depth = 3)$. However, when an image is read as the input of an algorithm it is converted to a vector of shape $(length*height*3, 1)$. In other words, it is \"unrolled\", \"flattened\" or \"reshaped\" from a 3D array into a 1D vector as can be seen below.\n",
    "\n",
    "<img src=\"images/vectorization.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells explains of this process using the `train_set_x_orig` numpy array. The end result for the input to the model is a is a numpy array where where each column represents a flattened image in a matrix with all the input features (images) being a column, $209$ for the training set and $50$ for the test set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (209, 64, 64, 3)\n",
      "Flattened shape: (209, 12288)\n",
      "Transpose: (12288, 209)\n"
     ]
    }
   ],
   "source": [
    "# Copy of origional training set\n",
    "orig = train_set_x_orig\n",
    "print(\"Original shape: \" + str(orig.shape))\n",
    "\n",
    "# \"vectorize\" or flatten out the array into an 1D vector\n",
    "flatten = orig.reshape(orig.shape[0], -1)\n",
    "print(\"Flattened shape: \"+ str(flatten.shape))\n",
    "\n",
    "# Transpose into a colums\n",
    "flatten_T = flatten.T\n",
    "print(\"Transpose: \" + str(flatten_T.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For further intuition of what the above code is doing, the following shows a more \"manual\", alternate way.\n",
    "\n",
    "#### Standardize\n",
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from $0$ to $255$. One common preprocessing step in machine learning is to subtract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by $255$ (the maximum value of a pixel channel). \n",
    "\n",
    ">**Note:** During the training of the model, the weights are multiplied and biases added to the initial inputs in order to observe neuron activations. Then it will backward propagate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (12288, 209)\n",
      "sample value: 0.266666666667\n"
     ]
    }
   ],
   "source": [
    "# Load datsets for preprocessing after vectorization\n",
    "train_set_x = (train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T) / 255\n",
    "test_set_x = (test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T) / 255\n",
    "print(\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"sample value: \" + str(train_set_x[index][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model-Parallel Neural Network Implementation\n",
    "To implement the Neural Network using the **SNN** framework and the above Network configuration, the workflow is comprised of five key steps:  \n",
    "1. Network Initialization.\n",
    "2. Forward Propagation.\n",
    "3. Calculate the Loss (Cost Function).\n",
    "4. Backward Propagation.\n",
    "5. Parameter Optimization (Gradient Descent).\n",
    "\n",
    "The outcome of the above stages provides the optimal model parameters, for use in final prediction, as can be seen in the process diagram below.  \n",
    "\n",
    "<img src=\"images/final_outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>\n",
    "\n",
    "The next sections will further describe each phase in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization\n",
    "#### Weight and Bias Initialization\n",
    "For an **L-Layer** network, the *Weights* and *Bias* must be initialized for each individual layer, therefore the dimensions for these matrices must match to the dimensions of each layer. For example, if $n^{[l]}$ is the number of hidden units (neurons) in layer $l$ and the size of the input $X$ is $(12288, 209)$, for $m = 209$ training examples, then:\n",
    "\n",
    "\n",
    "|               \t|      **Shape of W**      \t|  **Shape of b**  \t|                 **Activation**                \t| **Shape of Activation** \t|\n",
    "|---------------\t|:------------------------:\t|:----------------:\t|:---------------------------------------------:\t|:-----------------------:\t|\n",
    "| **Layer 1**   \t| $(n^{[1]},12288)$        \t| $(n^{[1]},1)$    \t| $Z^{[1]} = W^{[1]},X + b^{[1]}$               \t| $(n^{[1]},209)$         \t|\n",
    "| **Layer 2**   \t| $(n^{[2]}, n^{[1]})$     \t| $(n^{[2]},1)$    \t| $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$         \t| $(n^{[2]}, 209)$        \t|\n",
    "| $\\vdots$      \t| $\\vdots$                 \t| $\\vdots$         \t| $\\vdots$                                      \t| $\\vdots$                \t|\n",
    "| **Layer L-1** \t| $(n^{[L-1]}, n^{[L-2]})$ \t| $(n^{[L-1]}, 1)$ \t| $Z^{[L-1]} =,W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ \t| $(n^{[L-1]}, 209)$      \t|\n",
    "| **Layer L**   \t| $(n^{[L]}, n^{[L-1]})$   \t| $(n^{[L]}, 1)$   \t| $Z^{[L]} =,W^{[L]} A^{[L-1]} + b^{[L]}$       \t| $(n^{[L]}, 209)$        \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute $W X + b$ in python, it carries out broadcasting. For example, if:  \n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "j  & k  & l \\\\\\\n",
    "m  & n & o  \\\\\\\n",
    "p  & q & r\n",
    "\\end{bmatrix}\n",
    "\\space\n",
    "\\space\n",
    "X = \\begin{bmatrix}\n",
    "a  & b  & c \\\\\\\n",
    "d  & e & f \\\\\\\n",
    "g  & h & i \n",
    "\\end{bmatrix}\n",
    "\\space\n",
    "\\space\n",
    "b =\\begin{bmatrix}\n",
    "s  \\\\\\\n",
    "t  \\\\\\\n",
    "u\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$\n",
    "WX + b = \\begin{bmatrix}\n",
    "(ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\\\n",
    "(ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\\\n",
    "(pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the initialization process, the *Weights* are initialized randomly using a \"standard\" normal distribution with a mean of $0$ and a standard deviation of $1$. To further constrain the weights to be close to zero **but** not exactly zero (for *symmetry breaking*), each random weight is multiplied by $0.01$. The *Bias* is initialized to zero but also multiplied by $0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Weights* and *Bias* are further optmimized using [ReLU](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf) initialization. This means that for all layers that use the **ReLU** activation function, the *Weights* are initialized using the following formula:\n",
    "$$\\sqrt \\frac{2}{n}$$\n",
    "Where $n$ is the number of Neurons (hidden units) for the Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch Initialization\n",
    "To build mini-batches from the Training Set, the following two steps are applied:\n",
    "- **Shuffle:**  \n",
    "    Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/kiank_shuffle.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Partition:**  \n",
    "    Partition the shuffled (X, Y) into mini-batches of size `batch_size` (**64** from `parameters.json`). The number of training examples is **not** always divisible by `batch_size`, therefore last mini batch might be smaller. When the final mini-batch is smaller than the full `batch_size`, it will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/kiank_partition.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** The last mini-batch might end up smaller than mini_batch_size=64. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer, using the `math.floor(s)` function in Python. If the total number of examples is not a multiple of 64 then there will be:  \n",
    "$$\\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor$$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be:  \n",
    "$$m-mini_\\_batch_\\_size \\times \\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "The *Forward Propagation* step of the process is comprised of two separate pieces, the **Linear** activation and the **Non-Linear** activation to constrain the outputs between $0$ and $1$.  \n",
    "\n",
    "#### Linear Activation\n",
    "The *Linear* part of the activation computes the following equation:  \n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l]} + b^{[l]}$$\n",
    "\n",
    ">**Note:** It is important to cache the Linear Activations ($Z$) for later use in he Backward Propagation process.\n",
    "\n",
    "Where $A^{[0]} = X$\n",
    "\n",
    "#### Non-Linear Activation\n",
    "The **L-Layer** Neural Network implements two differnt non-linear activation functions:  \n",
    "\n",
    "- **Rectified Linear Unit (ReLU):** The mathematical formula for the *ReLU* function is $A = ReLU(Z) = max(0, Z)$.  \n",
    "- **Sigmoid:** The methematical formula for the *Sigmoid* function is $\\sigma(Z) = \\sigma(W\\cdot A+b) = \\frac{1}{1 + e^{(-z)}}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "**Cross Entropy** is commonly-used in binary classification (labels are assumed to take values $0$ or $1$) as a loss function which is computed by:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\big[y^{(i)}\\cdot\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\cdot\\log\\left(1-a^{[L](i)}\\right)\\big]$$\n",
    "\n",
    "Where $a^{[L](i)}$ is the last layer of the network and is synonymous with $\\hat{y}$.\n",
    "\n",
    "Cross entropy measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other. Generally, comparing to quadratic cost function, cross entropy cost function has the advantages that fast convergence and is more likely to reach the global optimization. For the mathematical details, see [wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "*Backward Propagation* is used to calculate the the gradient of the *Loss* function with respect to the various paramaters, as follows:\n",
    "\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linear Derivative\n",
    "As with *Forward Propagation*, there are two derivative non-linear activation functions for *Sigmoid* and *ReLU* respectively. If $g(\\cdot)$ is the activations function, then the derivative of *Sigmoid* and *ReLU* compute:\n",
    "$$dZ^{[l]} = \\frac{\\partial\\mathcal{L}}{\\partial Z^{[l]}} = dA^{[l]} \\cdot g^{'}(Z^{[l]})$$\n",
    "\n",
    "#### Linear Derivative\n",
    "Once the derivative of the non-linear activation is computed, the derivatives of $W^{[l]}$, $b^{[l]}$ and $A^{[l]}$, are computed using the input $dZ^{[l]}$, to get , $dW^{[l]}$, $db^{[l]}$, $dA^{[l-1]}$ as follows:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$  \n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$  \n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Update using Smallest Error Mini-batch Gradient Descent\n",
    "The Model parameters ($W^{[l]}$ and $b^{[l]}$) are updated using **Gradient Descent** using the following formula:  \n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha\\cdot dW^{[l]}$$  \n",
    "$$b^{[l]} = b^{[l]} - \\alpha\\cdot db^{[l]}$$  \n",
    "\n",
    "Where $l$ is the *layer* and $\\alpha$ is the *Learning Rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "After the fitted parameters are updated using the *Smallest Error Mini-batch Gradient Descent*, the paramaters can be used to predict wether a new image can classified as a **cat** or **non-cat** image. For further information on how the accuracy of the trained model fairs against testing data or unseen data, see the **Analysis** Notebook.\n",
    "\n",
    "---\n",
    "## Next: Code Overview\n",
    "Now that the **Serverless Neural Network** has beenn introduced, it's time to review the Python code that makes up the various Lambda Functions in the [**Codebook**](./Codebook.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
