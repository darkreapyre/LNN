{
    "epochs": 10,
    "layers": 2,
    "activations": {
        "layer1": "relu",
        "layer2": "sigmoid"
    },
    "neurons": {
        "layer1": 3,
        "layer2": 1
    },
    "learning_rate": 0.0075,
    "beta1": 0.9,
    "beta2": 0.9,
    "epsilon": 1e-8,
    "optimizer": "adam",
    "batch_size": 64
}