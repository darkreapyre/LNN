{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Lambda Function\n",
    "The `launch.py` Lambda Function is triggered by the S3 event where training data is uploaded to S3. It further initiliazes the various components needed, such as:\n",
    "1. State as well as the outputs of each epoch/iteration in DynamoDB:\n",
    "    - Training Set Accuracy.\n",
    "    - Test Set Accuracy.\n",
    "    - Weight paramter.\n",
    "    - Bias parameter.\n",
    "2. Temporary S3 Storage:\n",
    "    - The Output data of each layer and neauron over each epoch.\n",
    "    - This data is a series of `numpy` arrays that is leveraged as input data by the next layer or epoch/iteration.\n",
    "3. Preprocessing the Input Data: \n",
    "    - Read in the the initial *training*, *test* sets as well as the associated *class* of Cat images.\n",
    "    - The function initially loads the data in `h5py` format and extracts the *training*, *test* and *class* information.\n",
    "    - The function further performs any standardization and normalization of the input data.\n",
    "    - The function also \"*flattens*\" the data into a column vector, thus performing **Vectorization**.\n",
    "    - This data is dumped to the temporary S3 location and will thus serve as **Layer 0** of the Neural Network.\n",
    "4. Environment Variables:\n",
    "    - These setting are stored on the S3 along with the Input Data, in a `settings.json` file.\n",
    "    - The settings include overall parameters used by the `trainer` and `neuron` Lambda Functions, such as:\n",
    "        - Total number of epochs/iterations.\n",
    "        - Total number of layers in the Neural Network (including the Output layer).\n",
    "        - Total number of \"neurons\" in each layer.\n",
    "        - The activation function to be used for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Libraries, Global and Event Variables\n",
    "\n",
    "The cell below imports all the packages that will be needed by the Lambda Function. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n",
    "- [boto3](https://pypi.python.org/pypi/boto3) is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.\n",
    "- [json](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax (although it is not a strict subset of JavaScript.\n",
    "- [os](https://docs.python.org/3/library/os.html) is a module the provides a portable way of using operating system dependent functionality. Particularly the  `environ` object is a nmapping object representing the environment.\n",
    "- [uuid](https://docs.python.org/2/library/uuid.html#uuid.uuid4) creates a unique, random ID.\n",
    "- The [io](https://docs.python.org/2/library/io.html) module provides the Python interfaces to stream handling.\n",
    "- The Python interface to the [Redis](https://pypi.python.org/pypi/redis) key-value store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adiitionally, the S3 event trigger will supply the specifics of the Input Data being uploaded to a dedicated bucket and folder in S3. For the sake of testing, the event parameters are simulated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simulate TrainerLambda ARN\n",
    "#environ[str('TrainerLambda')] = str(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For this version of the implementation, the S3 Bucket is called **lnn** and the folder is called **training_input**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish client connectivity to the various AWS services that the function will leverage, the following cell creates the needed clients as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "#Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Overview\n",
    "### 2.1 - Datasets\n",
    "It is **very important** in Neural Network programming (without the useof a Deep Learning Framework), to have a full understanding of the dimensions of the input data as well as how the dimensions are transformed at each layer, therefore to build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat, the following cells explain the datsets.\n",
    "\n",
    "To train the Neural Network, we are provided with a dataset (`datasets.h5`) contaning:\n",
    "- a training set of $m$ images containing cats and non-cats as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- a test set of $m$ images containing cats and non-cat sas well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- classes list for cat and non-cat images.\n",
    "\n",
    ">**Note:** The original dataset was comprised of two seprate files, `test_catvnoncat.h5` and `train_catvnoncat.h5`. For the sake of this implementation a single file is needed to upload to the *S3 Bucket*, `datasets.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main dataset\n",
    "dataset = h5py.File('datasets/datasets.h5', \"r\")\n",
    "\n",
    "# Get the names of the unique datsets\n",
    "datasetNames = [n for n in dataset.keys()]\n",
    "for n in datasetNames:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create numpy arrays of the various unique datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "classes = np.array(dataset[\"list_classes\"][:]) # the list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaye the dimensions of each unique data set\n",
    "print(\"train_set_x_orig dimensions: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_y_orig dimension: \" + str(train_set_y_orig.shape))\n",
    "print(\"test_set_x_orig dimensions: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_y_orig dimensions: \" + str(test_set_y_orig.shape))\n",
    "test_set_y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, the image data (`train_set_x_orig` and `test_set_x_orig`) are 4-dimensional arrays consiting of $209$ training examoples (**m_train**) and $50$ testing images (**m_test**) respecitivaly. Each image is in turn of *height*, *width* and *depth* (**R**ed, **G**reen **B**lue values) of $64 \\times 64 \\times 3$.\n",
    "\n",
    "Additionally, the dimentsion for the labels (`train_set_y_orig` and `test_set_y_orig`) only show a $209$ and $50$ column structure. So it is recommended when coding new networks, don't use data structures where the shape is $5$, or $n$, rank 1 array. Instead, this is set to, `(1, 209)` and `(1, 50)`, to make them a **row vector**, and in essence add another dimension to the `Numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create row vectors for the labels.\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_set_y dimensions: \" + str(train_set_y.shape))\n",
    "print(\"test_set_y dimensions: \" + str(test_set_y.shape))\n",
    "test_set_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Now that the additional dimension has been added to the label data, we can note the additional \"[[ ]]\" when displaying the array.\n",
    "\n",
    "Next we can see the label data and view the corresponding image, in this case, $index = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cat picture\n",
    "index = 2\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \\\n",
    "       \", and therefore it's a '\" + \\\n",
    "       classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \\\n",
    "       \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.squeeze()` method extracts the \" inner dimension\" of the array, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_y[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(train_set_y[:, index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">**Note:** The \"[ ]\" has been removed.\n",
    "\n",
    "\n",
    "### 2.2 - Data Preprocessing\n",
    "The final model is expecting a traing set and a test set represented by a numpy array of shape (no. pixels $\\times$ no. pixels $\\times$ depth, data set size) respectivley. In turn, the model is expecting the training set and test set labels represented as a numpy array (vector) of shape (1, data set size) respectivley.\n",
    "\n",
    ">**Note:** It is not determined as yet wether the \"vectorization\" of the images should be performed by the `TrainerLambda` to set up the inputs for *Layer 0*. For the sake of Version 1.0, the preprocessing of the input data will be performed by `launch.py` as various helper functions.\n",
    "\n",
    "#### 2.2.1 - Vectorize\n",
    "The images are represented by a 3D array of shape $(length, height, depth = 3)$. However, when an image is read as the input of an algorithm it is converted to a vector of shape $(length*height*3, 1)$. In other words, it is \"unrolled\", \"flattened\" or \"reshaped\" from a 3D array into a 1D vector as can be seen below.\n",
    "\n",
    "<img src=\"images/vectorization.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "The following cells show explais of this process using the `train_set_x_orig` numpy array. The end result for the input to the model is a is a numpy array where where each column represents a flattenned image in a matrix with all the inpute features (images) being a colum, $209$ for the training set and $50$ for the test set repsectivley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of origional training set\n",
    "orig = train_set_x_orig\n",
    "print(\"Original shape: \" + str(orig.shape))\n",
    "\n",
    "# \"vectorize\" or flatten out the array into an 1D vector\n",
    "flatten = orig.reshape(orig.shape[0], -1)\n",
    "print(\"Flattened shape: \"+ str(flatten.shape))\n",
    "\n",
    "# Transpose into a colums\n",
    "flatten_T = flatten.T\n",
    "print(\"Transpose: \" + str(flatten_T.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For further intuation of what the above code is doing, the following shows a more \"manual\", alternate way.\n",
    "\n",
    "#### 2.2.2 - Standardize\n",
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from $0$ to $255$. One common preprocessing step in machine learning is to substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by $255$ (the maximum value of a pixel channel). \n",
    "\n",
    "\n",
    ">**Note:** During the training of the model, the weights willbe multiplied and biases added to the initial inputs in order to observe neuron activations. Then it will backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datsets for preprocessing after vectorization\n",
    "train_set_x = (train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T) / 255\n",
    "test_set_x = (test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T) / 255\n",
    "print(\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"sample value: \" + str(train_set_x[index][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Function Components\n",
    "### 3.1 Helper Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_cache(endpoint, obj, name):\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is dict:\n",
    "        #x = json.dumps(obj)\n",
    "        #val = json.loads(x)\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_cache(endpoint, key):\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return array\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return array\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        #parsed = json.loads(data)\n",
    "        #array = json.dumps(parsed, indent=4, sort_keys=True)\n",
    "        #return array\n",
    "        return data\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        return int(data)\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### `dump2cache(endpoint, dump, name)`\n",
    "Serializes a string or JSON to a binary string and stores it in ElastiCache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj2cache(endpoint, obj, name):\n",
    "    def isjson(x):\n",
    "        try:\n",
    "            json_obj = json.loads(x)\n",
    "        except ValueError as e:\n",
    "            return False\n",
    "        return True\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = array.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif isjson(obj):\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        val = json.dumps(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        vale = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache2obj(endpoint, key):\n",
    "    if 'float' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return array\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        array = json.loads(data)\n",
    "        return array\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        array = int(data)\n",
    "        return array\n",
    "    elif \"string\" in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        array = data\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump2cache(endpoint, dump, name):\n",
    "    \"\"\"\n",
    "    Serializes a string or JSON to a binary string and stores it in ElastiCache\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint\n",
    "    dump -- JSON dump or string to be stored in ElastiCache\n",
    "    name -- name used as the hash key of the data\n",
    "    \n",
    "    Return:\n",
    "    Hash key for the data\n",
    "    \"\"\"\n",
    "    \n",
    "    key = str(name)\n",
    "    cache = redis(host=endpoint, port=6379, db=0)\n",
    "    val = dump\n",
    "    cache.set(key, val)\n",
    "    return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `cache2numpy(endpoint, key)`\n",
    "De-serializes binary string from Elasticache to a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache2numpy(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary string from Elasticache to a Numpy array\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint\n",
    "    key -- name of te hash key of the data to be retrieved\n",
    "    \n",
    "    Return:\n",
    "    Numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get array from Redis\n",
    "    cache = redis(host=endpoint, port=6379, db=0)\n",
    "    data = cache.get(key)\n",
    "    # De-serialize the value\n",
    "    array_dtype, length, width = key.split('|')[1].split('#')\n",
    "    array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy2cache(endpoint, array, name)`\n",
    "Seializes a Numpy array to a binary string and stores it in ElastiCache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numpy2cache(endpoint, array, name):\n",
    "    \"\"\"\n",
    "    Seializes a Numpy array to a binary string and stores it in the Redis cache\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint\n",
    "    array -- Numpy array to stored in ElastiCache\n",
    "    name -- name used as the hash key of the data\n",
    "    \n",
    "    Return:\n",
    "    Hash key for the array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get parameters for the key\n",
    "    array_dtype = str(array.dtype)\n",
    "    length, width = array.shape\n",
    "    # Convert the array to string\n",
    "    val = array.ravel().tostring()\n",
    "    # Create a key from the name and necessary parameters from the array\n",
    "    # i.e. {name}|{type}#{length}#{width} \n",
    "    key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "    # Store the binary string to Redis\n",
    "    cache = redis(host=endpoint, port=6379, db=0)\n",
    "    cache.set(key, val)\n",
    "    return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `name2str()`\n",
    "Converts the name of the numpy array to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note**: An alternate method to *List Comprehension* is to use the `chain()` function to get the names of the Numpy arrays.\n",
    "```python\n",
    "from itertools import chain\n",
    "list(chain.from_iterable(a_names))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `vectorize()`\n",
    "Reshapes (flatten) the image data to column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `standardize()`\n",
    "Preprocess the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `initialize_data()`\n",
    ">**Note:** For the sake of testing, the following is not defined as function.\n",
    "\n",
    "Extracts the training and testing data from S3, flattens, standardizes, initializes the weights and bias and then creates an entry in Elastiache for neurons to process as layer $a^{[0]}$.\n",
    "\n",
    "Returns:  \n",
    "a_name -- list of the Numpy array names  \n",
    "dims --- dimensions of each of the data sets  \n",
    "params -- dictionary of the weight and bias parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fake parameters for w and b since this is not a function\n",
    "w = 0\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main dataset\n",
    "dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "\n",
    "# Retieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "\n",
    "# Create numpy arrays from the various h5 datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "#classes = np.array(dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "# Reshape labels\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "# Preprocess inputs\n",
    "train_set_x = standardize(train_set_x_orig)\n",
    "test_set_x = standardize(test_set_x_orig)\n",
    "\n",
    "# Dump the inputs to the temporary s3 bucket for TrainerLambda\n",
    "#bucket = storage_init() # Creates a temporary bucket for the propogation steps\n",
    "data_keys = {} # Dictionary for the hask keys of the data set\n",
    "dims = {} # Dictionary of data set dimensions\n",
    "a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "a_names = [] # Placeholder for array names\n",
    "for i in range(len(a_list)):\n",
    "    # Create a lis of the names of the numpy arrays\n",
    "    a_names.append(name2str(a_list[i], globals()))\n",
    "for j in range(len(a_list)): \n",
    "    #data_keys[str(a_names[j][0])] = numpy2cache(endpoint, array=a_list[j], name=a_names[j][0])\n",
    "    data_keys[str(a_names[j][0])] = to_cache(endpoint, obj=a_list[j], name=a_name[j][0])\n",
    "    dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "# Initialize weights\n",
    "if w == 0: # Initialize weights to dimensions of the input data\n",
    "    dim = dims.get('train_set_x')[0]\n",
    "    weights = np.zeros((dim, 1))\n",
    "    # Store the initial weights as a column vector on S3\n",
    "    #data_keys['weights'] = numpy2cache(endpoint, array=weights, name='weights')\n",
    "    data_keys['weights'] = to_cache(endpoint, obj=weights, name='weights')\n",
    "else:\n",
    "    #placeholder for random weight initialization\n",
    "    pass\n",
    "        \n",
    "# Initialize Bias\n",
    "if b != 0:\n",
    "    #placeholder for random bias initialization\n",
    "    #data_keys['bias'] = numpy2cache(endpoint, array=bias, name='bias')\n",
    "    pass\n",
    "else:\n",
    "    #data_keys['bias'] = dump2cache(endpoint, dump=str(b), name='bias')\n",
    "    data_keys['bias'] = to_cache(endpoint, obj=b, name='bias')\n",
    "    \n",
    "# Initialize the results tracking object\n",
    "#dump2cache(endpoint, dump='', name='results')\n",
    "to_cache(endpoint, obj='', name='results')\n",
    "        \n",
    "\n",
    "#return data_keys, [j for i in a_names for j in i], dims, #arams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note:** It might be a good practice to insert assertion statements here as part of debuging.\n",
    "\n",
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "# Retieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "\n",
    "# Load datsets for preprocessing after vectorization\n",
    "print(\"Shape of train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"Sample Value from numpy array: \" + str(train_set_x[index][index]))\n",
    "\n",
    "# Retrieve data from cache\n",
    "data_key = data_keys.get('train_set_x')\n",
    "#data = cache2numpy(endpoint, data_key)\n",
    "data = from_cache(endpoint, data_key)\n",
    "\n",
    "# Get the cache data from \n",
    "print(\"Shape of ElastiCache Data: \" + str(data.shape))\n",
    "print(\"Sample Value from Elasticache Data: \" + str(data[index][index]))\n",
    "\n",
    "# Get weights and bias\n",
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "data_key = data_keys.get('bias')\n",
    "data = int(cache.get(data_key))\n",
    "print(\"Bias Value from ElastiCache: \" + str(data))\n",
    "data_key = data_keys.get('weights')\n",
    "#data = cache2numpy(endpoint, data_key)\n",
    "data = from_cache(endpoint, data_key)\n",
    "print(\"Weights Value Shape from Elasticache: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lambda Handler\n",
    "#### Process `event` variables\n",
    "Within the `event` variables are the specifics of the S3 environment from which the Lambda Function is triggered. The first objective is to capture the S3 *Bucket* and S3 *Key* in order to get the Network Architecture setting and the input data that traiggered the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve datasets and setting from S3\n",
    "input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "try:\n",
    "    input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "    input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(\"Error downloading input data from S3, S3 object does not exist\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Neural Network Settings\n",
    "the various settings from `settings.json` are appended to the environment settings to be used later as the pyaload for the *Trainer* Lambda Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the neural network parameters\n",
    "with open('/tmp/parameters.json') as parameters_file:\n",
    "    parameters = json.load(parameters_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the `payload` to send to the *Trainer* Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build in additional parameters from neural network parameters\n",
    "parameters['epoch'] = 1\n",
    "\n",
    "# Next Layer to process\n",
    "parameters['layer'] = 1\n",
    "\n",
    "# Input data sets\n",
    "# Simulate return variables from initialize_data()\n",
    "parameters['input_data'] = [j for i in a_names for j in i]\n",
    "\n",
    "parameters['data_keys'] = data_keys\n",
    "parameters['data_dimensions'] = dims\n",
    "\n",
    "# Initialize payload to `TrainerLambda`\n",
    "payload = {}\n",
    "\n",
    "# Initialize the overall state\n",
    "payload['state'] = 'start'\n",
    "\n",
    "# Dump the parameters to ElastiCache\n",
    "#payload['parameters'] = dump2cache(endpoint, dump=dumps(parameters), name='parameters')\n",
    "payload['parameters'] = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "# ElastiCache endpoint \n",
    "payload['endpoint'] = endpoint\n",
    "\n",
    "# Prepare the payload for `TrainerLambda`\n",
    "payloadbytes = dumps(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload['parameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the neural network parameters from ElastiCache\n",
    "key = payload.get('parameters')\n",
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "data = cache.get(key)\n",
    "parsed = json.loads(data)\n",
    "print(json.dumps(parsed, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch the `TrainerLambda` Function\n",
    "\n",
    "```python\n",
    "# kick off lambda for next layer\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=environ['TrainerLambda'], #ENSURE ARN POPULATED BY CFN OR S3 EVENT\n",
    "        InvocationType='Event',\n",
    "        Payload=payloadbytes\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Trainer Lambda Function\n",
    "The `trainer.py` Lambda Function is the most critical funciton in the set in that it:\n",
    "1. Tracks and updates the state across the interations/epochs and the various layers of the Neural Network.\n",
    "2. Launches the various Neurons (`NeuraonLamabda`) in ech layer and tracks their output.\n",
    "\n",
    "In order to accomplish this, the `TrainerLambda` has three possible states, `start`, `forward` and `backward`:\n",
    "1. `start`: This state starts the initial or subsequent training epochs and performs the following:\n",
    "    - Initializes the new weights and bias for the epoch.\n",
    "    - Updates the state table with these values.\n",
    "2. `forward`: This state processes the *forward* porpogation step and launches the various hidden layer/s Neurons and supplies the necessary state information to these functions, such as:\n",
    "    - Input data location\n",
    "    - Wights and Bias.\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Activation Funciton for the Layer.\n",
    "3. `backward`: This state processes the *back* propogation/optimization step and launches the various hidden layer/s Neurons as well as supplies the necessary information for these functions, like:\n",
    "    - Gradient Parameters\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Learning Rate.\n",
    "    - Loss function calculated fromthe Forward propogation step.\n",
    "\n",
    ">**Side Note**: Ther may be a necessity later on when dealing with multiple hidden layers, to combine the `forward` and `backward` states into a `propogate` state and then have a separate `optimize` state for **Gradient Decent**.\n",
    "\n",
    "## 1 - Libraries, Global and Event Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from decimal import Decimal, Inexact, Rounded\n",
    "from boto3.dynamodb.types import DYNAMODB_CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "dynamo_client = client('dynamodb', region_name='us-west-2') # DynamoDB access\n",
    "dynamodb = resource('dynamodb', region_name='us-west-2')\n",
    "DYNAMODB_CONTEXT.traps[Inexact] = 0\n",
    "DYNAMODB_CONTEXT.traps[Rounded] = 0\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Function Components\n",
    "### 2.1 Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy2s3(array, name, bucket)`\n",
    "Write a numpy array to S3 as a file, without using local copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numpy2s3(array, name, bucket):\n",
    "    \"\"\"\n",
    "    Write a numpy array to S3 as a file, without using local copy\n",
    "    \n",
    "    Arguments:\n",
    "    array -- Numpy array to save to s3\n",
    "    name -- file of the saved Numpy array\n",
    "    \"\"\"\n",
    "    f_out = io.BytesIO()\n",
    "    np.save(f_out, array)\n",
    "    try:\n",
    "        s3_client.put_object(Key=name, Bucket=bucket, Body=f_out.getvalue(), ACL='bucket-owner-full-control')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `finish()`\n",
    "\n",
    "#### `propogate()`\n",
    "\n",
    "#### `optimize()` -> May not be necessary!!!\n",
    "\n",
    "#### `loss()`\n",
    "\n",
    "#### `update_state()`\n",
    "\n",
    "### 2.2 Lambda Handler\n",
    "#### Process `event` variables\n",
    "Within the `event` variables are the specifics of the state, either passed from the `LaunchLambda` function or `NeuronLambda` Function and detail the \"next strep\". To provide an overview of this, the following code will execute based on data passed from the `LaunchLambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate event varibales from `TrainerLambda`\n",
    "event = payload\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute appropriate action based on the the current state\n",
    "# Get the current state\n",
    "current_state = event.get('state')\n",
    "if current_state == 'forward':\n",
    "    # Get important state variables\n",
    "    \n",
    "    # Determine the location within forwardprop\n",
    "    if layer > layers:\n",
    "        # Location is at the end of forwardprop\n",
    "        # Caculate the Loss function\n",
    "        \n",
    "        # Update the Loss function to Dynamo\n",
    "        \n",
    "        # Start backprop\n",
    "        #propogate(direction='backward', layer=layer-1)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # Move to the next hidden layer\n",
    "        #propogate(direction='forward', layer=layer+1, activations=activations)\n",
    "        \n",
    "        pass\n",
    "\n",
    "elif current_state == 'backward':\n",
    "    # Get important state variables\n",
    "    \n",
    "    # Determine the location within backprop\n",
    "    if epoch == epochs and layer == 0:\n",
    "        # Location is at the end of the final epoch\n",
    "        \n",
    "        # Caculate derivative?????????????????????????\n",
    "        \n",
    "        # Caclulate the absolute final weight\n",
    "        \n",
    "        # Update the final weights and results (cost) to DynamoDB\n",
    "        \n",
    "        # Finalize the the process and clean up\n",
    "        #finish()\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    elif epoch < epochs and layer == 0:\n",
    "        # Location is at the end of the current epoch and backprop is finished\n",
    "        # Calculate the derivative?????????????????????????\n",
    "        \n",
    "        # Calculate the weights for this epoch\n",
    "        \n",
    "        # Update the weights and results (cost) to DynamoDB\n",
    "        \n",
    "        # Start the next epoch\n",
    "        #epoch = epoch + 1\n",
    "        #start(epoch)\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        # Move to the next hidden layer\n",
    "        #propogate(direction='backward', layer=layer-1)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "elif current_state == 'start':\n",
    "    # Start of a new run of the process\n",
    "    # Get important event variables from event triggerd from `LaunchLambda`\n",
    "    s3_bucket = event['s3_bucket']\n",
    "    state_table = event['state_table']\n",
    "    learning_rate = event['learning_rate']\n",
    "    #weights = event['w']\n",
    "    #bias = event['b']\n",
    "    epochs = event['epochs']\n",
    "    epoch = event['epoch']\n",
    "    layers = event['layers']\n",
    "    layer = event['layer']\n",
    "    activations = event['activations']\n",
    "    #neurons = event.get('neurons')['layer' + str(layer)]\n",
    "    #current_activation = event.get('activations')['layer' + str(current_layer)]\n",
    "    \n",
    "    # Initialize Weights\n",
    "    #if weights == 0: # Initial weights to dimensions of input data\n",
    "    #    dims = event.get('dimensions')['train_set_x'][0]\n",
    "    #    w = np.zeros((dims, 1))\n",
    "    #    # Store the initial Weights data to S3 for Neurons\n",
    "    #    numpy2s3(w, name='weights', bucket=s3_bucket)\n",
    "        \n",
    "    #else:\n",
    "    #    #placeholder for random initialization of weights\n",
    "    #    pass\n",
    "    \n",
    "    # Initialize Bias\n",
    "    #if bias != 0:\n",
    "    #    #placeholder for other bias initialization\n",
    "    #    pass\n",
    "    #else:\n",
    "    #    b = bias\n",
    "    #    # Store the initial Bias data to S3 for Neurons\n",
    "    #    numpy2s3(b, name='bias', bucket=s3_bucket)\n",
    "    \n",
    "    # Create a epoch 1 in DynamoDB with ALL initial parameters\n",
    "    table = dynamodb.Table('state')\n",
    "    table.put_item(\n",
    "        Item = {\n",
    "            'epoch': epoch,\n",
    "            'epochs': epochs,\n",
    "            'layer': layer+1,\n",
    "            'learning_rate': Decimal(learning_rate),\n",
    "            'activations': activations,\n",
    "            'state_table': state_table,\n",
    "            's3_bucket': s3_bucket,\n",
    "            'params': event['params']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Start forwardprop\n",
    "    #layer = layer + 1 # Shuould equate to 0+1\n",
    "    #propogate(direction='forward', layer=layer+1, activations=activations)\n",
    "\n",
    "else:\n",
    "    print(\"No state informaiton has been provided.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note**: At the point of writing the codebook, the easiest way for the Neurons to process the input data as well as the weights (since it's a vector the same size as the input data *AND* an `ndarray`) is to store the weights with the input data. However as the networks become more complex, it would be a good practice to store the weights as part of the state in DynamoDB or an alternate Key,Value store similar to what `mxnet` uses. (See **Notes.md**)  \n",
    "\n",
    "**Sanity Check to confirm Weights from S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s32numpy = io.BytesIO(s3_client.get_object(Bucket=bucket, Key='weights')['Body'].read())\n",
    "content = s32numpy.getvalue()\n",
    "data = np.load(io.BytesIO(content))\n",
    "print(\"Shape of train_set_x: \" + str(train_set_x.shape))\n",
    "print(\"Shape of w: \" + str(w.shape))\n",
    "print(\"Shape of data from s3: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: When getting the Bias data from S3, the result is a numpy array -> `array(0)`. Therefore is may be necessary when using the data in the `NeuronLambda`, to convert it to an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix A: Build the Lambda Deployment Package"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
