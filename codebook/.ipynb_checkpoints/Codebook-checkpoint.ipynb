{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Neural Network: 0.2  (L-Layer)\n",
    "## Overview\n",
    "The Lambda Neural Network is a solution for generating a Cloud Native image classification Neural Network, using idempotent Amazon Web Services ([AWS](https://aws.amazon.com/what-is-aws/)) [Lambda](https://aws.amazon.com/lambda/) Functions. The LNN framework should conform to leveraging multiple Layers (excluding a single layer) with multiple neurons per layer. But for the sake of this version, the following *Codebook* will implement a **2-Layer** Neural Network as follows:\n",
    "\n",
    "<img src=\"images/2layerNN_kiank.png\" style=\"width:700px;height:400;\">\n",
    "\n",
    "The Neural Network Model (shown above) can be summarized as:\n",
    "\n",
    "***INPUT --> LINEAR/RELU --> LINEAR/SIGMOID --> OUTPUT***\n",
    "- The **Input** is a $(64, 64, 3)$ image what is flattened to a vector $(12288, 1)$. See the **Data Overview** Section.\n",
    "- The corresponding vector: $[x_{0}, x_{1}, \\dots, x_{12287}]^T$ is then multiplied by the **weight matrix** $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
    "- The **bias** term is then added to take the **Relu** (non-linear activation) to get a vector of size $[a^{[1]}_0, a^{[2]}_1, \\dots, a^{[1]}_{n^{[1]}-1}]^T$.\n",
    "- The process is then repeatred for the next layer, by taking the resulting vector and multiplying it by the weight matrix $W^{[2]}$ and then adding the intercept (**bias**).\n",
    "- Lastly, the **sigmoid** activation is applied to the result. If the result is greater then $0.5$, it is classified as a **Cat**.\n",
    "\n",
    "Therefore, the **Network Model Parameters** (*parameters.json*) for the above process are as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"epochs\": 25,\n",
    "    \"layers\": 2,\n",
    "    \"activations\": {\n",
    "        \"layer1\": \"relu\",\n",
    "        \"layer2\": \"sigmoid\"\n",
    "    },\n",
    "    \"neurons\": {\n",
    "        \"layer1\": 3,\n",
    "        \"layer2\": 1\n",
    "    },\n",
    "    \"weight\": 0.01,\n",
    "    \"bias\": 0.01,\n",
    "    \"learning_rate\": 0.0075\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "To implement the Neural Network using the **LNN** framework and the above Network configuration, the workflow is comprised of six key steps:  \n",
    "1. Network Initialization.\n",
    "2. Forward Propagation.\n",
    "3. Calculate the Loss (Cost Function).\n",
    "4. Backward Propagation.\n",
    "5. Parameter Optmization (Gradient Descent).\n",
    "\n",
    "The outcome of the above stages provides the optimal model parameters, for use in final prediction, as can be seen in the process diagram below.  \n",
    "\n",
    "<img src=\"images/final_outline.png\" style=\"width:700px;height:600;\">\n",
    "\n",
    "The next sections will further deascribe each phase in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization\n",
    "For an **L-Layer** network, the *Weights* and *Bias* must be initialized for each inidivual layer, therfore the dimensions for these matricies must match to the dimensions of each layer. For example, if $n^{[l]}$ is the number of hidden units (neurons) in layer $l$ and the size of the input $X$ is $(12288, 209)$, for $m = 209$ training examples, then:\n",
    "\n",
    "<table style=\\\"width:100%\\\">\n",
    "    <tr>\n",
    "        <td>  </td>\n",
    "        <td> **Shape of W** </td>\n",
    "        <td> **Shape of b**  </td>\n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> **Layer 1** </td>\n",
    "        <td> $(n^{[1]},12288)$ </td>\n",
    "        <td> $(n^{[1]},1)$ </td>\n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td>\n",
    "        <td> $(n^{[1]},209)$ </td>\n",
    "        <tr>\n",
    "    <tr>\n",
    "        <td> **Layer 2** </td>\n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td>\n",
    "        <td> $(n^{[2]},1)$ </td>\n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td>\n",
    "        <td> $(n^{[2]}, 209)$ </td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> $\\vdots$ </td>\n",
    "        <td> $\\vdots$ </td>\n",
    "        <td> $\\vdots$ </td>\n",
    "        <td> $\\vdots$ </td>\n",
    "        <td> $\\vdots$ </td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> **Layer L-1** </td>\n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td>\n",
    "        <td> $(n^{[L-1]}, 1)$  </td>\n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td>\n",
    "        <td> $(n^{[L-1]}, 209)$ </td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> **Layer L** </td>\n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td>\n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td>\n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute $W X + b$ in python, it carries out broadcasting. For example, if:  \n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "j  & k  & l \\\\\\\n",
    "m  & n & o  \\\\\\\n",
    "p  & q & r\n",
    "\\end{bmatrix}\n",
    "X = \\begin{bmatrix}\n",
    "a  & b  & c \\\\\\\n",
    "d  & e & f \\\\\\\n",
    "g  & h & i \n",
    "\\end{bmatrix}\n",
    "b =\\begin{bmatrix}\n",
    "s  \\\\\\\n",
    "t  \\\\\\\n",
    "u\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$\n",
    "WX + b = \\begin{bmatrix}\n",
    "(ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\\\n",
    "(ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\\\n",
    "(pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the initialization process, the *Weights* are initialized randomly using a \"standard\" normal distribution with a mean of $0$ and a standard deviation of $1$. To further constrain the weights to be close to zero **but** not exactly zero (for *symmetry breaking*), each random weight is multiplied by $0.01$. The *Bias* is initialized to zero but also multiplied by $0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation\n",
    "The *Forward Propogation* step of the process is comprised of two separate pieces, the **Linear** activation and the **Non-Linear** activation to constrain the outputs between $0$ and $1$.   \n",
    "#### Linear Activation\n",
    "The *Linear* part of the activation computes the following equation:  \n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l]} + b^{[l]}$$\n",
    ">**Note:** It is important to cach the Linear Activations ($Z$) for later use in he Backward Propagation process.\n",
    "\n",
    "Where $A^{[0]} = X$\n",
    "#### Non-Linear Activation\n",
    "The **L-Layer** Neural Network implements two differnt non-linear activation functions:\n",
    "- **Rectified Linear Unit (ReLU):** The mathematical formula for the *ReLU* function is $A = ReLU(Z) = max(0, Z)$.\n",
    "- **Sigmoid:** The methematical formula for the *Sigmoid* function is $\\sigma(Z) = \\sigma(W\\cdot A+b) = \\frac{1}{1 + e^{(-z)}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "**Cross Entropy** is commonly-used in binary classification (labels are assumed to take values $0$ or $1$) as a loss function which is computed by:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\big[y^{(i)}\\cdot\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\cdot\\log\\left(1-a^{[L](i)}\\right)\\big]$$\n",
    "\n",
    "Where $a^{[L](i)}$ is the last layer of the network and is synonymous with $\\hat{y}$.\n",
    "\n",
    "Cross entropy measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other. Generally, comparing to quadratic cost function, cross entropy cost function has the advantages that fast convergence and is more likely to reach the global optimization. For the mathematical details, see [wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "*Backward Propagation* is used to calculate the the gradient of the *Loss* function with respect to the various paramaters, as follows:\n",
    "\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:900px;height:1200;\">\n",
    "\n",
    "#### Non-Linear Derivative\n",
    "As with *Forward Propagation*, there are two derivative non-linear activation functions for *Sigmoid* and *ReLU* respectivley. If $g(\\cdot)$ is the activations function, then the derivative of *Sigmoid* and *ReLU* compute:\n",
    "$$dZ^{[l]} = \\frac{\\partial\\mathcal{L}}{\\partial Z^{[l]}} = dA^{[l]} \\cdot g^{'}(Z^{[l]})$$\n",
    "\n",
    "#### Linear Derivative\n",
    "Once the derivative of the non-linear activation is computed, the derivatives of $W^{[l]}$, $b^{[l]}$ and $A^{[l]}$, are computed using the input $dZ^{[l]}$, to get , $dW^{[l]}$, $db^{[l]}$, $dA^{[l-1]}$ as follows:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$  \n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$  \n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Update using Gradient Descent\n",
    "The Model paramaters ($W^{[l]}$ and $b^{[l]}$) are updated using **Gradient Descent** using the following formula:  \n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha\\cdot dW^{[l]}$$  \n",
    "$$b^{[l]} = b^{[l]} - \\alpha\\cdot db^{[l]}$$  \n",
    "\n",
    "Where $\\alpha$ is the *Learning Rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "After the fitted parameters are updated using *Gradient Descent*, the paramaters can be used to predict wether a new image can classified as a **cat** or **non-cat** image. For further information on how the *Prediction* function is user, see the **app** in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries, Global and Event Variables\n",
    "\n",
    "The cell below imports all the packages that will be needed by the Lambda Function. \n",
    "- [datetime](https://docs.python.org/2/library/datetime.html) provides classes for manipulating dates and times in both simple and complex ways.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n",
    "- [boto3](https://pypi.python.org/pypi/boto3) is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.\n",
    "- [json](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax (although it is not a strict subset of JavaScript.\n",
    "- [os](https://docs.python.org/3/library/os.html) is a module the provides a portable way of using operating system dependent functionality. Particularly the  `environ` object is a nmapping object representing the environment.\n",
    "- [uuid](https://docs.python.org/2/library/uuid.html#uuid.uuid4) creates a unique, random ID.\n",
    "- The [io](https://docs.python.org/2/library/io.html) module provides the Python interfaces to stream handling.\n",
    "- The Python interface to the [Redis](https://pypi.python.org/pypi/redis) key-value store.\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "import redis\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Trigger Event\n",
    "T0 initiate the network training process, the dataset (**datasets.h5**) is uploaded to Amazon Simple Storage Services ([S3](https://aws.amazon.com/s3/)). This porcess triggers the S# bucket event wich starts the trianing process. A sample of the event payload sent to the LNN fromework is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "context = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For this version of the implementation, the S3 Bucket is called **lnn** and the folder is called **training_input**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish client connectivity to the various AWS services that the function will leverage, the following code creates the needed clients for the various AWS services, as global variables.\n",
    "\n",
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 low level class object\n",
    "s3_resource = resource('s3') # S3 high level service class\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocation client\n",
    "redis_client = client('elasticache', region_name='us-west-2') # ElastiCache low level object\n",
    "\n",
    "# Find and retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "cache = redis(host=endpoint, port=6379, db=0) # Connect Python to Redis Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Overview\n",
    "### Training and Test Datasets\n",
    "It is **very important** in Neural Network programming (without the useof a Deep Learning Framework), to have a full understanding of the dimensions of the input data as well as how the dimensions are transformed at each layer, therefore to build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat, the following cells explain the datsets.\n",
    "\n",
    "To train the Neural Network, we are provided with a dataset (`datasets.h5`) contaning:\n",
    "- a training set of $m$ images containing cats and non-cats as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- a test set of $m$ images containing cats and non-cat sas well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- classes list for cat and non-cat images.\n",
    "\n",
    ">**Note:** The original dataset was comprised of two seprate files, `test_catvnoncat.h5` and `train_catvnoncat.h5`. For the sake of this implementation a single file is needed to upload to the *S3 Bucket*, `datasets.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_classes\n",
      "test_set_x\n",
      "test_set_y\n",
      "train_set_x\n",
      "train_set_y\n"
     ]
    }
   ],
   "source": [
    "# Load main dataset that's stored locally\n",
    "dataset = h5py.File('datasets/datasets.h5', \"r\")\n",
    "\n",
    "# Get the names of the unique datsets\n",
    "datasetNames = [n for n in dataset.keys()]\n",
    "for n in datasetNames:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays of the various unique datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "classes = np.array(dataset[\"list_classes\"][:]) # the list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_orig dimensions: (209, 64, 64, 3)\n",
      "train_set_y_orig dimension: (209,)\n",
      "test_set_x_orig dimensions: (50, 64, 64, 3)\n",
      "test_set_y_orig dimensions: (50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaye the dimensions of each unique data set\n",
    "print(\"train_set_x_orig dimensions: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_y_orig dimension: \" + str(train_set_y_orig.shape))\n",
    "print(\"test_set_x_orig dimensions: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_y_orig dimensions: \" + str(test_set_y_orig.shape))\n",
    "test_set_y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, the image data (`train_set_x_orig` and `test_set_x_orig`) are 4-dimensional arrays consiting of $209$ training examoples (**m_train**) and $50$ testing images (**m_test**) respecitivaly. Each image is in turn of *height*, *width* and *depth* (**R**ed, **G**reen **B**lue values) of $64 \\times 64 \\times 3$.\n",
    "\n",
    "Additionally, the dimentsion for the labels (`train_set_y_orig` and `test_set_y_orig`) only show a $209$ and $50$ column structure. So it is recommended when coding new networks, don't use data structures where the shape is $5$, or $n$, rank 1 array. Instead, this is set to, `(1, 209)` and `(1, 50)`, to make them a **row vector**, and in essence add another dimension to the `Numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create row vectors for the labels.\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_y dimensions: (1, 209)\n",
      "test_set_y dimensions: (1, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_set_y dimensions: \" + str(train_set_y.shape))\n",
    "print(\"test_set_y dimensions: \" + str(test_set_y.shape))\n",
    "test_set_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Now that the additional dimension has been added to the label data, we can note the additional \"[[ ]]\" when displaying the array.\n",
    "\n",
    "Next we can see the label data and view the corresponding image, in this case, $index = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], and therefore it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f245e3f6dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a cat picture\n",
    "index = 2\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \\\n",
    "       \", and therefore it's a '\" + \\\n",
    "       classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \\\n",
    "       \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.squeeze()` method extracts the \" inner dimension\" of the array, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_y[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_set_y[:, index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">**Note:** The \"[ ]\" has been removed.\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "The final model is expecting a traing set and a test set represented by a numpy array of shape (no. pixels $\\times$ no. pixels $\\times$ depth, data set size) respectivley. In turn, the model is expecting the training set and test set labels represented as a numpy array (vector) of shape (1, data set size) respectivley.\n",
    "\n",
    ">**Note:** It is not determined as yet wether the \"vectorization\" of the images should be performed by the `TrainerLambda` to set up the inputs for *Layer 0*. For the sake of Version 1.0, the preprocessing of the input data will be performed by `launch.py` as various helper functions.\n",
    "\n",
    "#### Vectorize\n",
    "The images are represented by a 3D array of shape $(length, height, depth = 3)$. However, when an image is read as the input of an algorithm it is converted to a vector of shape $(length*height*3, 1)$. In other words, it is \"unrolled\", \"flattened\" or \"reshaped\" from a 3D array into a 1D vector as can be seen below.\n",
    "\n",
    "<img src=\"images/vectorization.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "The following cells show explais of this process using the `train_set_x_orig` numpy array. The end result for the input to the model is a is a numpy array where where each column represents a flattenned image in a matrix with all the inpute features (images) being a colum, $209$ for the training set and $50$ for the test set repsectivley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (209, 64, 64, 3)\n",
      "Flattened shape: (209, 12288)\n",
      "Transpose: (12288, 209)\n"
     ]
    }
   ],
   "source": [
    "# Copy of origional training set\n",
    "orig = train_set_x_orig\n",
    "print(\"Original shape: \" + str(orig.shape))\n",
    "\n",
    "# \"vectorize\" or flatten out the array into an 1D vector\n",
    "flatten = orig.reshape(orig.shape[0], -1)\n",
    "print(\"Flattened shape: \"+ str(flatten.shape))\n",
    "\n",
    "# Transpose into a colums\n",
    "flatten_T = flatten.T\n",
    "print(\"Transpose: \" + str(flatten_T.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For further intuation of what the above code is doing, the following shows a more \"manual\", alternate way.\n",
    "\n",
    "#### Standardize\n",
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from $0$ to $255$. One common preprocessing step in machine learning is to substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by $255$ (the maximum value of a pixel channel). \n",
    "\n",
    ">**Note:** During the training of the model, the weights willbe multiplied and biases added to the initial inputs in order to observe neuron activations. Then it will backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (12288, 209)\n",
      "sample value: 0.266666666667\n"
     ]
    }
   ],
   "source": [
    "# Load datsets for preprocessing after vectorization\n",
    "train_set_x = (train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T) / 255\n",
    "test_set_x = (test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T) / 255\n",
    "print(\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"sample value: \" + str(train_set_x[index][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Overview\n",
    "### Helper Functions\n",
    "#### `to_cache(endpoint, obj, name)`\n",
    "Serializes multiple data type to ElastiCache and returns the Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        if len(obj.shape) == 0:\n",
    "            length = 0\n",
    "            width = 0\n",
    "        else:\n",
    "            length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported serialization type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `from_cache(endpoint, key)`\n",
    "De-serializes binary object from ElastiCache by reading the type of object from the name and converting it to the appropriate data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCacheendpoint\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        if int(length) == 0:\n",
    "            obj = np.float64(np.fromstring(val))\n",
    "        else:\n",
    "            obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported serialization type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `name2str(obj, namespace)`\n",
    "Converts the name of the numpy array to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note**: An alternate method to *List Comprehension* is to use the `chain()` function to get the names of the Numpy arrays.\n",
    "```python\n",
    "from itertools import chain\n",
    "list(chain.from_iterable(a_names))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `vectorize()`\n",
    "Reshapes (flattens) the image data to column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `standardize()`\n",
    "Preprocess the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `initialize_data(endpoint, parameters)`\n",
    "Extracts the training and testing data from S3, flattens, standardizes and then dumps the data to ElastiCache for neurons to process as layer $a^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data(endpoint, parameters):\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes and then dumps the data to ElastiCache \n",
    "    for neurons to process as layer a^0.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    parameters -- The initial/running parameters dictionary object\n",
    "    \n",
    "    Returns:\n",
    "    data_keys -- Hash keys for the various numpy arrays\n",
    "    input_data -- Reference for the Input data extracted for the h5 file\n",
    "    dims -- Referenece to the dimensions of the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "    \n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "    \n",
    "    # Reshape labels\n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # Preprocess inputs\n",
    "    train_set_x = standardize(train_set_x_orig)\n",
    "    test_set_x = standardize(test_set_x_orig)\n",
    "    \n",
    "    # Create necessary keys for the data in ElastiCache\n",
    "    data_keys = {} # Dictionary for the hask keys of the data set\n",
    "    dims = {} # Dictionary of data set dimensions\n",
    "    a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "    a_names = [] # Placeholder for array names\n",
    "    for i in range(len(a_list)):\n",
    "        # Create a lis of the names of the numpy arrays\n",
    "        a_names.append(name2str(a_list[i], locals()))\n",
    "    for j in range(len(a_list)):\n",
    "        # Dump the numpy arrays to ElastiCache\n",
    "        data_keys[str(a_names[j][0])] = to_cache(endpoint=endpoint, obj=a_list[j], name=a_names[j][0])\n",
    "        # Append the array dimensions to the list\n",
    "        dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "    # Initialize A0 and Y names from `train_setx` and `train_set_y`\n",
    "    data_keys['A0'] = to_cache(endpoint=endpoint, obj=train_set_x, name='A0')\n",
    "    data_keys['Y'] = to_cache(endpoint=endpoint, obj=train_set_y, name='Y')\n",
    "    # Initialize training example size\n",
    "    m = train_set_x.shape[1]\n",
    "    data_keys['m'] = to_cache(endpoint, obj=m, name='m')\n",
    "    \n",
    "    # Multiple layer weight and bias initialization\n",
    "    for l in range(1, parameters['layers']+1):\n",
    "        if l == 1:\n",
    "            W = np.random.randn(parameters['neurons']['layer'+str(l)], train_set_x.shape[0]) / np.sqrt(train_set_x.shape[0])\n",
    "        else:\n",
    "            W = np.random.randn(parameters['neurons']['layer'+str(l)], parameters['neurons']['layer'+str(l-1)]) / np.sqrt(parameters['neurons']['layer'+str(l-1)])\n",
    "        b = np.zeros((parameters['neurons']['layer'+str(l)], 1))\n",
    "        # Store the initial weights and bias in ElastiCache\n",
    "        data_keys['W'+str(l)] = to_cache(endpoint=endpoint, obj=W, name='W'+str(l))\n",
    "        data_keys['b'+str(l)] = to_cache(endpoint=endpoint, obj=b, name='b'+str(l))\n",
    "    \n",
    "    # Initialize the results tracking object\n",
    "    results = {}\n",
    "    data_keys['results'] = to_cache(endpoint, obj=results, name='results')\n",
    "\n",
    "    return data_keys, [j for i in a_names for j in i], dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy2s3(array, name, bucket)`\n",
    "Serialize a Numpy array to S3 without using local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy2s3(array, name, bucket):\n",
    "    \"\"\"\n",
    "    Serialize a Numpy array to S3 without using local copy\n",
    "    \n",
    "    Arguments:\n",
    "    array -- Numpy array of any shape\n",
    "    name -- filename on S3\n",
    "    bucket -- S3 Bucket name\n",
    "    \"\"\"\n",
    "    f_out = io.BytesIO()\n",
    "    np.save(f_out, array)\n",
    "    try:\n",
    "        s3_client.put_object(Key=name, Bucket=bucket, Body=f_out.getvalue(), ACL='bucket-owner-full-control')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `start_epoch(epoch, layer, parameter_key)`\n",
    "Starts a new epoch and configures the necessary state tracking objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_epoch(epoch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Starts a new epoch and configures the necessary state tracking objcts.\n",
    "    \n",
    "    Arguments:\n",
    "    epoch -- Integer representing the \"current\" epoch.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the results object for the new epoch\n",
    "    parameters = from_cache(endpoint=endpoint, key=parameter_key)\n",
    "    \n",
    "    # Add current epoch to results\n",
    "    epoch2results = from_cache(endpoint=endpoint, key=parameters['data_keys']['results'])\n",
    "    epoch2results['epoch' + str(epoch)] = {}\n",
    "    parameters['data_keys']['results'] = to_cache(endpoint=endpoint, obj=epoch2results, name='results')\n",
    "   \n",
    "    # Update paramaters with this functions data\n",
    "    parameters['epoch'] = epoch\n",
    "    parameters['layer'] = layer\n",
    "    parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "    \n",
    "    # Start forwardprop\n",
    "    propogate(direction='forward', epoch=epoch, layer=layer+1, parameter_key=parameter_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `end(parameter_key)`\n",
    "Finishes the oveall training sequence and saves the \"optmized\" weights and bias to S3, for the prediction aplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end(parameter_key):\n",
    "    \"\"\"\n",
    "    Finishes the oveall training sequence and saves the \"optmized\" \n",
    "    weights and bias to S3, for the prediction aplication.\n",
    "    \n",
    "    Arguments:\n",
    "    parameter_key -- The ElastiCache key for the current set of state parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the latest parameters\n",
    "    parameters = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameter_key\n",
    "    )\n",
    "\n",
    "    # Get the results key\n",
    "    final_results = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameters['data_keys']['results']\n",
    "    )\n",
    "    # Upload the final results to S3\n",
    "    bucket = parameters['s3_bucket']\n",
    "    results_obj = s3_resource.Object(bucket,'training_results/results.json')\n",
    "    try:\n",
    "        results_obj.put(Body=json.dumps(final_results))\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    \n",
    "    # Get the final weights and bias for each layer and upload them to S3 for\n",
    "    # use by the prediction app\n",
    "    for l in range(1, parameters['layers']+1):\n",
    "        W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(l)])\n",
    "        b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(l)])\n",
    "        # Put the weights and bias onto S3 for prediction\n",
    "        numpy2s3(array=W, name='predict_input/W'+str(l), bucket=bucket)\n",
    "        numpy2s3(array=b, name='predict_input/b'+str(l), bucket=bucket)\n",
    "\n",
    "    print(\"Training Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `propogate(direction, epoch, layer, parameter_key)`\n",
    "Determines the amount of \"hidden\" units based on the layer and loops through launching the necessary `NeuronLambda` functions with the appropriate state or \"direction\" to propogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate(direction, epoch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Determines the amount of \"hidden\" units based on the layer and loops\n",
    "    through launching the necessary `NeuronLambda` functions with the \n",
    "    appropriate state. Each `NeuronLambda` implements the cost function \n",
    "    OR the gradients depending on the direction.\n",
    "\n",
    "    Arguments:\n",
    "    direction -- The current direction of the propogation, either `forward` or `backward`.\n",
    "    epoch -- Integer representing the \"current\" epoch to close out.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "\n",
    "    Note: When launching NeuronLambda with multiple hidden unit,\n",
    "    remember to assign an ID, also remember to start at 1\n",
    "    and not 0. for example:\n",
    "    num_hidden_units = 5\n",
    "    for i in range(1, num_hidden_units + 1):\n",
    "        # Do stuff\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the parameters for the layer\n",
    "    parameters = from_cache(endpoint=endpoint, key=parameter_key)\n",
    "    num_hidden_units = parameters['neurons']['layer' + str(layer)]\n",
    "    \n",
    "    # Build the NeuronLambda payload\n",
    "    payload = {}\n",
    "    # Add the parameters to the payload\n",
    "    payload['state'] = direction\n",
    "    payload['parameter_key'] = parameter_key\n",
    "    payload['epoch'] = epoch\n",
    "    payload['layer'] = layer\n",
    "\n",
    "    # Determine process based on direction\n",
    "    if direction == 'forward':\n",
    "        # Launch Lambdas to propogate forward\n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this function's updates\n",
    "        parameters['epoch'] = epoch\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "        # Debug Statements\n",
    "        #print(\"Starting Forward Propogation for epoch \" + str(epoch) + \", layer \" + str(layer))\n",
    "\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            # Prepare the payload for `NeuronLambda`\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            #payloadbytes = dumps(payload)\n",
    "            # Debug Styatements\n",
    "            #print(\"Payload to be sent NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            neuron_handler(event=payload, context='')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    elif direction == 'backward':\n",
    "        # Launch Lambdas to propogate backward        \n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this functions updates\n",
    "        parameters['epoch'] = epoch\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "        # Debug Statement\n",
    "        #print(\"Starting Backward Propogation for epoch \" + str(epoch) + \", layer \" + str(layer))\n",
    "\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            # Prepare the payload for `NeuronLambda`\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            payloadbytes = dumps(payload)\n",
    "            # Debug ststements\n",
    "            #print(\"Payload to be sent to NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            neuron_handler(event=payload, context='')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sigmoid(z)`\n",
    "Computes the sigmoid of `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    sigmoid(z)\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sigmoid_backward(dA, z)`\n",
    "Computes the derivative of the sigmoid function, given `z`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, z):\n",
    "    \"\"\"\n",
    "    Implement the derivative of the sigmoid function\n",
    "\n",
    "    Arguments:\n",
    "    dA -- Post-activation gradient, of any shape\n",
    "    z -- Cached linear activation from Forward prop\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the Cost with respect to z\n",
    "    \"\"\"\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    # Debug statement\n",
    "    assert(dZ.shape == z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `relu(z)`\n",
    "Implements the Rectified Linear Unit function of `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function.\n",
    "\n",
    "    Arguments:\n",
    "    z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    a -- Post-activation parameter, of the same shape as z\n",
    "    \"\"\"\n",
    "    a = np.maximum(0, z)\n",
    "    assert(a.shape == z.shape)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `relu_backward(dA, z)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- Post-activation gradient, of any shape\n",
    "    z -- Cached linear activation from Forward propagation\n",
    "\n",
    "    Returns:\n",
    "    dz -- Gradient of the cost with respect to z\n",
    "    \"\"\"\n",
    "    dz = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dz[z <= 0] = 0\n",
    "    assert (dz.shape == z.shape)\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Handler Functions\n",
    "#### `launch_handler(event, context)`\n",
    "This `lambda_handler()` is triggered by the S3 event where training data is uploaded to S3. It further initiliazes the various components needed, such as:\n",
    "1. State tracking Objects:\n",
    "    - Overall Results (Cost) for each Epoch.\n",
    "    - Gradients for erach layer.\n",
    "    - Initial and updated Weight paramter for each layer.\n",
    "    - Initial and updated Bias parameter for each layer.\n",
    "2. DynamoDB Storage:\n",
    "    - Invoction ID for each Lambda Function invokation to prevent duplicate invocation.\n",
    "    >**Note:** The DynamoDB Initialization is **NOT** recorded within the **Codebook**.\n",
    "3. Preprocessing the Input Data: \n",
    "    - Read in the the initial *training*, *test* of Cat and Non-cat images.\n",
    "    - The function initially loads the data in `h5py` format and extracts the *training* and *test* data.\n",
    "    - The function further performs any standardization and normalization of the input data.\n",
    "    - The function also \"*flattens*\" the data into a column vector, thus performing **Vectorization**.\n",
    "    - This data is dumped to ElastiCache and will thus serve as **Layer 0** of the Neural Network.\n",
    "4. Environment and State Tracking Variables:\n",
    "    - Loading the initial Neural Network Parameters (`parameters.json`) and augmenting these parameter with the state variables during the training process. The settings include overall parameters used by the `trainer` and `neuron` Lambda Functions, such as:\n",
    "        - Total number of epochs/iterations.\n",
    "        - Total number of layers in the Neural Network (including the Output layer).\n",
    "        - Total number of \"neurons\" in each layer.\n",
    "        - The activation function to be used for each layer.\n",
    "    - Initializing the **Hash Keys** for the various data sets in ElastiCache to be used by the subsequent functions to get access to the numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_handler(event, context):\n",
    "    # Retrieve datasets and setting from S3\n",
    "    input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "    dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "    settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "    try:\n",
    "        input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "        input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(\"Error downloading input data from S3, S3 object does not exist\")\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Extract the neural network parameters\n",
    "    with open('/tmp/parameters.json') as parameters_file:\n",
    "        parameters = json.load(parameters_file)\n",
    "    \n",
    "    # Build in additional neural network parameters\n",
    "    # Input data sets and data set parameters\n",
    "    parameters['s3_bucket'] = event['Records'][0]['s3']['bucket']['name']\n",
    "    parameters['data_keys'],\\\n",
    "    parameters['input_data'],\\\n",
    "    parameters['dims'] = initialize_data(\n",
    "        endpoint=endpoint,\n",
    "        parameters=parameters\n",
    "    )\n",
    "    \n",
    "    # Initialize payload to `TrainerLambda`\n",
    "    payload = {}\n",
    "    # Initialize the overall state\n",
    "    payload['state'] = 'start'\n",
    "    # Dump the parameters to ElastiCache\n",
    "    payload['parameter_key'] = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "    #payload['endpoint'] = endpoint\n",
    "    # Prepare the payload for `TrainerLambda`\n",
    "    #payloadbytes = dumps(payload)\n",
    "    \n",
    "    # Debug statements\n",
    "    print(\"Complete Neural Network Settings: \\n\")\n",
    "    print(dumps(parameters, indent=4, sort_keys=True))\n",
    "    print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "    trainer_handler(event=payload, context='')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `trainer_handler(event, context)`\n",
    "This `lambda_handler()` function is the most critical funciton in the set in that it:\n",
    "1. Tracks and updates the state across the interations/epochs and the various layers of the Neural Network.\n",
    "2. Performs Vectorization on the Activation Row Vectors from each Neuron to create a *Matrix* of Activations.\n",
    "3. Launches the various Neurons (`NeuronLamabda`) in each layer and tracks their output for *Forward* or *Backward* propogation.\n",
    "4. Calculates the *Cost* for each interation of *Forward* propogation.\n",
    "5. Performs *Gradient Descent* for each Epoch.\n",
    "\n",
    "In order to accomplish this, the `TrainerLambda` has three possible states, `start`, `forward` and `backward`:\n",
    "1. `start`: This state starts the initial or subsequent training epochs and performs the following:\n",
    "    - Initializes the new weights and bias for the epoch.\n",
    "    - Updates the state table with these values.\n",
    "2. `forward`: This state processes the *forward* porpogation step and launches the various hidden layer Neurons and supplies the necessary state information to these functions, such as:\n",
    "    - Input/Activation data location\n",
    "    - Weights and Bias.\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Activation Funciton for the Layer.\n",
    "3. `backward`: This state processes the *back* propogation step and launches the various hidden layer Neurons as well as supplies the necessary information for these functions, like:\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Current and previous Activations calculated from the Forward propogation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_handler(event, context):\n",
    "    \"\"\"\n",
    "    1. Processes the `event` vaiables from the various Lambda functions that call it, \n",
    "        i.e. `TrainerLambda` and `NeuronLambda`.\n",
    "    2. Determines the \"current\" state and then directs the next steps.\n",
    "    3. Performs Vectorization from the NeuronLambda Forward propogation outputs.\n",
    "    4. Calculates the Cost.\n",
    "    5. Performs Gradient Descent given the gradients from the Backward propogation outputs.\n",
    "    \"\"\"\n",
    "    # Get the current state from the invoking lambda\n",
    "    state = event.get('state')\n",
    "    global parameters\n",
    "    parameters = from_cache(endpoint=endpoint, key=event.get('parameter_key'))\n",
    "    \n",
    "    # Execute appropriate action based on the the current state\n",
    "    if state == 'forward':\n",
    "        # Perform vectorization to create a matrix of activations and/or calculate the Cost\n",
    "        # Get important state variables\n",
    "        epoch = event.get('epoch')\n",
    "        layer = event.get('layer')\n",
    "\n",
    "        # First pre-process the Activations from the \"previous\" layer\n",
    "        # Use the following Redis command to ensure a pure string is return for the key\n",
    "        r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "        key_list = []\n",
    "        # Compile a list of activations\n",
    "        for key in r.scan_iter(match='layer'+str(layer-1)+'_a_*'):\n",
    "            key_list.append(key)\n",
    "        # Create a dictionary of activation results\n",
    "        A_dict = {}\n",
    "        for i in key_list:\n",
    "            A_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "        # Number of Neuron Activations\n",
    "        num_activations = len(key_list)\n",
    "        # Create a numpy array of the results, depending on the number\n",
    "        # of hidden units (a Matrix of Activations)\n",
    "        A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "        if num_activations == 1:\n",
    "            # Single Neuron Activation\n",
    "            dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "            #debug\n",
    "            #print(\"Dimensions to reshape single hidden unit activations: \" + str(dims))\n",
    "            A = A.reshape(int(dims[0]), int(dims[1]))\n",
    "            assert(A.shape == (parameters['dims']['train_set_y'][0], parameters['dims']['train_set_y'][1]))\n",
    "        else:\n",
    "            # Multiple Neuron Activcatoins\n",
    "            A = np.squeeze(A)\n",
    "            assert(A.shape == (parameters['neurons']['layer'+str(layer-1)], parameters['dims']['train_set_x'][1]))\n",
    "\n",
    "        # Add the `A` Matrix to `data_keys` for later Neuron use\n",
    "        A_name = 'A' + str(layer-1)\n",
    "        parameters['data_keys'][A_name] = to_cache(endpoint=endpoint, obj=A, name=A_name)\n",
    "\n",
    "        # Update ElastiCache with this function's data\n",
    "        parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "        \n",
    "        # Determine the location within forwardprop\n",
    "        if layer > parameters['layers']:\n",
    "            # Location is at the end of forwardprop, therefore calculate Cost\n",
    "            # Get the training examples data and no. examples (`Y` and `m`)\n",
    "            Y = from_cache(endpoint=endpoint, key=parameters['data_keys']['Y'])\n",
    "            m = from_cache(endpoint=endpoint, key=parameters['data_keys']['m'])\n",
    "            \n",
    "            # Calculate the Cost\n",
    "            #cost = -1 / m * np.sum(np.multiply(Y, np.log(A)) + np.multiply((1 - Y), np.log(1 - A))) #from ste-by-step\n",
    "            cost = (1./m) * (-np.dot(Y, np.log(A).T) - np.dot(1 - Y, np.log(1 - A).T)) #from application\n",
    "            \"\"\"\n",
    "            Note: The cost calculation above returns `nd.array`, therefore converting to\n",
    "            type `float` for for the results upload\n",
    "            \"\"\"\n",
    "            #cost = (-1 / m) * np.sum(Y * (np.log(A)) + ((1 - Y) * np.log(1 - A))) #from S-Layer\n",
    "            cost = np.squeeze(cost)\n",
    "            assert(cost.shape == ())\n",
    "\n",
    "            # Update results with the Cost\n",
    "            # Get the results object\n",
    "            cost2results = from_cache(endpoint=endpoint, key=parameters['data_keys']['results'])\n",
    "            # Append the cost to results object\n",
    "            #cost2results['epoch' + str(epoch)]['cost'] = cost\n",
    "            cost2results['epoch' + str(epoch)]['cost'] = float(cost)\n",
    "            # Update results key in ElastiCache\n",
    "            parameters['data_keys']['results'] = to_cache(endpoint=endpoint, obj=cost2results, name='results')\n",
    "\n",
    "            #if epoch % 100 == 0:\n",
    "                #print(\"Cost after epoch {0}: {1}\".format(epoch, cost))\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, cost))\n",
    "\n",
    "            # Initialize backprop\n",
    "            # Calculate the derivative of the Cost with respect to the last activation\n",
    "            # Ensure that `Y` is the correct shape as the last activation\n",
    "            Y = Y.reshape(A.shape)\n",
    "            dA = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "            dA_name = 'dA' + str(layer-1)\n",
    "            parameters['data_keys'][dA_name] = to_cache(endpoint=endpoint, obj=dA, name=dA_name)\n",
    "\n",
    "            # Update parameters from this function in ElastiCache\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "            # Start Backpropogation on NeuronLambda\n",
    "            propogate(direction='backward', epoch=epoch, layer=layer-1, parameter_key=parameter_key)\n",
    "\n",
    "        else:\n",
    "            # Move to the next hidden layer for multiple layer networks\n",
    "            #debug\n",
    "            #print(\"Propogating forward onto Layer \" + str(layer))\n",
    "            propogate(direction='forward', epoch=epoch, layer=layer, parameter_key=parameter_key)\n",
    "        \n",
    "    elif state == 'backward':\n",
    "        # Get important state variables\n",
    "        epoch = event.get('epoch')\n",
    "        layer = event.get('layer')\n",
    "\n",
    "        # Vectorize the derivatives\n",
    "        \"\"\"\n",
    "        Note: Need to create a function to do this later, but want to test the functionality\n",
    "        first by manuall creting the vectors\n",
    "        \"\"\"\n",
    "        # First pre-process the derivative of the linear activation\n",
    "        # Use the following Redis command to ensure a pure string is return for the key\n",
    "        r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "        key_list = []\n",
    "        # Compile a list of dertivatives\n",
    "        for key in r.scan_iter(match='layer'+str(layer+1)+'_dZ_*'):\n",
    "            key_list.append(key)\n",
    "        # Create a dictionary of activation results\n",
    "        dZ_dict = {}\n",
    "        for i in key_list:\n",
    "            dZ_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "            # Number of Neuron Activations\n",
    "            num_activations = len(key_list)\n",
    "            # Create a numpy array of the results, depending on the number\n",
    "            # of hidden units (a Matrix of derivatives `dZ`)\n",
    "            dZ = np.array([arr.tolist() for arr in dZ_dict.values()])\n",
    "            if num_activations == 1:\n",
    "                # Single Neuron\n",
    "                dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "                dZ = dZ.reshape(int(dims[0]), int(dims[1]))\n",
    "            else:\n",
    "                # Multiple Neurons\n",
    "                dZ = np.squeeze(dZ)\n",
    "        \n",
    "        # Next pre-process the derivative of the weights\n",
    "        # Use the following Redis command to ensure a pure string is return for the key\n",
    "        r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "        key_list = []\n",
    "        # Compile a list of dertivatives\n",
    "        for key in r.scan_iter(match='layer'+str(layer+1)+'_dw_*'):\n",
    "            key_list.append(key)\n",
    "        # Create a dictionary of activation results\n",
    "        dW_dict = {}\n",
    "        for i in key_list:\n",
    "            dW_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "            # Number of Neuron Activations\n",
    "            num_activations = len(key_list)\n",
    "            # Create a numpy array of the results, depending on the number\n",
    "            # of hidden units (a Matrix of derivatives `dZ`)\n",
    "            dW = np.array([arr.tolist() for arr in dW_dict.values()])\n",
    "            if num_activations == 1:\n",
    "                # Single Neuron\n",
    "                dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "                dW = dW.reshape(int(dims[0]), int(dims[1]))\n",
    "            else:\n",
    "                # Multiple Neurons\n",
    "                dW = np.squeeze(dW)\n",
    "        \n",
    "        # pre-process the derivatives of the bias\n",
    "        # Use the following Redis command to ensure a pure string is return for the key\n",
    "        r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "        key_list = []\n",
    "        # Compile a list of dertivatives\n",
    "        for key in r.scan_iter(match='layer'+str(layer+1)+'_db_*'):\n",
    "            key_list.append(key)\n",
    "        # Create a dictionary of activation results\n",
    "        db_dict = {}\n",
    "        for i in key_list:\n",
    "            db_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "            # Number of Neuron Activations\n",
    "            num_activations = len(key_list)\n",
    "            # Create a numpy array of the results, depending on the number\n",
    "            # of hidden units (a Matrix of derivatives `dZ`)\n",
    "            db = np.array([arr.tolist() for arr in db_dict.values()])\n",
    "            db = db.reshape(db.shape[0], 1)\n",
    "        \n",
    "        # Determine the location within backprop\n",
    "        if epoch == parameters['epochs']-1 and layer == 0:\n",
    "            # Location is at the end of the final epoch\n",
    "            # Get relavent parameters for bacprop\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            # Run Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "            # Update ElastiCache with the new Weights and new Bias to be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['W'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=W,\n",
    "                name='W'+str(layer+1)\n",
    "            )\n",
    "            parameters['data_keys']['b'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='b'+str(layer+1)\n",
    "            )\n",
    "            \n",
    "            # Update parameters for the next epoch\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "                        \n",
    "            # Finalize the the process and clean up\n",
    "            end(parameter_key=parameter_key)\n",
    "            \n",
    "        elif epoch < parameters['epochs']-1 and layer == 0:\n",
    "            # Location is at the end of the current epoch and backprop is finished\n",
    "            # Get relavent parameters for bacprop\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            # Run Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "            # Update ElastiCache with the new Weights and new Bias to be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['W'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=W,\n",
    "                name='W'+str(layer+1)\n",
    "            )\n",
    "            parameters['data_keys']['b'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='b'+str(layer+1)\n",
    "            )\n",
    "            \n",
    "            # Update parameters for the next epoch\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "                        \n",
    "            # Start the next epoch\n",
    "            start_epoch(epoch=epoch+1, layer=0, parameter_key=parameter_key)\n",
    "            \n",
    "        else:\n",
    "            # Location is still within the backprop process, therefore calculate \n",
    "            # the derivative of the current layer's activations with respect to the \n",
    "            # Cost as well as perform gradient decent to get and new weights and bias\n",
    "            # Get relavent parameters for bacprop\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            dA = np.dot(W.T, dZ)\n",
    "            dA_name = 'dA' + str(layer)\n",
    "            parameters['data_keys'][dA_name] = to_cache(endpoint=endpoint, obj=dA, name=dA_name)\n",
    "\n",
    "            # Run Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            # Update ElastiCache with the new Weights and new Bias to be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['W'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=W,\n",
    "                name='W'+str(layer+1)\n",
    "            )\n",
    "            parameters['data_keys']['b'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='b'+str(layer+1)\n",
    "            )\n",
    "\n",
    "            # Update parameters from this function in ElastiCache\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "            # Move to the next hidden layer\n",
    "            propogate(direction='backward', epoch=epoch, layer=layer, parameter_key=parameter_key)\n",
    "            \n",
    "    elif state == 'start':\n",
    "        # Start training process        \n",
    "        # Create initial parameters\n",
    "        epoch = 0\n",
    "        layer = 0\n",
    "        start_epoch(epoch=epoch, layer=layer, parameter_key=event.get('parameter_key'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `neuron_handler(event, context)`\n",
    "This `lambda_handler()` simulates a single *Perceptron* for both forward and backward propogation. If the state is `forward` then the function simulates forward propogation for $X$ to $Cost$ for the current layer. If the state is backward, then the function calculates the gradient of the derivative of the activation function for the current layer.\n",
    "\n",
    ">**Note:** This function also moves the state to the next or previous layer, depending on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_handler(event, context):\n",
    "    \"\"\"\n",
    "    This Lambda Funciton simulates a single Perceptron for both \n",
    "    forward and backward propogation.\n",
    "    \"\"\"    \n",
    "    # Get the Neural Network parameters from Elasticache\n",
    "    parameters = from_cache(endpoint, key=event.get('parameter_key'))\n",
    "       \n",
    "    # Get the current state\n",
    "    state = event.get('state')\n",
    "    epoch = event.get('epoch')\n",
    "    layer = event.get('layer')\n",
    "    ID = event.get('id') # To be used when multiple activations\n",
    "    # Determine is this is the last Neuron in the layer\n",
    "    last = event.get('last')\n",
    "    activation = event.get('activation')\n",
    "    # Debug Statement\n",
    "    #print(\"Starting {} propagation on Neuron: {}, for Epoch {} and Layer {}\".format(state, str(ID), str(epoch), str(layer)))\n",
    "\n",
    "    if state == 'forward':\n",
    "        # Forward propogation from A0 to Cost\n",
    "        # Activations from the previous layer\n",
    "        A_prev = from_cache(endpoint=endpoint, key=parameters['data_keys']['A'+str(layer - 1)])\n",
    "        # Get the weights for this neuron\n",
    "        w = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer)])[ID-1, :]\n",
    "        # Convert weights to a row vector\n",
    "        w = w.reshape(1, w.shape[0])\n",
    "        # Get the bias for this neuron as row vector\n",
    "        b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer)])[ID-1, :]\n",
    "        # Perform the linear part of the layer's forward propogation\n",
    "        z = np.dot(w, A_prev) + b\n",
    "        # Upload the linear transformation results to ElastiCache for use with Backprop\n",
    "        to_cache(endpoint=endpoint, obj=z, name='layer'+str(layer)+'_z_'+str(ID))\n",
    "\n",
    "        # Perform non-linear activation based on the activation function\n",
    "        if activation == 'sigmoid':\n",
    "            a = sigmoid(z)\n",
    "        elif activation == 'relu':\n",
    "            a = relu(z)\n",
    "        else:\n",
    "            # No other functions supported at this time\n",
    "            pass\n",
    "        # Upload the results to ElastiCache for `TrainerLambda` to vectorize\n",
    "        to_cache(endpoint=endpoint, obj=a, name='layer'+str(layer)+'_a_'+str(ID))\n",
    "\n",
    "        # Debug Statement\n",
    "        #print(\"Completed Forward Propogation for epoch {}, layer {}\".format(str(epoch), str(layer)))\n",
    "        \n",
    "        if last == \"True\":\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['epoch'] = epoch\n",
    "            parameters['layer'] = layer + 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'forward'\n",
    "            payload['epoch'] = epoch\n",
    "            payload['layer'] = layer + 1\n",
    "\n",
    "            # Debug Statement\n",
    "            #print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            trainer_handler(event=payload, context='')\n",
    "\n",
    "        return\n",
    "\n",
    "    elif state == 'backward':\n",
    "        # Backprop from Cost to X (A0)\n",
    "        \"\"\"\n",
    "        Inutition: TrainerLambda launched back prop with `layer-1`, therefore this should be \n",
    "        last \"active\" layer. That means that the \"dA\" for this layer has already been\n",
    "        calculate. Thus, no need to do the `A - Y` error calculation. Additionally, \n",
    "        the following code structure makes the it more idempotenent for multiple layers.\n",
    "        \"\"\"\n",
    "        # Get necessary parameters\n",
    "        r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "        z_key = []\n",
    "        for z in r.scan_iter(match='layer'+str(layer)+'_z_'+str(ID)+'*'):\n",
    "            z_key.append(z)\n",
    "        z = from_cache(endpoint=endpoint, key=z_key[0])\n",
    "        m = from_cache(endpoint=endpoint, key=parameters['data_keys']['m'])\n",
    "        A_prev = from_cache(endpoint=endpoint, key=parameters['data_keys']['A'+str(layer-1)])\n",
    "\n",
    "        # Get the derivative of the current layer's activation,\n",
    "        # based on the size of the layer.\n",
    "        if layer == parameters['layers']:\n",
    "            # If this is the last layer, then:\n",
    "            dA = from_cache(endpoint=endpoint, key=parameters['data_keys']['dA'+str(layer)])\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer)])\n",
    "        else:\n",
    "            dA = from_cache(endpoint=endpoint, key=parameters['data_keys']['dA'+str(layer)])[ID-1, :]\n",
    "            dA = dA.reshape(1, dA.shape[0])\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer)])[ID-1, :]\n",
    "            W = W.reshape(1, W.shape[0])\n",
    "        \n",
    "        # Calculate the derivative of the Activations\n",
    "        if activation=='sigmoid':\n",
    "            dZ = sigmoid_backward(dA, z)\n",
    "        elif activation == 'relu':\n",
    "            dZ = relu_backward(dA, z)\n",
    "        elif activaion == 'leaky_relu':\n",
    "            dZ = leaky_relu_backward(dA, z)\n",
    "        else:\n",
    "            # No other functions supported at this time\n",
    "            pass\n",
    "        # Upload the derivative of the activation to ElastiCache for use by `TrainerLambda`\n",
    "        to_cache(endpoint=endpoint, obj=dZ, name='layer'+str(layer)+'_dZ_'+str(ID))\n",
    "        \n",
    "        # Calculate the derivatives of the weights\n",
    "        dw = 1 / m * np.dot(dZ, A_prev.T)\n",
    "        # Upload the derivative of the weights to ElastiCache for use by `TrainerLambda`\n",
    "        to_cache(endpoint=endpoint, obj=dw, name='layer'+str(layer)+'_dw_'+str(ID))\n",
    "        \n",
    "        # Debug statement\n",
    "        assert(dw.shape == W.shape)\n",
    "\n",
    "        # Calculate the derivatives of the bias\n",
    "        db = 1 / m * np.sum(dZ, axis=1, keepdims=True) #<-- Could be an issue here and may have to reshape in TrainerLambda when runnign backprop oin layer 1\n",
    "        #db = 1 / m * np.sum(dZ)\n",
    "        # Upload the erivative of the bis to ElastiCache for use by `TrainerLambda`\n",
    "        to_cache(endpoint=endpoint, obj=db, name='layer'+str(layer)+'_db_'+str(ID))\n",
    "\n",
    "        # Debug Statement\n",
    "        #print(\"Completed Back Propogation for epoch {}, layer {}\".format(str(epoch), str(layer)))\n",
    "\n",
    "        if last == \"True\":\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['epoch'] = epoch\n",
    "            parameters['layer'] = layer - 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'backward'\n",
    "            payload['epoch'] = epoch\n",
    "            payload['layer'] = layer - 1\n",
    "\n",
    "            # Debug Statement\n",
    "            #print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            trainer_handler(event=payload, context='')\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Workflow for a 2-Layer, 10 Epochs\n",
    "### Trigger Event from S3\n",
    "**Simulate the training data being uploaded to S3 and Launching the training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache.flushdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Neural Network Settings: \n",
      "\n",
      "{\n",
      "    \"activations\": {\n",
      "        \"layer1\": \"relu\",\n",
      "        \"layer2\": \"relu\",\n",
      "        \"layer3\": \"relu\",\n",
      "        \"layer4\": \"sigmoid\"\n",
      "    },\n",
      "    \"bias\": 0.01,\n",
      "    \"data_keys\": {\n",
      "        \"A0\": \"A0|float64#12288#209\",\n",
      "        \"W1\": \"W1|float64#20#12288\",\n",
      "        \"W2\": \"W2|float64#7#20\",\n",
      "        \"W3\": \"W3|float64#5#7\",\n",
      "        \"W4\": \"W4|float64#1#5\",\n",
      "        \"Y\": \"Y|int64#1#209\",\n",
      "        \"b1\": \"b1|float64#20#1\",\n",
      "        \"b2\": \"b2|float64#7#1\",\n",
      "        \"b3\": \"b3|float64#5#1\",\n",
      "        \"b4\": \"b4|float64#1#1\",\n",
      "        \"m\": \"m|int\",\n",
      "        \"results\": \"results|json\",\n",
      "        \"test_set_x\": \"test_set_x|float64#12288#50\",\n",
      "        \"test_set_y\": \"test_set_y|int64#1#50\",\n",
      "        \"train_set_x\": \"train_set_x|float64#12288#209\",\n",
      "        \"train_set_y\": \"train_set_y|int64#1#209\"\n",
      "    },\n",
      "    \"dims\": {\n",
      "        \"test_set_x\": [\n",
      "            12288,\n",
      "            50\n",
      "        ],\n",
      "        \"test_set_y\": [\n",
      "            1,\n",
      "            50\n",
      "        ],\n",
      "        \"train_set_x\": [\n",
      "            12288,\n",
      "            209\n",
      "        ],\n",
      "        \"train_set_y\": [\n",
      "            1,\n",
      "            209\n",
      "        ]\n",
      "    },\n",
      "    \"epochs\": 1,\n",
      "    \"input_data\": [\n",
      "        \"train_set_x\",\n",
      "        \"train_set_y\",\n",
      "        \"test_set_x\",\n",
      "        \"test_set_y\"\n",
      "    ],\n",
      "    \"layers\": 4,\n",
      "    \"learning_rate\": 0.0075,\n",
      "    \"neurons\": {\n",
      "        \"layer1\": 20,\n",
      "        \"layer2\": 7,\n",
      "        \"layer3\": 5,\n",
      "        \"layer4\": 1\n",
      "    },\n",
      "    \"s3_bucket\": \"lnn\",\n",
      "    \"weight\": 0.01\n",
      "}\n",
      "Payload to be sent to TrainerLambda: \n",
      "{\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"start\"\n",
      "}\n",
      "Cost after epoch 0: 0.6925294080404927\n",
      "Training Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simulate S3 event trigger data\n",
    "np.random.seed(1)\n",
    "launch_handler(event, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activations': {'layer1': 'relu',\n",
       "  'layer2': 'relu',\n",
       "  'layer3': 'relu',\n",
       "  'layer4': 'sigmoid'},\n",
       " 'bias': 0.01,\n",
       " 'data_keys': {'A0': 'A0|float64#12288#209',\n",
       "  'A1': 'A1|float64#20#209',\n",
       "  'A2': 'A2|float64#7#209',\n",
       "  'A3': 'A3|float64#5#209',\n",
       "  'A4': 'A4|float64#1#209',\n",
       "  'W1': 'W1|float64#20#12288',\n",
       "  'W2': 'W2|float64#7#20',\n",
       "  'W3': 'W3|float64#5#7',\n",
       "  'W4': 'W4|float64#1#5',\n",
       "  'Y': 'Y|int64#1#209',\n",
       "  'b1': 'b1|float64#20#1',\n",
       "  'b2': 'b2|float64#7#1',\n",
       "  'b3': 'b3|float64#5#1',\n",
       "  'b4': 'b4|float64#1#1',\n",
       "  'dA1': 'dA1|float64#20#209',\n",
       "  'dA2': 'dA2|float64#7#209',\n",
       "  'dA3': 'dA3|float64#5#209',\n",
       "  'dA4': 'dA4|float64#1#209',\n",
       "  'm': 'm|int',\n",
       "  'results': 'results|json',\n",
       "  'test_set_x': 'test_set_x|float64#12288#50',\n",
       "  'test_set_y': 'test_set_y|int64#1#50',\n",
       "  'train_set_x': 'train_set_x|float64#12288#209',\n",
       "  'train_set_y': 'train_set_y|int64#1#209'},\n",
       " 'dims': {'test_set_x': [12288, 50],\n",
       "  'test_set_y': [1, 50],\n",
       "  'train_set_x': [12288, 209],\n",
       "  'train_set_y': [1, 209]},\n",
       " 'epoch': 0,\n",
       " 'epochs': 1,\n",
       " 'input_data': ['train_set_x', 'train_set_y', 'test_set_x', 'test_set_y'],\n",
       " 'layer': 0,\n",
       " 'layers': 4,\n",
       " 'learning_rate': 0.0075,\n",
       " 'neurons': {'layer1': 20, 'layer2': 7, 'layer3': 5, 'layer4': 1},\n",
       " 's3_bucket': 'lnn',\n",
       " 'weight': 0.01}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = from_cache(endpoint=endpoint, key='parameters|json')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer1_a_9|float64#1#209',\n",
       " 'layer1_a_6|float64#1#209',\n",
       " 'layer1_a_16|float64#1#209',\n",
       " 'layer1_a_5|float64#1#209',\n",
       " 'layer1_a_3|float64#1#209',\n",
       " 'layer1_a_1|float64#1#209',\n",
       " 'layer1_a_18|float64#1#209',\n",
       " 'layer1_a_20|float64#1#209',\n",
       " 'layer1_a_12|float64#1#209',\n",
       " 'layer1_a_11|float64#1#209',\n",
       " 'layer1_a_13|float64#1#209',\n",
       " 'layer1_a_14|float64#1#209',\n",
       " 'layer1_a_15|float64#1#209',\n",
       " 'layer1_a_4|float64#1#209',\n",
       " 'layer1_a_2|float64#1#209',\n",
       " 'layer1_a_8|float64#1#209',\n",
       " 'layer1_a_10|float64#1#209',\n",
       " 'layer1_a_19|float64#1#209',\n",
       " 'layer1_a_17|float64#1#209',\n",
       " 'layer1_a_7|float64#1#209']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 2\n",
    "epoch = 0\n",
    "# Use the following Redis command to ensure a pure string is return for the key\n",
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "key_list = []\n",
    "# Compile a list of activations\n",
    "for key in r.scan_iter(match='layer'+str(layer-1)+'_a_*'):\n",
    "    key_list.append(key)\n",
    "#key_list.sort()\n",
    "key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Not Ordered!!! Test new methodology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['layer1_a_1|float64#1#209'],\n",
       " ['layer1_a_2|float64#1#209'],\n",
       " ['layer1_a_3|float64#1#209'],\n",
       " ['layer1_a_4|float64#1#209'],\n",
       " ['layer1_a_5|float64#1#209'],\n",
       " ['layer1_a_6|float64#1#209'],\n",
       " ['layer1_a_7|float64#1#209'],\n",
       " ['layer1_a_8|float64#1#209'],\n",
       " ['layer1_a_9|float64#1#209'],\n",
       " ['layer1_a_10|float64#1#209'],\n",
       " ['layer1_a_11|float64#1#209'],\n",
       " ['layer1_a_12|float64#1#209'],\n",
       " ['layer1_a_13|float64#1#209'],\n",
       " ['layer1_a_14|float64#1#209'],\n",
       " ['layer1_a_15|float64#1#209'],\n",
       " ['layer1_a_16|float64#1#209'],\n",
       " ['layer1_a_17|float64#1#209'],\n",
       " ['layer1_a_18|float64#1#209'],\n",
       " ['layer1_a_19|float64#1#209'],\n",
       " ['layer1_a_20|float64#1#209']]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list = []\n",
    "for n in range(1, parameters['neurons']['layer'+str(layer-1)]+1):\n",
    "    tmp = r.keys('layer'+str(layer-1)+'_a_'+str(n)+'|*')\n",
    "    tmp_list.append(tmp)\n",
    "tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer1_a_1|float64#1#209',\n",
       " 'layer1_a_2|float64#1#209',\n",
       " 'layer1_a_3|float64#1#209',\n",
       " 'layer1_a_4|float64#1#209',\n",
       " 'layer1_a_5|float64#1#209',\n",
       " 'layer1_a_6|float64#1#209',\n",
       " 'layer1_a_7|float64#1#209',\n",
       " 'layer1_a_8|float64#1#209',\n",
       " 'layer1_a_9|float64#1#209',\n",
       " 'layer1_a_10|float64#1#209',\n",
       " 'layer1_a_11|float64#1#209',\n",
       " 'layer1_a_12|float64#1#209',\n",
       " 'layer1_a_13|float64#1#209',\n",
       " 'layer1_a_14|float64#1#209',\n",
       " 'layer1_a_15|float64#1#209',\n",
       " 'layer1_a_16|float64#1#209',\n",
       " 'layer1_a_17|float64#1#209',\n",
       " 'layer1_a_18|float64#1#209',\n",
       " 'layer1_a_19|float64#1#209',\n",
       " 'layer1_a_20|float64#1#209']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list = []\n",
    "for i in tmp_list:\n",
    "    key_list.append(i[0])\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1_a_10|float64#1#209': array([[ 0.54442674,  0.34593692,  0.25944104,  0.13683582,  0.93303899,\n",
       "          0.08280478,  0.02239419,  0.        ,  0.        ,  0.        ,\n",
       "          0.38968726,  0.        ,  0.33607475,  0.83338407,  0.37917124,\n",
       "          0.34318459,  0.        ,  0.        ,  0.38020611,  0.78264717,\n",
       "          0.25335092,  0.21036373,  0.14814063,  0.        ,  0.6973153 ,\n",
       "          0.18679312,  0.07819318,  0.44238734,  0.        ,  0.11214885,\n",
       "          0.25136566,  0.58207313,  0.02651407,  0.6247312 ,  0.56354792,\n",
       "          0.        ,  0.25134311,  0.13269239,  0.        ,  0.24426924,\n",
       "          0.13369705,  0.50802181,  0.        ,  0.37198647,  0.        ,\n",
       "          0.        ,  0.19381725,  0.05126231,  0.14776232,  0.49082265,\n",
       "          0.22008579,  0.        ,  0.18096098,  0.36694671,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.32534392,  0.2802818 ,\n",
       "          0.48462001,  0.38346025,  0.        ,  0.25205784,  0.0890953 ,\n",
       "          0.38522435,  0.        ,  0.2883333 ,  0.        ,  0.        ,\n",
       "          0.37929428,  0.26390749,  0.        ,  0.32856853,  0.92633699,\n",
       "          0.21851368,  0.24838039,  0.        ,  0.02012015,  0.        ,\n",
       "          0.23509876,  0.42548922,  0.02135957,  0.14697239,  0.22509854,\n",
       "          0.39678301,  0.33887369,  0.20232779,  0.37011021,  0.13373109,\n",
       "          0.        ,  0.55438153,  0.        ,  0.14065579,  0.32903636,\n",
       "          0.4751475 ,  0.01966355,  0.5095023 ,  0.06367293,  0.        ,\n",
       "          0.        ,  0.1173449 ,  0.06310289,  0.11751602,  0.        ,\n",
       "          0.19225447,  0.36693307,  0.        ,  0.01898779,  0.57038743,\n",
       "          0.93657406,  0.81462012,  0.12703015,  0.26833908,  0.21780643,\n",
       "          0.61083148,  0.        ,  0.39750918,  0.16648614,  0.42990524,\n",
       "          0.20473426,  0.35775953,  0.18711999,  0.02887513,  0.21885912,\n",
       "          0.10047495,  0.28274242,  0.24912502,  0.3908569 ,  0.39676271,\n",
       "          0.0984387 ,  0.        ,  0.        ,  0.37563809,  0.32271204,\n",
       "          0.        ,  0.        ,  0.14812799,  0.12081742,  0.        ,\n",
       "          0.03023393,  0.16663006,  0.        ,  0.17481087,  0.51691216,\n",
       "          0.24867829,  0.        ,  0.        ,  0.06198085,  0.        ,\n",
       "          0.14272343,  0.20957019,  0.37978242,  0.        ,  0.41073288,\n",
       "          0.07648215,  0.35226041,  0.07620021,  0.22953938,  0.09240999,\n",
       "          0.34359837,  0.32250839,  0.08651802,  0.42042122,  0.55610584,\n",
       "          0.10647267,  0.19241634,  0.42488087,  0.09569575,  0.08142056,\n",
       "          0.        ,  0.51135491,  0.18095554,  0.        ,  0.17550167,\n",
       "          0.36998908,  0.27734987,  0.07604927,  0.30140312,  0.11235685,\n",
       "          0.        ,  0.02817254,  0.17754384,  0.80235973,  0.19506384,\n",
       "          0.04678204,  0.17551443,  0.        ,  0.        ,  0.        ,\n",
       "          0.47528067,  0.14612542,  0.        ,  0.        ,  0.51555454,\n",
       "          0.        ,  0.1895162 ,  0.        ,  0.29030711,  0.        ,\n",
       "          0.29747136,  0.        ,  0.        ,  0.03069807,  0.53132247,\n",
       "          0.        ,  0.00643756,  0.03648293,  0.37560759]]),\n",
       " 'layer1_a_11|float64#1#209': array([[ 0.51198921,  0.75072639,  1.1094455 ,  0.46346983,  0.29436306,\n",
       "          0.46020885,  0.85885444,  0.51039194,  0.91937425,  0.34922958,\n",
       "          1.35367308,  0.58312163,  0.63451674,  0.46294815,  0.5818021 ,\n",
       "          1.31991284,  0.91784918,  1.23169778,  1.0065433 ,  0.41516689,\n",
       "          1.00819979,  0.51371717,  1.16422001,  1.5654395 ,  0.82763455,\n",
       "          0.        ,  0.60835458,  0.75388761,  0.53284101,  0.35565117,\n",
       "          0.        ,  1.36990023,  0.72434631,  1.2143185 ,  0.38835418,\n",
       "          0.82906195,  0.59032191,  1.32174329,  0.20700712,  0.65774137,\n",
       "          0.54399014,  0.08440948,  0.91166411,  0.94383288,  1.01229268,\n",
       "          0.40321623,  1.07267354,  0.60844603,  0.44090087,  0.53814112,\n",
       "          1.14817642,  0.67228332,  0.9898568 ,  1.0762852 ,  0.50198152,\n",
       "          1.55745215,  0.76369967,  1.47835979,  0.27350425,  0.17505534,\n",
       "          0.60868789,  0.45594135,  1.16847683,  0.63246134,  0.78149092,\n",
       "          0.71241273,  1.22591268,  0.63519403,  0.84911543,  1.18044121,\n",
       "          1.05091702,  0.42686909,  0.48172767,  0.65081248,  0.98198504,\n",
       "          0.89550738,  0.4986066 ,  0.39269918,  0.48571144,  0.28378921,\n",
       "          0.87603995,  1.0397954 ,  0.08002996,  0.36851375,  0.70958353,\n",
       "          0.26555141,  0.85776518,  0.25805668,  0.96656969,  0.60362555,\n",
       "          0.56182208,  1.05925217,  0.50900601,  0.68291828,  1.28387765,\n",
       "          1.01177219,  0.97642925,  0.83225364,  1.34728185,  0.54684695,\n",
       "          1.08275733,  0.59203911,  0.49666163,  0.78160238,  0.65596911,\n",
       "          0.99110993,  1.22818103,  0.81747523,  0.72244057,  0.43735545,\n",
       "          0.85113715,  0.74509679,  0.02748213,  0.88085908,  1.17344937,\n",
       "          1.1995792 ,  0.45480078,  0.96327359,  0.74669704,  0.41716157,\n",
       "          1.15583849,  1.32741299,  0.48346758,  1.17414289,  1.13276166,\n",
       "          1.1750473 ,  0.68516344,  0.27173309,  0.93965058,  0.34061782,\n",
       "          0.75495239,  1.20471599,  0.98808259,  0.57499677,  0.47241273,\n",
       "          0.79842042,  1.35527045,  1.59137623,  0.71433318,  0.55121662,\n",
       "          1.0906968 ,  1.3495238 ,  1.3899898 ,  0.25714307,  1.12024541,\n",
       "          0.63156921,  1.00751739,  1.13323402,  0.10854019,  0.87200322,\n",
       "          1.04299847,  0.50596945,  0.76056038,  0.82781932,  0.20272718,\n",
       "          0.40412099,  0.18275056,  0.63871519,  1.08770909,  0.58020215,\n",
       "          0.59936921,  0.85827155,  0.94500449,  0.17457951,  0.4392227 ,\n",
       "          0.46741085,  0.8834414 ,  0.45494341,  1.05599905,  1.38347746,\n",
       "          0.99199121,  0.68793364,  1.08498264,  1.02447689,  0.87697177,\n",
       "          0.13044135,  0.63998776,  0.48341266,  0.34517361,  0.81009864,\n",
       "          0.68787666,  0.92172707,  0.80051489,  0.87634643,  0.51707141,\n",
       "          0.50584619,  0.91893088,  1.07616841,  0.7650786 ,  0.71764872,\n",
       "          1.48926348,  1.59788974,  0.67348083,  0.83136937,  0.3795758 ,\n",
       "          0.15272019,  0.73587038,  0.89503307,  0.3752195 ,  0.72830593,\n",
       "          1.10166637,  1.28368429,  0.4762416 ,  0.40273545,  1.01027747,\n",
       "          0.        ,  0.76222816,  0.27899368,  0.31108189]]),\n",
       " 'layer1_a_12|float64#1#209': array([[ 0.04657034,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.49894057,  0.32040654,  0.        ,\n",
       "          0.        ,  0.21550528,  0.13454385,  0.        ,  0.        ,\n",
       "          0.19523017,  0.        ,  0.05966736,  0.        ,  0.        ,\n",
       "          0.14805536,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.10884324,  0.03192245,  0.40394161,  0.2029804 ,  0.        ,\n",
       "          0.63907094,  0.        ,  0.        ,  0.        ,  0.47870685,\n",
       "          0.14048019,  0.3617787 ,  0.        ,  0.14533916,  0.        ,\n",
       "          0.        ,  0.        ,  0.59078731,  0.        ,  0.3195978 ,\n",
       "          0.19774073,  0.30647543,  0.        ,  0.12726159,  0.14649908,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.45161378,\n",
       "          0.        ,  0.01792707,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.12811787,  0.        ,  0.        ,  0.        ,\n",
       "          0.13958253,  0.0218618 ,  0.04836193,  0.        ,  0.        ,\n",
       "          0.10723388,  0.10554023,  0.27710527,  0.23898858,  0.        ,\n",
       "          0.        ,  0.02034069,  0.02632055,  0.        ,  0.04165287,\n",
       "          0.        ,  0.        ,  0.22648842,  0.29839482,  0.03232819,\n",
       "          0.2551496 ,  0.        ,  0.00555483,  0.        ,  0.06314589,\n",
       "          0.20900114,  0.        ,  0.03661997,  0.23428194,  0.1767171 ,\n",
       "          0.        ,  0.        ,  0.        ,  0.13551813,  0.37271728,\n",
       "          0.        ,  0.08868977,  0.35675253,  0.05514297,  0.10253709,\n",
       "          0.1956066 ,  0.        ,  0.1331202 ,  0.00612924,  0.        ,\n",
       "          0.        ,  0.        ,  0.11702586,  0.        ,  0.        ,\n",
       "          0.03307164,  0.        ,  0.        ,  0.        ,  0.1070886 ,\n",
       "          0.1293164 ,  0.        ,  0.14030548,  0.        ,  0.        ,\n",
       "          0.08129756,  0.        ,  0.05108445,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.11940566,  0.        ,  0.3654751 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.33030166,  0.02015947,  0.09671023,  0.00271892,  0.03959856,\n",
       "          0.21246622,  0.        ,  0.        ,  0.342891  ,  0.16313512,\n",
       "          0.22290459,  0.19696134,  0.45529275,  0.        ,  0.05832635,\n",
       "          0.40882757,  0.27610419,  0.        ,  0.07736724,  0.        ,\n",
       "          0.02251202,  0.        ,  0.        ,  0.        ,  0.16190597,\n",
       "          0.28393634,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.57703995,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.35623202,  0.        ,  0.01505688,  0.        ,  0.08186601,\n",
       "          0.27247446,  0.14015477,  0.45258223,  0.27209671,  0.35274177,\n",
       "          0.13510762,  0.        ,  0.05228595,  0.        ,  0.2153987 ,\n",
       "          0.00992503,  0.        ,  0.36851895,  0.        ,  0.2606315 ,\n",
       "          0.        ,  0.22832753,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.01517291,  0.        ,  0.06816197]]),\n",
       " 'layer1_a_13|float64#1#209': array([[ 0.33424902,  0.08774207,  0.30666969,  0.        ,  0.3091232 ,\n",
       "          0.00465701,  0.        ,  0.77723325,  0.27223185,  0.80763815,\n",
       "          0.        ,  0.37211826,  0.2272689 ,  0.94036089,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.15947212,\n",
       "          0.54526973,  0.02680102,  0.47457068,  0.08710542,  0.11018276,\n",
       "          0.33904174,  0.1620931 ,  0.31372711,  0.16708402,  0.        ,\n",
       "          0.        ,  0.01468259,  0.        ,  0.26764122,  0.09415908,\n",
       "          0.06475371,  0.        ,  0.38644843,  0.        ,  0.29994738,\n",
       "          0.        ,  0.        ,  0.37512187,  0.29553614,  0.01735286,\n",
       "          0.34492236,  0.        ,  0.        ,  0.        ,  0.37577336,\n",
       "          0.27537963,  0.20146376,  0.04972305,  0.36336132,  0.40833125,\n",
       "          0.        ,  0.4637635 ,  0.30051244,  0.        ,  0.31493334,\n",
       "          0.196202  ,  0.23916356,  0.13329541,  0.        ,  0.27074351,\n",
       "          0.19239052,  0.        ,  0.07632801,  0.41676636,  0.25439534,\n",
       "          0.26724395,  0.47479648,  0.        ,  0.5124974 ,  0.00552417,\n",
       "          0.31623039,  0.        ,  0.54815367,  0.        ,  0.27063667,\n",
       "          0.        ,  0.        ,  0.06106891,  0.        ,  0.41484623,\n",
       "          0.28831537,  0.        ,  0.41060005,  0.31222331,  0.        ,\n",
       "          0.33214953,  0.27294582,  0.        ,  0.31688254,  0.        ,\n",
       "          0.47646575,  0.09519374,  0.        ,  0.083706  ,  0.20422538,\n",
       "          0.        ,  0.        ,  0.57858467,  0.        ,  0.38291508,\n",
       "          0.11064537,  0.        ,  0.08196594,  0.15868098,  0.73071702,\n",
       "          0.        ,  0.        ,  0.30844416,  0.17814721,  0.11631767,\n",
       "          0.        ,  0.36393943,  0.        ,  0.33594339,  0.87998172,\n",
       "          0.        ,  0.15471815,  0.31745437,  0.        ,  0.        ,\n",
       "          0.10921077,  0.30869508,  0.22523142,  0.34024026,  0.55409512,\n",
       "          0.13329609,  0.        ,  0.15657408,  0.47348147,  0.37042913,\n",
       "          0.28561595,  0.        ,  0.        ,  0.33256288,  0.        ,\n",
       "          0.3490657 ,  0.26252173,  0.        ,  0.0841548 ,  0.32973013,\n",
       "          0.27103953,  0.14721571,  0.00669399,  0.        ,  0.        ,\n",
       "          0.        ,  0.19035842,  0.19094793,  0.02110048,  0.0986995 ,\n",
       "          0.44429396,  0.47884669,  0.        ,  0.        ,  0.04633537,\n",
       "          0.        ,  0.34514753,  0.38951707,  0.3123518 ,  0.43946447,\n",
       "          0.14657641,  0.        ,  0.61380111,  0.20321083,  0.0454986 ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.17767498,\n",
       "          0.60047746,  0.23782672,  0.12977004,  0.139363  ,  0.        ,\n",
       "          0.01467553,  0.        ,  0.07972305,  0.02253252,  0.49826383,\n",
       "          0.2048386 ,  0.21737878,  0.33418883,  0.37482528,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.72193739,  0.14022482,\n",
       "          0.13241356,  0.25348382,  0.03705382,  0.22625163,  0.09206508,\n",
       "          0.        ,  0.19345447,  0.59067624,  0.        ,  0.53726191,\n",
       "          0.        ,  0.07407814,  0.        ,  0.31952777]]),\n",
       " 'layer1_a_14|float64#1#209': array([[ 0.        ,  0.        ,  0.00590198,  0.00843825,  0.11694914,\n",
       "          0.        ,  0.        ,  0.06824881,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.16889769,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.10357047,  0.18209093,  0.05848247,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.00148838,  0.        ,  0.        ,  0.08080505,  0.16979512,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.05304426,\n",
       "          0.13894387,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.016595  ,  0.00238197,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.08949261,  0.        ,  0.34568377,  0.        ,\n",
       "          0.        ,  0.14273711,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.07536979,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.11959184,  0.        ,  0.1407516 ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.12975923,  0.        ,  0.        ,  0.09869747,\n",
       "          0.        ,  0.01337356,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.00101685,  0.00700715,\n",
       "          0.        ,  0.        ,  0.        ,  0.25100668,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.21981414,  0.10078994,  0.        ,\n",
       "          0.        ,  0.06425827,  0.        ,  0.0100795 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.05441355,  0.        ,  0.09519779,\n",
       "          0.07266565,  0.        ,  0.        ,  0.        ,  0.06095118,\n",
       "          0.1487481 ,  0.        ,  0.        ,  0.        ,  0.02853034,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.01547111,  0.03969878,\n",
       "          0.        ,  0.26939508,  0.        ,  0.        ,  0.2774993 ,\n",
       "          0.        ,  0.        ,  0.        ,  0.04071111,  0.        ,\n",
       "          0.09427208,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]]),\n",
       " 'layer1_a_15|float64#1#209': array([[ 0.83133377,  0.65627676,  1.52560239,  0.34379543,  0.54944611,\n",
       "          0.28992485,  0.2621105 ,  0.91423091,  0.88478478,  0.7198443 ,\n",
       "          0.64207526,  1.07570442,  0.71717052,  1.7924223 ,  1.14937306,\n",
       "          1.17094921,  0.30846221,  0.99560862,  0.89041389,  0.46260498,\n",
       "          0.97201695,  0.60393734,  1.30992403,  0.91097781,  0.93144269,\n",
       "          0.28732122,  0.55184036,  1.59579417,  0.58561655,  0.81001294,\n",
       "          0.63248028,  1.43995395,  0.35606811,  1.274277  ,  0.79916493,\n",
       "          0.53746279,  0.88814058,  1.67804627,  0.0698135 ,  0.91645734,\n",
       "          0.21406991,  0.40344041,  1.14727348,  1.190074  ,  0.70261703,\n",
       "          0.49922587,  0.84932925,  0.73350889,  0.24792714,  0.99138241,\n",
       "          1.32441406,  0.76534816,  0.87396988,  1.35465267,  0.74302233,\n",
       "          0.54335487,  0.82859023,  1.38878136,  0.32849634,  0.6847508 ,\n",
       "          0.66057483,  0.95266533,  0.85334673,  0.52973371,  1.54155593,\n",
       "          0.6703309 ,  0.91530943,  0.92075741,  0.91144577,  0.9695826 ,\n",
       "          1.10769153,  0.72398238,  0.20441696,  0.82309278,  0.90360181,\n",
       "          1.14237707,  0.83010948,  0.66514831,  0.55938648,  0.33568141,\n",
       "          0.14740557,  1.13752803,  0.66097158,  0.30497761,  0.85926946,\n",
       "          1.00667559,  1.04861842,  0.41160936,  0.80093284,  0.21554104,\n",
       "          1.08985999,  1.06837615,  0.72954055,  0.81706853,  0.60621697,\n",
       "          1.7157296 ,  0.94639645,  0.58889284,  1.26164355,  0.99065103,\n",
       "          0.77515407,  0.89213927,  1.72980282,  0.32917261,  0.83558073,\n",
       "          1.03970794,  1.22131357,  1.0258495 ,  0.74848003,  1.29068665,\n",
       "          1.17950245,  0.90970816,  0.8262817 ,  1.0343844 ,  1.35408841,\n",
       "          0.84163056,  1.36505435,  0.32413896,  0.36201961,  1.25021719,\n",
       "          0.55297308,  1.17314019,  0.90271147,  0.98054454,  0.66089039,\n",
       "          0.46261612,  0.65721712,  0.71479023,  1.0534579 ,  0.78161176,\n",
       "          0.76577922,  0.58455983,  1.11932843,  0.73118586,  0.63714985,\n",
       "          1.01855725,  1.32886107,  0.36976467,  0.93119109,  0.61339853,\n",
       "          0.84663152,  1.08229165,  0.26809482,  0.45834564,  1.4592363 ,\n",
       "          0.9006408 ,  1.01551934,  0.90587641,  0.27420314,  0.24701853,\n",
       "          0.88561494,  0.46306811,  1.07922573,  0.76935017,  0.54701371,\n",
       "          0.86180543,  1.35381505,  0.49089749,  0.80833688,  0.85193995,\n",
       "          0.66897353,  0.46574925,  1.11137872,  1.2361413 ,  1.27811272,\n",
       "          0.45560855,  0.59383051,  1.16422025,  0.84887511,  1.00832947,\n",
       "          0.56249672,  0.59904365,  1.14210447,  1.05339649,  1.01505426,\n",
       "          1.03546857,  0.73074733,  0.69387349,  0.60351315,  0.53458148,\n",
       "          0.16497593,  1.05767491,  0.83693664,  1.28598285,  0.89039678,\n",
       "          1.17622222,  0.50602112,  0.76849564,  0.77021907,  0.71952355,\n",
       "          1.33912179,  0.72160791,  1.0862884 ,  1.58364512,  0.80214244,\n",
       "          0.18784398,  0.94032172,  0.52781613,  0.63042587,  0.53424051,\n",
       "          0.76066772,  0.93720847,  0.91432413,  0.40822094,  1.43497518,\n",
       "          0.18624083,  0.7745881 ,  0.21320557,  0.66661004]]),\n",
       " 'layer1_a_16|float64#1#209': array([[ 0.00672829,  0.09373865,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.19439321,  0.23225997,  0.13535843,\n",
       "          0.19264079,  0.        ,  0.07596365,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.0108219 ,  0.        ,  0.        ,\n",
       "          0.00982468,  0.02732656,  0.        ,  0.        ,  0.        ,\n",
       "          0.01001628,  0.11911996,  0.08726945,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.11759577,  0.        ,  0.        ,\n",
       "          0.2458893 ,  0.16083688,  0.3366603 ,  0.01060541,  0.        ,\n",
       "          0.01840842,  0.        ,  0.06148342,  0.055269  ,  0.098295  ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.08810576,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.02139424,  0.        ,  0.        ,  0.11646362,  0.0888208 ,\n",
       "          0.        ,  0.1987366 ,  0.        ,  0.        ,  0.        ,\n",
       "          0.02529518,  0.        ,  0.00801985,  0.        ,  0.        ,\n",
       "          0.        ,  0.122291  ,  0.05277702,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.10512917,\n",
       "          0.        ,  0.12743508,  0.        ,  0.15903733,  0.02639545,\n",
       "          0.01478912,  0.        ,  0.1302005 ,  0.32553814,  0.        ,\n",
       "          0.        ,  0.0645678 ,  0.        ,  0.03390306,  0.18193593,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.1300205 ,  0.        ,  0.10866469,  0.        ,  0.10784915,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.21117458,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.14995496,  0.0596039 ,  0.61037412,\n",
       "          0.02043129,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.00615131,  0.01702788,\n",
       "          0.        ,  0.        ,  0.        ,  0.16277285,  0.05248019,\n",
       "          0.        ,  0.        ,  0.17263765,  0.        ,  0.2578042 ,\n",
       "          0.        ,  0.25260772,  0.27621713,  0.03567768,  0.        ,\n",
       "          0.        ,  0.01385263,  0.13139781,  0.00774799,  0.        ,\n",
       "          0.02787617,  0.108117  ,  0.        ,  0.04976725,  0.21942942,\n",
       "          0.        ,  0.        ,  0.14053579,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.02688779,  0.        ,  0.        ,\n",
       "          0.03689323,  0.        ,  0.16672688,  0.        ,  0.        ,\n",
       "          0.01338779,  0.07570277,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.15025422,  0.        ,  0.        ,  0.        ,\n",
       "          0.00163427,  0.        ,  0.13419085,  0.        ,  0.        ,\n",
       "          0.16587274,  0.        ,  0.        ,  0.50638759,  0.        ,\n",
       "          0.13458801,  0.        ,  0.        ,  0.10463944,  0.12671251,\n",
       "          0.33845092,  0.        ,  0.25907045,  0.18789043,  0.        ,\n",
       "          0.        ,  0.        ,  0.10219631,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.16785507]]),\n",
       " 'layer1_a_17|float64#1#209': array([[ 0.        ,  0.        ,  0.22117202,  0.09489771,  0.        ,\n",
       "          0.12695427,  0.        ,  0.54042656,  0.02201706,  0.18984247,\n",
       "          0.12998821,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.61084784,  0.1302995 ,  0.        ,  0.01244858,  0.01157277,\n",
       "          0.19637324,  0.        ,  0.        ,  0.23273745,  0.10274976,\n",
       "          0.        ,  0.37444003,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.43841196,  0.38852742,  0.        ,  0.        ,\n",
       "          0.34809611,  0.        ,  0.        ,  0.09195443,  0.35720644,\n",
       "          0.        ,  0.        ,  0.14577785,  0.        ,  0.52561179,\n",
       "          0.30425831,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.16783477,  0.2760492 ,  0.40986864,  0.16425986,  0.40891352,\n",
       "          0.28078191,  0.        ,  0.59809907,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.08195667,  0.07765786,  0.36428243,  0.        ,  0.56690712,\n",
       "          0.        ,  0.        ,  0.        ,  0.39988117,  0.        ,\n",
       "          0.18636024,  0.        ,  0.23635502,  0.11812433,  0.10698308,\n",
       "          0.27048232,  0.        ,  0.24687782,  0.3210514 ,  0.54165226,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.12005212,  0.62598352,  0.        ,  0.21579297,  0.25177628,\n",
       "          0.        ,  0.02170749,  0.        ,  0.05363995,  0.00360891,\n",
       "          0.23694586,  0.        ,  0.15899435,  0.        ,  0.19134295,\n",
       "          0.01387848,  0.07583545,  0.17604437,  0.22222838,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.48220317,  0.00253971,\n",
       "          0.        ,  0.03059237,  0.        ,  0.        ,  0.37651712,\n",
       "          0.16764774,  0.16779493,  0.        ,  0.47962192,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.09638843,  0.04979232,\n",
       "          0.10981874,  0.16432739,  0.32853321,  0.        ,  0.10583405,\n",
       "          0.4711751 ,  0.24996946,  0.        ,  0.05624986,  0.17020912,\n",
       "          0.20048016,  0.38651744,  0.05003049,  0.01061638,  0.0454335 ,\n",
       "          0.22376853,  0.3615869 ,  0.1233403 ,  0.09227251,  0.23220869,\n",
       "          0.34527966,  0.        ,  0.        ,  0.74107195,  0.        ,\n",
       "          0.11225199,  0.11595272,  0.11532899,  0.17246128,  0.        ,\n",
       "          0.        ,  0.24979363,  0.0740749 ,  0.78246155,  0.        ,\n",
       "          0.01729757,  0.25002845,  0.        ,  0.11326679,  0.20206043,\n",
       "          0.20038256,  0.31221785,  0.21968172,  0.05624165,  0.07013049,\n",
       "          0.11668139,  0.        ,  0.18973147,  0.        ,  0.18090633,\n",
       "          0.69981729,  0.        ,  0.17788446,  0.        ,  0.        ,\n",
       "          0.00117392,  0.12123094,  0.59462587,  0.60727687,  0.43450392,\n",
       "          0.23559226,  0.32939852,  0.32063133,  0.04208375,  0.25358717,\n",
       "          0.        ,  0.04617831,  0.40236178,  0.        ,  0.45791413,\n",
       "          0.16446424,  0.37418907,  0.29306129,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.10377998,  0.        ]]),\n",
       " 'layer1_a_18|float64#1#209': array([[ 0.        ,  0.03190587,  0.        ,  0.38110672,  0.42028903,\n",
       "          0.34239644,  0.24571232,  0.03029303,  0.        ,  0.1989932 ,\n",
       "          0.        ,  0.58198384,  0.2363427 ,  0.67059126,  0.19028862,\n",
       "          0.30414339,  0.26167186,  0.7171884 ,  0.06472568,  0.28031337,\n",
       "          0.38553216,  0.29686314,  0.32392586,  0.17243205,  0.10679204,\n",
       "          0.2630481 ,  0.47137341,  0.11833585,  0.32000902,  0.61077109,\n",
       "          0.33013562,  0.        ,  0.21392206,  0.0975831 ,  0.66490177,\n",
       "          0.20796349,  0.        ,  0.        ,  0.        ,  0.14992184,\n",
       "          0.57335137,  0.29000222,  0.83088577,  0.08786569,  0.0609668 ,\n",
       "          0.1546315 ,  0.52356415,  0.        ,  0.80894998,  0.        ,\n",
       "          0.18158218,  0.03015984,  0.22538968,  0.        ,  0.18473268,\n",
       "          0.08301654,  0.1264062 ,  0.        ,  0.15662097,  0.19727361,\n",
       "          0.        ,  0.64214168,  0.08311053,  0.2802105 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.41294092,  0.        ,  0.25718419,  0.        ,  0.        ,\n",
       "          0.10879206,  0.36483406,  0.18066727,  0.51131464,  0.2770363 ,\n",
       "          0.61787259,  0.        ,  0.23837565,  0.        ,  0.30653954,\n",
       "          0.3317946 ,  0.34436233,  0.19590545,  0.08395555,  0.42329485,\n",
       "          0.05408084,  0.        ,  0.46814984,  0.13962823,  0.        ,\n",
       "          0.14995248,  0.2197339 ,  0.35100591,  0.22041084,  0.43992962,\n",
       "          0.16827137,  0.15032492,  0.45134518,  0.        ,  0.        ,\n",
       "          0.18761918,  0.        ,  0.27434043,  0.19166727,  0.10988243,\n",
       "          0.31263242,  0.08008131,  0.41901106,  0.1629733 ,  0.10749898,\n",
       "          0.26710989,  0.03043694,  0.        ,  0.25383853,  0.        ,\n",
       "          0.        ,  0.        ,  0.18111805,  0.25170965,  0.        ,\n",
       "          0.5162191 ,  0.07970496,  0.04880494,  0.16466043,  0.        ,\n",
       "          0.17098365,  0.34152936,  0.        ,  0.        ,  0.11371742,\n",
       "          0.        ,  0.2067624 ,  0.        ,  0.        ,  0.        ,\n",
       "          0.10268291,  0.        ,  0.16763985,  0.42206715,  0.        ,\n",
       "          0.1429479 ,  0.04565459,  0.28282502,  0.10805039,  0.03253228,\n",
       "          0.22118056,  0.37319051,  0.22412641,  0.        ,  0.32979683,\n",
       "          0.31205886,  0.47782455,  0.29605406,  0.        ,  0.18450274,\n",
       "          0.33524309,  0.14601334,  0.14778699,  0.        ,  0.30569679,\n",
       "          0.        ,  0.        ,  0.        ,  0.14207322,  0.19604032,\n",
       "          0.        ,  0.10657519,  0.        ,  0.        ,  0.        ,\n",
       "          0.25918368,  0.        ,  0.        ,  0.29829953,  0.05498185,\n",
       "          0.        ,  0.        ,  0.49743821,  0.        ,  0.21331259,\n",
       "          0.18944771,  0.17898064,  0.18014446,  0.13778086,  0.27480587,\n",
       "          0.34748175,  0.        ,  0.09478389,  0.09694027,  0.70636135,\n",
       "          0.        ,  0.21275783,  0.        ,  0.15790562,  0.3616763 ,\n",
       "          0.        ,  0.54936466,  0.        ,  0.9070825 ,  0.        ,\n",
       "          0.17026873,  0.18634333,  0.18820044,  0.17551744]]),\n",
       " 'layer1_a_19|float64#1#209': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.01537041,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.03590379,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.06374886,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.01663765,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.09600227,  0.        ,  0.07156221,  0.        ]]),\n",
       " 'layer1_a_1|float64#1#209': array([[ 0.07196659,  0.29407045,  0.52046231,  0.09343685,  0.12976176,\n",
       "          0.38780812,  0.18236846,  0.45314931,  0.77577132,  0.37616874,\n",
       "          0.79625066,  0.94278434,  0.64641115,  0.50456989,  0.39602468,\n",
       "          1.52029335,  0.98990611,  0.80218367,  0.43561214,  0.1661862 ,\n",
       "          0.4593066 ,  0.1826932 ,  0.83601041,  1.02238284,  1.00384115,\n",
       "          0.02618883,  0.47903721,  0.24610057,  0.30652524,  0.20952271,\n",
       "          0.61139564,  0.72075437,  0.84001132,  0.86068477,  0.48137024,\n",
       "          0.34693258,  0.40182512,  0.57375818,  0.29320029,  0.72107656,\n",
       "          0.25508334,  0.15430004,  1.08578943,  0.5299248 ,  1.2342094 ,\n",
       "          0.52354935,  0.62431598,  0.59566127,  0.44361809,  0.54878411,\n",
       "          0.98349274,  0.78303661,  0.48811317,  0.85929427,  0.61873208,\n",
       "          0.8868635 ,  0.653358  ,  1.19513315,  0.05363451,  0.42779078,\n",
       "          0.13744964,  0.52990531,  0.7473251 ,  0.54570133,  0.84465617,\n",
       "          0.49949149,  0.45775946,  0.39543273,  0.85162641,  0.64764261,\n",
       "          0.71447862,  0.41159326,  0.44088036,  0.47932097,  0.33933293,\n",
       "          0.73181295,  0.30130291,  0.85463396,  0.33317304,  0.13875898,\n",
       "          0.59172132,  0.60271534,  0.60889265,  0.28668958,  0.84625221,\n",
       "          0.40791358,  0.38223879,  0.33429663,  0.75822543,  0.3351154 ,\n",
       "          1.03159584,  0.59890076,  0.38083024,  0.67999414,  0.94871319,\n",
       "          0.82358217,  0.86425516,  0.54084589,  1.11525614,  0.59466229,\n",
       "          0.66876746,  0.6090743 ,  0.95875316,  0.49128029,  0.54323651,\n",
       "          0.46393662,  0.1598827 ,  0.63779469,  0.73320166,  0.53126057,\n",
       "          0.        ,  0.1406782 ,  0.72108913,  0.54965102,  0.74315474,\n",
       "          0.51543125,  0.        ,  0.35455636,  0.22133578,  0.83737307,\n",
       "          0.47758769,  1.0305362 ,  0.45044696,  0.6052133 ,  0.46105382,\n",
       "          0.41336959,  0.39583188,  0.40837582,  0.78183285,  0.49988762,\n",
       "          0.61249673,  0.4366696 ,  0.71005132,  0.        ,  0.31487007,\n",
       "          0.7166231 ,  1.16993687,  0.37020344,  0.67934671,  0.38608553,\n",
       "          0.48130213,  0.96734381,  0.51856906,  0.17349366,  0.55036679,\n",
       "          0.68690653,  0.9927514 ,  0.96387036,  0.12827397,  0.80725138,\n",
       "          1.04366419,  0.1208362 ,  0.59558513,  0.59393617,  0.16961722,\n",
       "          0.6817053 ,  0.63886483,  0.42147361,  0.57059455,  0.57600106,\n",
       "          0.52622958,  0.74304877,  0.79573787,  0.39513606,  0.51203288,\n",
       "          0.29033609,  0.28544348,  0.66604337,  0.67145651,  0.98914433,\n",
       "          0.62521489,  0.9712897 ,  0.71723212,  0.88951852,  0.96921126,\n",
       "          0.79147917,  0.5430999 ,  0.60348454,  0.53938257,  0.39434214,\n",
       "          0.62696741,  0.51337238,  0.64077138,  0.64172545,  0.92473651,\n",
       "          0.72069816,  0.61771517,  1.21167462,  0.78619282,  0.61197561,\n",
       "          0.75235681,  0.86674125,  1.09672369,  0.81783745,  0.57459568,\n",
       "          0.04526916,  0.62401094,  0.6521791 ,  0.16242618,  0.66592201,\n",
       "          0.70285583,  1.36351314,  0.44046611,  0.15547192,  1.11479281,\n",
       "          0.        ,  0.38338442,  0.19791222,  0.15667228]]),\n",
       " 'layer1_a_20|float64#1#209': array([[ 0.22106564,  0.29806307,  0.36277627,  0.79432518,  0.        ,\n",
       "          0.21303467,  0.30037076,  0.35862768,  0.32694954,  0.23321599,\n",
       "          0.58306871,  0.41470942,  0.17315617,  0.58200089,  0.25510812,\n",
       "          0.06632162,  1.07666635,  0.37746968,  0.69373704,  0.48868906,\n",
       "          0.53195428,  0.38160741,  0.60289253,  0.85956284,  0.35298081,\n",
       "          0.03172976,  0.10511439,  0.23635399,  0.42637759,  0.21552384,\n",
       "          0.31614456,  0.25996465,  0.52779243,  0.19298174,  0.26496765,\n",
       "          0.54562194,  0.2012453 ,  0.43852927,  0.18529224,  0.20016862,\n",
       "          0.41020488,  0.07997671,  0.18996091,  0.11309572,  0.34672644,\n",
       "          0.34027561,  0.20713073,  0.15021822,  0.35031695,  0.29907392,\n",
       "          0.55703325,  0.18360622,  0.48143867,  0.48294195,  0.00421397,\n",
       "          0.37168913,  0.54294914,  0.93392443,  0.13177162,  0.45275481,\n",
       "          0.57139814,  0.31559912,  0.35846372,  0.72133544,  0.38361729,\n",
       "          0.60576002,  0.        ,  0.23490909,  0.26254961,  0.10773647,\n",
       "          0.62531806,  0.5605179 ,  0.18800986,  0.25554765,  0.05379028,\n",
       "          0.38835978,  0.10440333,  0.92154061,  0.58769622,  0.0761129 ,\n",
       "          0.42114652,  0.08372687,  0.17098392,  0.        ,  0.39341524,\n",
       "          0.3844387 ,  0.24929056,  0.55860063,  0.39620064,  0.24764605,\n",
       "          0.0574931 ,  0.66760559,  0.30719381,  0.43671268,  0.56888504,\n",
       "          0.5775482 ,  0.53037149,  0.23582378,  0.63551088,  0.        ,\n",
       "          0.32091037,  0.57855322,  0.27665168,  0.55868351,  0.34543793,\n",
       "          0.53737565,  0.11935169,  0.34938792,  0.46917257,  0.24021863,\n",
       "          0.19350478,  0.25620177,  0.52312634,  0.44337047,  0.6138976 ,\n",
       "          0.57390645,  0.        ,  0.03004548,  0.53032956,  0.10058714,\n",
       "          0.39974963,  0.71647739,  0.32783845,  0.60574758,  0.7197756 ,\n",
       "          0.43616007,  0.47057239,  0.302821  ,  0.92620332,  0.65331007,\n",
       "          0.3826339 ,  0.36575016,  0.19583374,  0.35709806,  0.31670582,\n",
       "          0.37960329,  0.64955354,  0.4247049 ,  0.61769481,  0.29231428,\n",
       "          0.        ,  0.23285424,  0.86082383,  0.08219663,  0.05285854,\n",
       "          0.50223945,  0.26961703,  0.35241636,  0.        ,  0.24104708,\n",
       "          0.54929812,  0.17337494,  0.23963238,  0.39639003,  0.13238247,\n",
       "          0.67421081,  0.37914352,  0.        ,  0.26781116,  0.49913038,\n",
       "          0.22326858,  0.43062707,  0.62107208,  0.34128618,  0.16434018,\n",
       "          0.21430285,  0.20525489,  0.54160501,  0.        ,  0.41369533,\n",
       "          0.41391993,  0.64436724,  0.24959964,  0.46403685,  0.66761312,\n",
       "          0.37171196,  0.19176787,  0.35761925,  0.        ,  0.06891575,\n",
       "          0.17454868,  0.76645202,  0.18076487,  0.53408905,  0.66826802,\n",
       "          0.39273406,  0.40478042,  0.44285532,  0.12229116,  0.        ,\n",
       "          0.8070593 ,  0.62957383,  0.        ,  0.19923347,  0.31477637,\n",
       "          0.14934108,  0.25586178,  0.3167632 ,  0.80023384,  0.13454179,\n",
       "          0.95467099,  0.15141039,  0.69146044,  0.05744882,  0.44582057,\n",
       "          0.23580397,  0.58466116,  0.1014699 ,  0.02667976]]),\n",
       " 'layer1_a_2|float64#1#209': array([[ 0.        ,  0.        ,  0.00957635,  0.26064608,  0.22727679,\n",
       "          0.        ,  0.20644152,  0.023468  ,  0.        ,  0.15281748,\n",
       "          0.3993699 ,  0.        ,  0.        ,  0.        ,  0.0927942 ,\n",
       "          0.10913644,  0.26262416,  0.07218962,  0.        ,  0.10506336,\n",
       "          0.00233624,  0.        ,  0.04562826,  0.24098263,  0.        ,\n",
       "          0.22041196,  0.27309051,  0.12585681,  0.1276694 ,  0.19010971,\n",
       "          0.        ,  0.        ,  0.        ,  0.01909642,  0.        ,\n",
       "          0.        ,  0.        ,  0.29499537,  0.        ,  0.10746498,\n",
       "          0.        ,  0.        ,  0.03102186,  0.        ,  0.14095384,\n",
       "          0.05430747,  0.        ,  0.        ,  0.25863234,  0.        ,\n",
       "          0.08482296,  0.24518713,  0.12597076,  0.17713473,  0.08361188,\n",
       "          0.38907027,  0.02343915,  0.00955169,  0.        ,  0.        ,\n",
       "          0.12975538,  0.06543914,  0.1347265 ,  0.12234926,  0.        ,\n",
       "          0.        ,  0.        ,  0.33927323,  0.22656871,  0.19534413,\n",
       "          0.        ,  0.        ,  0.03597992,  0.        ,  0.        ,\n",
       "          0.        ,  0.40644024,  0.        ,  0.06223879,  0.08062241,\n",
       "          0.52408606,  0.03262479,  0.18162363,  0.        ,  0.        ,\n",
       "          0.        ,  0.1729493 ,  0.27287638,  0.        ,  0.12495941,\n",
       "          0.        ,  0.08124107,  0.31610185,  0.03022362,  0.0072017 ,\n",
       "          0.00120292,  0.01628199,  0.        ,  0.08289765,  0.        ,\n",
       "          0.        ,  0.15296285,  0.        ,  0.05544797,  0.        ,\n",
       "          0.        ,  0.31070682,  0.36535877,  0.24226191,  0.06981761,\n",
       "          0.        ,  0.        ,  0.24101674,  0.29473091,  0.04790253,\n",
       "          0.20611196,  0.        ,  0.03324984,  0.65914898,  0.09004469,\n",
       "          0.00450909,  0.37387545,  0.39654278,  0.33970391,  0.30663818,\n",
       "          0.        ,  0.0264801 ,  0.14254313,  0.        ,  0.        ,\n",
       "          0.05729272,  0.05455123,  0.08159636,  0.        ,  0.        ,\n",
       "          0.        ,  0.05978824,  0.26313095,  0.46811422,  0.06289352,\n",
       "          0.2036283 ,  0.        ,  0.05048403,  0.01307299,  0.2709129 ,\n",
       "          0.        ,  0.        ,  0.23344989,  0.03076833,  0.19029187,\n",
       "          0.3529348 ,  0.        ,  0.        ,  0.05684448,  0.        ,\n",
       "          0.13769118,  0.        ,  0.1603485 ,  0.        ,  0.        ,\n",
       "          0.04065157,  0.08283209,  0.        ,  0.29212223,  0.21301531,\n",
       "          0.03559586,  0.12697424,  0.10802191,  0.        ,  0.1173077 ,\n",
       "          0.        ,  0.03360695,  0.15039963,  0.07912801,  0.08842305,\n",
       "          0.        ,  0.13039446,  0.        ,  0.        ,  0.07815427,\n",
       "          0.42003489,  0.48730052,  0.        ,  0.        ,  0.02544625,\n",
       "          0.        ,  0.10711859,  0.21676901,  0.00617826,  0.26536556,\n",
       "          0.30274614,  0.12435918,  0.        ,  0.04955919,  0.        ,\n",
       "          0.        ,  0.07851476,  0.26189483,  0.21659767,  0.        ,\n",
       "          0.17880672,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.06601297,  0.        ,  0.17248276,  0.        ]]),\n",
       " 'layer1_a_3|float64#1#209': array([[ 0.31849711,  0.11535033,  0.33366745,  0.        ,  0.        ,\n",
       "          0.09291177,  0.        ,  0.27459426,  0.38294807,  0.25688579,\n",
       "          0.02899711,  0.46008252,  0.16013369,  0.07453879,  0.        ,\n",
       "          0.19994799,  0.18239019,  0.        ,  0.34708145,  0.3484089 ,\n",
       "          0.14433235,  0.        ,  0.32939807,  0.72065606,  0.52702546,\n",
       "          0.        ,  0.        ,  0.45171709,  0.3681859 ,  0.        ,\n",
       "          0.        ,  0.78794998,  0.        ,  0.21064369,  0.        ,\n",
       "          0.21286211,  0.46918269,  0.        ,  0.15780127,  0.32307617,\n",
       "          0.        ,  0.        ,  0.24721355,  0.        ,  0.27373066,\n",
       "          0.14128058,  0.45637443,  0.20044389,  0.        ,  0.05330701,\n",
       "          0.49026754,  0.38443221,  0.51549295,  0.37605378,  0.11030223,\n",
       "          0.        ,  0.44544831,  0.97468194,  0.        ,  0.04511586,\n",
       "          0.2108569 ,  0.        ,  0.06755481,  0.08224232,  0.33003652,\n",
       "          0.14448662,  0.45579994,  0.        ,  0.33643142,  0.17470583,\n",
       "          0.03916684,  0.26373236,  0.        ,  0.65811549,  0.24157573,\n",
       "          0.16916752,  0.02248881,  0.5503516 ,  0.        ,  0.01957717,\n",
       "          0.        ,  0.21388189,  0.37047562,  0.08478007,  0.20090789,\n",
       "          0.0255752 ,  0.11909851,  0.0333607 ,  0.        ,  0.        ,\n",
       "          0.27210575,  0.77046435,  0.        ,  0.35858902,  0.        ,\n",
       "          0.44662325,  0.1284529 ,  0.05638911,  0.48830909,  0.        ,\n",
       "          0.        ,  0.46518348,  0.43341956,  0.17535958,  0.43453613,\n",
       "          0.2679741 ,  0.3712071 ,  0.09214756,  0.02549157,  0.        ,\n",
       "          0.        ,  0.21649405,  0.        ,  0.41675065,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.15320838,\n",
       "          0.        ,  0.2831381 ,  0.13512639,  0.54211266,  0.        ,\n",
       "          0.40645932,  0.16023798,  0.0165956 ,  0.16986333,  0.20597123,\n",
       "          0.32038485,  0.47304197,  0.48822276,  0.        ,  0.23931247,\n",
       "          0.39740771,  0.79836527,  0.        ,  0.15299569,  0.76785897,\n",
       "          0.34085275,  0.40927653,  0.10756043,  0.        ,  0.38440824,\n",
       "          0.28347727,  0.29644852,  0.21283333,  0.        ,  0.05888027,\n",
       "          0.02910461,  0.15385673,  0.38138171,  0.05539036,  0.        ,\n",
       "          0.41628758,  0.15178701,  0.07417707,  0.12319841,  0.27769191,\n",
       "          0.        ,  0.28877382,  0.38826617,  0.36714832,  0.11624659,\n",
       "          0.0513742 ,  0.30189013,  0.        ,  0.37488555,  0.03536423,\n",
       "          0.18811933,  0.05297805,  0.28654421,  0.09481881,  0.38696418,\n",
       "          0.7200714 ,  0.55365254,  0.27125959,  0.19035061,  0.53209379,\n",
       "          0.39085832,  0.33425963,  0.23970466,  0.12034683,  0.        ,\n",
       "          0.13611458,  0.        ,  0.66523507,  0.20081126,  0.37274979,\n",
       "          0.38434849,  0.        ,  0.39323754,  0.2345563 ,  0.02299587,\n",
       "          0.36949321,  0.17275791,  0.47086813,  0.11268351,  0.03121949,\n",
       "          0.33737626,  0.19672167,  0.17265138,  0.        ,  0.45826748,\n",
       "          0.05141606,  0.01670034,  0.09194017,  0.11541118]]),\n",
       " 'layer1_a_4|float64#1#209': array([[ 0.09981446,  0.22039521,  0.        ,  0.        ,  0.00603675,\n",
       "          0.02578662,  0.07844575,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.02761062,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.02699135,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.07277169,  0.        ,  0.        ,\n",
       "          0.19309432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.26988554,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.00441186,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.03766686,  0.        ,  0.05868942,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.17791285,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.02764534,  0.        ,\n",
       "          0.        ,  0.        ,  0.00357783,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.39818155,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.46944876,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.01464834,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.34816839,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.26812039,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.12075422,  0.        ,  0.02945214,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.12074857,  0.        ,\n",
       "          0.        ,  0.        ,  0.20543208,  0.2422394 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]]),\n",
       " 'layer1_a_5|float64#1#209': array([[ 0.0483741 ,  0.31738024,  0.44989283,  0.17481553,  0.06804298,\n",
       "          0.22967648,  0.27153553,  0.63044213,  0.07132538,  0.33543659,\n",
       "          0.35835533,  0.19864544,  0.0373958 ,  0.15691013,  0.18567787,\n",
       "          0.49117353,  0.35293573,  0.76939313,  0.43185276,  0.15562907,\n",
       "          0.36725308,  0.20218779,  0.48869578,  0.65803069,  0.39844742,\n",
       "          0.02379195,  0.24424753,  0.        ,  0.27286152,  0.76651506,\n",
       "          0.        ,  0.57062652,  0.21297118,  0.48710584,  0.35312608,\n",
       "          0.03042124,  0.10461678,  0.49977773,  0.34827285,  0.27417526,\n",
       "          0.34686958,  0.03122739,  0.51449426,  0.52327297,  0.34144379,\n",
       "          0.41052757,  0.33556147,  0.03546367,  0.37127743,  0.18246754,\n",
       "          0.61272967,  0.44852174,  0.45015106,  0.31933387,  0.06472384,\n",
       "          0.50182171,  0.1776984 ,  1.10278837,  0.01909997,  0.21160764,\n",
       "          0.        ,  0.01966133,  0.6568667 ,  0.66140146,  0.46157482,\n",
       "          0.10603971,  0.70669702,  0.28105227,  0.37101856,  0.54880566,\n",
       "          0.56227659,  0.14620179,  0.11884193,  0.92935702,  0.09703409,\n",
       "          0.44147008,  0.26867622,  0.50779257,  0.51236613,  0.11621321,\n",
       "          0.4331748 ,  0.51504244,  0.31186789,  0.22133784,  0.46465288,\n",
       "          0.39956297,  0.44514928,  0.38777599,  0.24830943,  0.        ,\n",
       "          0.6178354 ,  0.50287291,  0.26495137,  0.30597594,  0.24931156,\n",
       "          0.52960224,  0.4919782 ,  0.34622875,  0.46208044,  0.32697529,\n",
       "          0.30038751,  0.61303588,  0.40640765,  0.51991308,  0.69320428,\n",
       "          0.62837343,  0.47972609,  0.24202658,  0.24800525,  0.22909961,\n",
       "          0.08330973,  0.33132164,  0.12290503,  0.05942964,  0.49986527,\n",
       "          0.01380946,  0.        ,  0.19333533,  0.19107659,  0.05474489,\n",
       "          0.31728779,  0.17418077,  0.14688893,  0.32372111,  0.22413392,\n",
       "          0.42752718,  0.3676934 ,  0.21966696,  0.26876632,  0.        ,\n",
       "          0.4345516 ,  0.79428416,  0.40444744,  0.12197179,  0.13597335,\n",
       "          0.67886089,  0.83477428,  0.49955093,  0.25137726,  0.40527827,\n",
       "          0.10741433,  0.37489695,  0.36331554,  0.        ,  0.37541315,\n",
       "          0.40594263,  0.5654695 ,  0.42395271,  0.08447754,  0.67690677,\n",
       "          0.50922716,  0.4365629 ,  0.50639701,  0.30813012,  0.        ,\n",
       "          0.56558668,  0.00660985,  0.36294892,  0.67327314,  0.25856566,\n",
       "          0.5005692 ,  0.39206621,  0.40811983,  0.        ,  0.33989739,\n",
       "          0.42825653,  1.15894333,  0.18556109,  0.18157263,  0.24949007,\n",
       "          0.40512858,  0.        ,  0.3146537 ,  0.38642127,  0.15046303,\n",
       "          0.41891404,  0.37235866,  0.29173193,  0.48054272,  0.50780821,\n",
       "          0.39843964,  0.49110797,  0.47787037,  0.12586362,  0.46067461,\n",
       "          0.18859059,  0.33117292,  0.73165132,  0.49260577,  0.43706903,\n",
       "          0.66538958,  0.52022198,  0.4557195 ,  0.43629043,  0.46408485,\n",
       "          0.12400411,  0.12851372,  0.66376374,  0.09390483,  0.23447668,\n",
       "          0.51983818,  0.4600428 ,  0.32209906,  0.33721747,  0.50821991,\n",
       "          0.25049175,  0.56641539,  0.06050201,  0.20100646]]),\n",
       " 'layer1_a_6|float64#1#209': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.05715511,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.05164787,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.02821124,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.02031739,  0.        ,  0.        ,  0.        ]]),\n",
       " 'layer1_a_7|float64#1#209': array([[ 0.31311657,  0.1763901 ,  0.67792316,  0.        ,  0.02686862,\n",
       "          0.13553981,  0.15630992,  0.78413996,  0.95909818,  0.        ,\n",
       "          0.1804422 ,  0.51679358,  0.26692053,  0.81695614,  0.32418623,\n",
       "          0.        ,  0.        ,  0.21773184,  0.3465791 ,  0.51407846,\n",
       "          0.20827412,  0.        ,  0.77829841,  0.19499743,  0.65533504,\n",
       "          0.33837767,  0.27279395,  1.06710669,  0.278415  ,  0.08253529,\n",
       "          0.39239955,  0.32443098,  0.30404023,  0.40668458,  0.59140649,\n",
       "          0.27013823,  0.37173731,  1.00705474,  0.01532624,  0.47707232,\n",
       "          0.        ,  0.26252088,  0.        ,  0.46804349,  0.70686321,\n",
       "          0.        ,  0.35641863,  0.43153996,  0.12727027,  0.15295161,\n",
       "          0.55496119,  0.27290093,  0.32372482,  0.43477025,  0.41361904,\n",
       "          0.43487733,  0.67808064,  0.69682614,  0.05894338,  0.51540751,\n",
       "          0.30068977,  0.26957048,  0.27352017,  0.09896405,  0.64067802,\n",
       "          0.28585282,  0.17334989,  0.35766413,  0.21315317,  0.94313486,\n",
       "          0.27848511,  0.91495705,  0.52436191,  0.26743736,  0.34408826,\n",
       "          0.54071701,  0.0543367 ,  0.26565959,  0.30596966,  0.17460802,\n",
       "          0.        ,  0.16265517,  0.24869224,  0.00263905,  0.62167625,\n",
       "          0.72271415,  0.27419178,  0.05287375,  0.45939746,  0.09413799,\n",
       "          0.36434844,  0.40746368,  0.        ,  0.32736607,  0.26120682,\n",
       "          0.50838056,  0.31561745,  0.26580208,  0.30108112,  0.34464242,\n",
       "          0.41026907,  0.26593905,  0.67943767,  0.        ,  0.42666142,\n",
       "          0.60229216,  0.5109519 ,  0.31227492,  0.32822873,  0.71077105,\n",
       "          0.14125736,  0.69639965,  0.61412404,  0.44201819,  0.85609549,\n",
       "          0.02447756,  0.15250518,  0.36569003,  0.13224667,  0.69263777,\n",
       "          0.04514115,  0.49064955,  0.36299905,  0.30024184,  0.36122552,\n",
       "          0.40999839,  0.45568616,  0.36410465,  0.60624308,  0.51177206,\n",
       "          0.3682598 ,  0.12548657,  0.25171182,  0.37504145,  0.54360204,\n",
       "          0.38276508,  0.52643062,  0.09717852,  0.17537005,  0.07155664,\n",
       "          0.50522214,  0.79149725,  0.15679327,  0.07749553,  0.66056664,\n",
       "          0.43641528,  0.41390746,  0.51993011,  0.41770815,  0.10032929,\n",
       "          0.48074348,  0.74713466,  0.28380438,  0.69167772,  0.36388097,\n",
       "          0.39611588,  0.6637875 ,  0.3099639 ,  0.05153951,  0.5070292 ,\n",
       "          0.39813907,  0.35492292,  0.32611713,  0.45351663,  0.53984566,\n",
       "          0.31339919,  0.66894013,  0.40008348,  0.61582594,  0.46324359,\n",
       "          0.41318678,  0.34322218,  0.58000249,  0.55505035,  1.1400543 ,\n",
       "          0.61933216,  0.86507514,  0.15376502,  0.20933349,  0.01294004,\n",
       "          0.50049375,  0.73790768,  0.08724664,  0.65858886,  0.31773732,\n",
       "          0.65329264,  0.25530123,  0.46547812,  0.53633317,  0.53625613,\n",
       "          0.31612209,  0.45830644,  0.12129412,  0.71006675,  0.33654056,\n",
       "          0.31664264,  0.43772773,  0.        ,  0.34929506,  0.22895231,\n",
       "          0.30804152,  0.25713261,  0.21767653,  0.        ,  0.67572736,\n",
       "          0.        ,  0.40125072,  0.        ,  0.42169783]]),\n",
       " 'layer1_a_8|float64#1#209': array([[ 0.47539513,  0.76013955,  0.98665952,  0.30847086,  0.73195747,\n",
       "          0.23422295,  0.26554716,  0.32090334,  0.78122003,  0.39599647,\n",
       "          0.76956816,  0.60736873,  0.43283154,  0.83576778,  0.7018862 ,\n",
       "          0.76134587,  0.49986621,  0.96477683,  0.78942187,  0.15342331,\n",
       "          0.64342662,  0.70426359,  1.1471659 ,  1.43646067,  0.66798823,\n",
       "          0.2294169 ,  0.87662168,  1.17984153,  1.26642998,  0.67750945,\n",
       "          0.38764819,  0.8122754 ,  0.17130466,  0.98173824,  0.33476137,\n",
       "          0.50383554,  0.57495845,  0.79705339,  0.03845899,  0.73372323,\n",
       "          0.51689094,  0.3614481 ,  0.69576513,  1.1952658 ,  0.09151173,\n",
       "          0.42623612,  0.79938657,  0.52726592,  0.34828522,  0.8031831 ,\n",
       "          0.95340169,  0.68222524,  0.75909852,  0.83134289,  0.84756471,\n",
       "          0.78441382,  0.61980208,  1.2950703 ,  0.26340053,  0.28958139,\n",
       "          0.71578649,  0.31727034,  0.5031802 ,  0.37862017,  1.42690959,\n",
       "          0.35630269,  0.86914063,  0.48618625,  0.94837996,  0.65922399,\n",
       "          0.65149116,  0.48217484,  0.27644433,  0.65634905,  1.37260462,\n",
       "          0.78966689,  0.32772619,  0.        ,  0.57147847,  0.18755468,\n",
       "          0.19184298,  1.02245365,  0.19817491,  0.79876064,  0.9101026 ,\n",
       "          0.62184181,  0.32565811,  0.25484496,  0.76655908,  0.58155519,\n",
       "          0.87977386,  0.68543931,  0.51234207,  0.53310733,  0.53869689,\n",
       "          0.97158785,  0.75831857,  0.67734838,  0.76810264,  0.62551783,\n",
       "          0.43241221,  0.72350008,  0.95484574,  0.47595507,  0.41437906,\n",
       "          0.94639695,  0.79366252,  0.67619571,  0.40911934,  0.84295412,\n",
       "          0.8921527 ,  0.94630969,  0.34814827,  1.26137025,  0.89510594,\n",
       "          0.83660115,  1.16967356,  0.41066157,  0.20673251,  1.13518533,\n",
       "          0.51930295,  0.82193182,  0.92731621,  1.13493589,  0.61579318,\n",
       "          0.17356118,  0.36814581,  0.98950285,  0.9098337 ,  0.39382021,\n",
       "          0.46562658,  0.51205351,  0.64111054,  0.80881299,  0.33559301,\n",
       "          1.20141824,  0.90621101,  1.05026693,  0.66130821,  0.44453176,\n",
       "          0.88041676,  0.66230058,  0.73536999,  0.18225168,  0.77488507,\n",
       "          0.61381199,  0.73965564,  0.9173051 ,  0.5179435 ,  0.53128675,\n",
       "          0.71383563,  0.46650931,  0.71490933,  0.40067648,  0.16019019,\n",
       "          0.82318354,  0.58875503,  0.28810923,  0.89880323,  0.5655638 ,\n",
       "          0.12651635,  0.63957563,  0.86107999,  1.24339903,  0.91357492,\n",
       "          0.34275844,  0.74708853,  1.06816392,  0.3173209 ,  0.96154046,\n",
       "          0.61482292,  0.85062689,  1.12534359,  0.80175187,  1.03590847,\n",
       "          0.2431885 ,  0.76433382,  0.47088536,  0.47565896,  0.48096846,\n",
       "          0.50741821,  0.46659116,  0.52307899,  0.70823196,  0.13428544,\n",
       "          0.72835075,  0.38934536,  0.58632246,  0.6456514 ,  0.51811231,\n",
       "          0.8343902 ,  0.87118191,  0.93028001,  1.25974025,  0.44343858,\n",
       "          0.        ,  0.77632791,  0.771502  ,  0.65143726,  0.58644135,\n",
       "          0.73335248,  0.35102532,  0.99524673,  0.66159265,  1.47220305,\n",
       "          0.33531343,  0.6482584 ,  0.20698683,  0.34818006]]),\n",
       " 'layer1_a_9|float64#1#209': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.02109813,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.05420035,  0.        ,  0.        ,  0.        ,\n",
       "          0.07818026,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.16464252,  0.        ,\n",
       "          0.        ,  0.01612992,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.36728855,  0.        ,  0.        ,\n",
       "          0.0843631 ,  0.        ,  0.        ,  0.00606271,  0.        ,\n",
       "          0.        ,  0.        ,  0.0237218 ,  0.        ,  0.14518376,\n",
       "          0.11556238,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.03816428,  0.        ,  0.        ,  0.        ,  0.20569996,\n",
       "          0.24819611,  0.14220162,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.00559291,  0.        ,  0.        ,  0.08746831,\n",
       "          0.11798009,  0.25008636,  0.        ,  0.        ,  0.23048646,\n",
       "          0.        ,  0.        ,  0.04710043,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.16228892,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.44362105,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.08531158,  0.        ,\n",
       "          0.3090906 ,  0.        ,  0.        ,  0.11337007,  0.03814192,\n",
       "          0.        ,  0.        ,  0.04120851,  0.01857221,  0.        ,\n",
       "          0.        ,  0.        ,  0.2171569 ,  0.        ,  0.        ,\n",
       "          0.        ,  0.23238266,  0.00830891,  0.        ,  0.        ,\n",
       "          0.        ,  0.09032255,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.17749186,  0.        ,  0.        ,\n",
       "          0.        ,  0.11613211,  0.        ,  0.        ,  0.        ,\n",
       "          0.04903449,  0.05484677,  0.03268959,  0.05645401,  0.22891806,\n",
       "          0.12583311,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.5937768 ,  0.28126491,  0.        ,  0.32954753,\n",
       "          0.19762428,  0.        ,  0.        ,  0.29386095,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.07293075,  0.        ,\n",
       "          0.03786459,  0.        ,  0.07423875,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.00233275,\n",
       "          0.10533995,  0.        ,  0.16409484,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.16699762,  0.26598819,  0.04177978,  0.        ,  0.        ,\n",
       "          0.        ,  0.08267413,  0.1741951 ,  0.46906544,  0.        ,\n",
       "          0.        ,  0.13220174,  0.02339793,  0.11629401,  0.        ,\n",
       "          0.12614057,  0.        ,  0.18345193,  0.        ,  0.        ,\n",
       "          0.        ,  0.48255072,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.05256885,  0.        ,  0.        ]])}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_dict = {}\n",
    "for i in key_list:\n",
    "    A_dict[i] = from_cache(endpoint=endpoint, key=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 209)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "A = np.squeeze(A)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(A.shape == (parameters['neurons']['layer'+str(layer-1)], parameters['dims']['train_set_x'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '209']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**REwork above code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 2\n",
    "epoch = 0\n",
    "# Use the following Redis command to ensure a pure string is return for the key\n",
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "search_results = []\n",
    "# Compile a list of all the activations\n",
    "for activation in range(1, parameters['neurons']['layer'+str(layer-1)]+1):\n",
    "    tmp = r.keys('layer'+str(layer-1)+'_a_'+str(activation)+'|*')\n",
    "    search_results.append(tmp)\n",
    "# Created an ordered list of activation data keys\n",
    "key_list = []\n",
    "for result in search_results:\n",
    "    key_list.append(result[0])\n",
    "# Create a dictionary of activation data\n",
    "A_dict = {}\n",
    "for data in key_list:\n",
    "    A_dict[data] = from_cache(endpoint=endpoint, key=data)\n",
    "# Number of Neuron Activations\n",
    "num_neurons = parameters['neurons']['layer'+str(layer-1)]\n",
    "# Create a numpy array of the results, depending on the number\n",
    "# of hidden units (a Matrix of Activations)\n",
    "A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "if num_neurons == 1:\n",
    "    # Single Neuron Activation\n",
    "    dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "    A = A.reshape(int(dims[0]), int(dims[1]))\n",
    "    assert(A.shape == (parameters['dims']['train_set_y'][0], parameters['dims']['train_set_y'][1]))\n",
    "else:\n",
    "    # Multiple Neuron Activcatoins\n",
    "    A = np.squeeze(A)\n",
    "    assert(A.shape == (parameters['neurons']['layer'+str(layer-1)], parameters['dims']['train_set_x'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 209)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 209)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 0\n",
    "epoch = 0\n",
    "# Use the following Redis command to ensure a pure string is return for the key\n",
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "search_results = []\n",
    "# Compile a list of all the activations\n",
    "for activation in range(1, parameters['neurons']['layer'+str(layer+1)]+1):\n",
    "    tmp = r.keys('layer'+str(layer+1)+'_dZ_'+str(activation)+'|*')\n",
    "    search_results.append(tmp)\n",
    "# Created an ordered list of activation data keys\n",
    "key_list = []\n",
    "for result in search_results:\n",
    "    key_list.append(result[0])\n",
    "\n",
    "# Create a dictionary of activation data\n",
    "dZ_dict = {}\n",
    "for data in key_list:\n",
    "    dZ_dict[data] = from_cache(endpoint=endpoint, key=data)\n",
    "# Number of Neuron Activations\n",
    "num_neurons = parameters['neurons']['layer'+str(layer+1)]\n",
    "# Create a numpy array of the results, depending on the number\n",
    "# of hidden units (a Matrix of Activations)\n",
    "dZ = np.array([arr.tolist() for arr in dZ_dict.values()])\n",
    "if num_neurons == 1:\n",
    "    # Single Neuron Activation\n",
    "    dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "    dZ = dZ.reshape(int(dims[0]), int(dims[1]))\n",
    "else:\n",
    "    # Multiple Neuron Activcatoins\n",
    "    dZ = np.squeeze(dZ)\n",
    "dZ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Results object stored on S3\n",
    "bucket = parameters['s3_bucket']\n",
    "content = s3_resource.Object(bucket, 'training_results/results.json')\n",
    "file = content.get()['Body'].read().decode('utf-8')\n",
    "json_content = json.loads(file)\n",
    "print(json_content)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
