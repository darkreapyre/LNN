{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Neural Network\n",
    "## Overview\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:500px;height:300;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries, Global and Event Variables\n",
    "\n",
    "The cell below imports all the packages that will be needed by the Lambda Function. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n",
    "- [boto3](https://pypi.python.org/pypi/boto3) is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.\n",
    "- [json](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax (although it is not a strict subset of JavaScript.\n",
    "- [os](https://docs.python.org/3/library/os.html) is a module the provides a portable way of using operating system dependent functionality. Particularly the  `environ` object is a nmapping object representing the environment.\n",
    "- [uuid](https://docs.python.org/2/library/uuid.html#uuid.uuid4) creates a unique, random ID.\n",
    "- The [io](https://docs.python.org/2/library/io.html) module provides the Python interfaces to stream handling.\n",
    "- The Python interface to the [Redis](https://pypi.python.org/pypi/redis) key-value store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simulate TrainerLambda ARN\n",
    "#environ[str('TrainerLambda')] = str(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For this version of the implementation, the S3 Bucket is called **lnn** and the folder is called **training_input**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish client connectivity to the various AWS services that the function will leverage, the following cell creates the needed clients as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "#Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Overview\n",
    "### Datasets\n",
    "It is **very important** in Neural Network programming (without the useof a Deep Learning Framework), to have a full understanding of the dimensions of the input data as well as how the dimensions are transformed at each layer, therefore to build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat, the following cells explain the datsets.\n",
    "\n",
    "To train the Neural Network, we are provided with a dataset (`datasets.h5`) contaning:\n",
    "- a training set of $m$ images containing cats and non-cats as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- a test set of $m$ images containing cats and non-cat sas well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- classes list for cat and non-cat images.\n",
    "\n",
    ">**Note:** The original dataset was comprised of two seprate files, `test_catvnoncat.h5` and `train_catvnoncat.h5`. For the sake of this implementation a single file is needed to upload to the *S3 Bucket*, `datasets.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load main dataset\n",
    "dataset = h5py.File('datasets/datasets.h5', \"r\")\n",
    "\n",
    "# Get the names of the unique datsets\n",
    "datasetNames = [n for n in dataset.keys()]\n",
    "for n in datasetNames:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create numpy arrays of the various unique datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "classes = np.array(dataset[\"list_classes\"][:]) # the list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Displaye the dimensions of each unique data set\n",
    "print(\"train_set_x_orig dimensions: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_y_orig dimension: \" + str(train_set_y_orig.shape))\n",
    "print(\"test_set_x_orig dimensions: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_y_orig dimensions: \" + str(test_set_y_orig.shape))\n",
    "test_set_y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, the image data (`train_set_x_orig` and `test_set_x_orig`) are 4-dimensional arrays consiting of $209$ training examoples (**m_train**) and $50$ testing images (**m_test**) respecitivaly. Each image is in turn of *height*, *width* and *depth* (**R**ed, **G**reen **B**lue values) of $64 \\times 64 \\times 3$.\n",
    "\n",
    "Additionally, the dimentsion for the labels (`train_set_y_orig` and `test_set_y_orig`) only show a $209$ and $50$ column structure. So it is recommended when coding new networks, don't use data structures where the shape is $5$, or $n$, rank 1 array. Instead, this is set to, `(1, 209)` and `(1, 50)`, to make them a **row vector**, and in essence add another dimension to the `Numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create row vectors for the labels.\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"train_set_y dimensions: \" + str(train_set_y.shape))\n",
    "print(\"test_set_y dimensions: \" + str(test_set_y.shape))\n",
    "test_set_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Now that the additional dimension has been added to the label data, we can note the additional \"[[ ]]\" when displaying the array.\n",
    "\n",
    "Next we can see the label data and view the corresponding image, in this case, $index = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of a cat picture\n",
    "index = 2\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \\\n",
    "       \", and therefore it's a '\" + \\\n",
    "       classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \\\n",
    "       \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.squeeze()` method extracts the \" inner dimension\" of the array, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_y[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.squeeze(train_set_y[:, index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">**Note:** The \"[ ]\" has been removed.\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "The final model is expecting a traing set and a test set represented by a numpy array of shape (no. pixels $\\times$ no. pixels $\\times$ depth, data set size) respectivley. In turn, the model is expecting the training set and test set labels represented as a numpy array (vector) of shape (1, data set size) respectivley.\n",
    "\n",
    ">**Note:** It is not determined as yet wether the \"vectorization\" of the images should be performed by the `TrainerLambda` to set up the inputs for *Layer 0*. For the sake of Version 1.0, the preprocessing of the input data will be performed by `launch.py` as various helper functions.\n",
    "\n",
    "#### Vectorize\n",
    "The images are represented by a 3D array of shape $(length, height, depth = 3)$. However, when an image is read as the input of an algorithm it is converted to a vector of shape $(length*height*3, 1)$. In other words, it is \"unrolled\", \"flattened\" or \"reshaped\" from a 3D array into a 1D vector as can be seen below.\n",
    "\n",
    "<img src=\"images/vectorization.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "The following cells show explais of this process using the `train_set_x_orig` numpy array. The end result for the input to the model is a is a numpy array where where each column represents a flattenned image in a matrix with all the inpute features (images) being a colum, $209$ for the training set and $50$ for the test set repsectivley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy of origional training set\n",
    "orig = train_set_x_orig\n",
    "print(\"Original shape: \" + str(orig.shape))\n",
    "\n",
    "# \"vectorize\" or flatten out the array into an 1D vector\n",
    "flatten = orig.reshape(orig.shape[0], -1)\n",
    "print(\"Flattened shape: \"+ str(flatten.shape))\n",
    "\n",
    "# Transpose into a colums\n",
    "flatten_T = flatten.T\n",
    "print(\"Transpose: \" + str(flatten_T.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For further intuation of what the above code is doing, the following shows a more \"manual\", alternate way.\n",
    "\n",
    "#### Standardize\n",
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from $0$ to $255$. One common preprocessing step in machine learning is to substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by $255$ (the maximum value of a pixel channel). \n",
    "\n",
    ">**Note:** During the training of the model, the weights willbe multiplied and biases added to the initial inputs in order to observe neuron activations. Then it will backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load datsets for preprocessing after vectorization\n",
    "train_set_x = (train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T) / 255\n",
    "test_set_x = (test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T) / 255\n",
    "print(\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"sample value: \" + str(train_set_x[index][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Overview\n",
    "### Lambda Function\n",
    "#### `launch.py`\n",
    "The `launch.py` Lambda Function is triggered by the S3 event where training data is uploaded to S3. It further initiliazes the various components needed, such as:\n",
    "1. State as well as the outputs of each epoch/iteration in DynamoDB:\n",
    "    - Training Set Accuracy.\n",
    "    - Test Set Accuracy.\n",
    "    - Weight paramter.\n",
    "    - Bias parameter.\n",
    "2. Temporary S3 Storage:\n",
    "    - The Output data of each layer and neauron over each epoch.\n",
    "    - This data is a series of `numpy` arrays that is leveraged as input data by the next layer or epoch/iteration.\n",
    "3. Preprocessing the Input Data: \n",
    "    - Read in the the initial *training*, *test* sets as well as the associated *class* of Cat images.\n",
    "    - The function initially loads the data in `h5py` format and extracts the *training*, *test* and *class* information.\n",
    "    - The function further performs any standardization and normalization of the input data.\n",
    "    - The function also \"*flattens*\" the data into a column vector, thus performing **Vectorization**.\n",
    "    - This data is dumped to the temporary S3 location and will thus serve as **Layer 0** of the Neural Network.\n",
    "4. Environment Variables:\n",
    "    - These setting are stored on the S3 along with the Input Data, in a `settings.json` file.\n",
    "    - The settings include overall parameters used by the `trainer` and `neuron` Lambda Functions, such as:\n",
    "        - Total number of epochs/iterations.\n",
    "        - Total number of layers in the Neural Network (including the Output layer).\n",
    "        - Total number of \"neurons\" in each layer.\n",
    "        - The activation function to be used for each layer.\n",
    "\n",
    "\n",
    "\n",
    "Adiitionally, the S3 event trigger will supply the specifics of the Input Data being uploaded to a dedicated bucket and folder in S3. For the sake of testing, the event parameters are simulated in the cell below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### `trainer.py`\n",
    "\n",
    "#### `neuron.py`\n",
    "\n",
    "### Helper Functions\n",
    "#### `to_cache(endpoint, obj, name)`\n",
    "Serializes multiple data type to ElastiCache and returns the Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is dict:\n",
    "        #x = json.dumps(obj)\n",
    "        #val = json.loads(x)\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `from_cache(endpoint, key)`\n",
    "De-serializes binary object from ElastiCache by reading the type of object from the name and converting it to the appropriate data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCacheendpoint\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return array\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return array\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        #parsed = json.loads(data)\n",
    "        #array = json.dumps(parsed, indent=4, sort_keys=True)\n",
    "        #return array\n",
    "        return json.loads(data)\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        return int(data)\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `name2str(obj, namespace)`\n",
    "Converts the name of the numpy array to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note**: An alternate method to *List Comprehension* is to use the `chain()` function to get the names of the Numpy arrays.\n",
    "```python\n",
    "from itertools import chain\n",
    "list(chain.from_iterable(a_names))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `vectorize()`\n",
    "Reshapes (flatten) the image data to column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `standardize()`\n",
    "Preprocess the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `initialize_data()`\n",
    ">**Note:** For the sake of testing, the following is not defined as function.\n",
    "\n",
    "Extracts the training and testing data from S3, flattens, standardizes, initializes the weights and bias and then creates an entry in Elastiache for neurons to process as layer $a^{[0]}$.\n",
    "\n",
    "Returns:  \n",
    "a_name -- list of the Numpy array names  \n",
    "dims --- dimensions of each of the data sets  \n",
    "params -- dictionary of the weight and bias parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fake parameters for w and b since this is not a function\n",
    "w = 0\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load main dataset\n",
    "dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "\n",
    "# Retieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "\n",
    "# Create numpy arrays from the various h5 datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "#classes = np.array(dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "# Reshape labels\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "# Preprocess inputs\n",
    "train_set_x = standardize(train_set_x_orig)\n",
    "test_set_x = standardize(test_set_x_orig)\n",
    "\n",
    "# Dump the inputs to the temporary s3 bucket for TrainerLambda\n",
    "#bucket = storage_init() # Creates a temporary bucket for the propogation steps\n",
    "data_keys = {} # Dictionary for the hask keys of the data set\n",
    "dims = {} # Dictionary of data set dimensions\n",
    "a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "a_names = [] # Placeholder for array names\n",
    "for i in range(len(a_list)):\n",
    "    # Create a lis of the names of the numpy arrays\n",
    "    a_names.append(name2str(a_list[i], globals()))\n",
    "for j in range(len(a_list)): \n",
    "    #data_keys[str(a_names[j][0])] = numpy2cache(endpoint, array=a_list[j], name=a_names[j][0])\n",
    "    data_keys[str(a_names[j][0])] = to_cache(endpoint, obj=a_list[j], name=a_names[j][0])\n",
    "    dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "# Initialize weights\n",
    "if w == 0: # Initialize weights to dimensions of the input data\n",
    "    dim = dims.get('train_set_x')[0]\n",
    "    weights = np.zeros((dim, 1))\n",
    "    # Store the initial weights as a column vector on S3\n",
    "    #data_keys['weights'] = numpy2cache(endpoint, array=weights, name='weights')\n",
    "    data_keys['weights'] = to_cache(endpoint, obj=weights, name='weights')\n",
    "else:\n",
    "    #placeholder for random weight initialization\n",
    "    pass\n",
    "        \n",
    "# Initialize Bias\n",
    "if b != 0:\n",
    "    #placeholder for random bias initialization\n",
    "    #data_keys['bias'] = numpy2cache(endpoint, array=bias, name='bias')\n",
    "    pass\n",
    "else:\n",
    "    #data_keys['bias'] = dump2cache(endpoint, dump=str(b), name='bias')\n",
    "    data_keys['bias'] = to_cache(endpoint, obj=b, name='bias')\n",
    "\n",
    "#return data_keys, [j for i in a_names for j in i], dims, #arams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note:** It might be a good practice to insert assertion statements here as part of debuging.\n",
    "\n",
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "# Retieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "\n",
    "# Load datsets for preprocessing after vectorization\n",
    "print(\"Shape of train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"Sample Value from numpy array: \" + str(train_set_x[index][index]))\n",
    "\n",
    "# Retrieve data from cache\n",
    "data_key = data_keys.get('train_set_x')\n",
    "#data = cache2numpy(endpoint, data_key)\n",
    "data = from_cache(endpoint, data_key)\n",
    "\n",
    "# Get the cache data from \n",
    "print(\"Shape of ElastiCache Data: \" + str(data.shape))\n",
    "print(\"Sample Value from Elasticache Data: \" + str(data[index][index]))\n",
    "\n",
    "# Get weights and bias\n",
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "data_key = data_keys.get('bias')\n",
    "#data = int(cache.get(data_key))\n",
    "data = from_cache(endpoint, data_key)\n",
    "print(\"Bias Value from ElastiCache: \" + str(data))\n",
    "print(\"Bias Valye type form ElastiCache: \" + str(type(data)))\n",
    "data_key = data_keys.get('weights')\n",
    "#data = cache2numpy(endpoint, data_key)\n",
    "data = from_cache(endpoint, data_key)\n",
    "print(\"Weights Value Shape from Elasticache: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lambda Handler\n",
    "#### Process `event` variables\n",
    "Within the `event` variables are the specifics of the S3 environment from which the Lambda Function is triggered. The first objective is to capture the S3 *Bucket* and S3 *Key* in order to get the Network Architecture setting and the input data that traiggered the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve datasets and setting from S3\n",
    "input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "try:\n",
    "    input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "    input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(\"Error downloading input data from S3, S3 object does not exist\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Neural Network Settings\n",
    "the various settings from `settings.json` are appended to the environment settings to be used later as the pyaload for the *Trainer* Lambda Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the neural network parameters\n",
    "with open('/tmp/parameters.json') as parameters_file:\n",
    "    parameters = json.load(parameters_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the `payload` to send to the *Trainer* Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build in additional parameters from neural network parameters\n",
    "parameters['epoch'] = 1\n",
    "\n",
    "# Next Layer to process\n",
    "parameters['layer'] = 1\n",
    "\n",
    "# Input data sets\n",
    "# Simulate return variables from initialize_data()\n",
    "parameters['input_data'] = [j for i in a_names for j in i]\n",
    "\n",
    "parameters['data_keys'] = data_keys\n",
    "parameters['data_dimensions'] = dims\n",
    "\n",
    "# Initialize payload to `TrainerLambda`\n",
    "payload = {}\n",
    "\n",
    "# Initialize the overall state\n",
    "payload['state'] = 'start'\n",
    "\n",
    "# Dump the parameters to ElastiCache\n",
    "#payload['parameters'] = dump2cache(endpoint, dump=dumps(parameters), name='parameters')\n",
    "payload['parameters'] = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "# ElastiCache endpoint \n",
    "#payload['endpoint'] = endpoint\n",
    "\n",
    "# Prepare the payload for `TrainerLambda`\n",
    "payloadbytes = dumps(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check: Using `from_cache()`**\n",
    ">**Note:** To work with `parameters` from Elasticache, either hard code the `json.dumps()` to the return value or manually convert the return value to a Pythin dictionary. The cells belowhave `json.dumps()` hard coded into the returnm value of `from_cache()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_key = payload['parameters']\n",
    "print(\"Key to test: \" + str(test_key) + '\\n')\n",
    "data = from_cache(endpoint, test_key)\n",
    "print(\"Data from Elasticache:\\n\")\n",
    "print(data)\n",
    "print(\"\\n Data type from Elasticache: \" + str(type(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check: Using the native `get` method from Redis (and then executing `json.dumps(indent=4, sort_keys=True)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the neural network parameters from ElastiCache\n",
    "key = payload.get('parameters')\n",
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "data = cache.get(key)\n",
    "parsed = json.loads(data)\n",
    "print(json.dumps(parsed, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch the `TrainerLambda` Function\n",
    "\n",
    "```python\n",
    "# kick off lambda for next layer\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=environ['TrainerLambda'], #ENSURE ARN POPULATED BY CFN OR S3 EVENT\n",
    "        InvocationType='Event',\n",
    "        Payload=payloadbytes\n",
    "    )\n",
    "```\n",
    "\n",
    "**Sanity Check: Data to be sent to the `TrainerLambda`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Payload to be sent to TrainerLambda\")\n",
    "print(dumps(payload, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Workflow\n",
    "### Trigger Event\n",
    "\n",
    "\n",
    "### Data Ingest and Preprocessing\n",
    "\n",
    "\n",
    "### Invoking the Trainer\n",
    "\n",
    "\n",
    "### Processing the First Layer\n",
    "\n",
    "\n",
    "### Calculatng the Cost\n",
    "\n",
    "\n",
    "### Determining the Gradients\n",
    "\n",
    "\n",
    "### Parameter Optmization\n",
    "---\n",
    "\n",
    "---\n",
    "## Neuron Lambda Function\n",
    ">**Note**: The `NeuronLamabda` function is added here for the sake of the flow of the Notebook.\n",
    "\n",
    "---\n",
    "\n",
    "# Trainer Lambda Function\n",
    "The `trainer.py` Lambda Function is the most critical funciton in the set in that it:\n",
    "1. Tracks and updates the state across the interations/epochs and the various layers of the Neural Network.\n",
    "2. Launches the various Neurons (`NeuraonLamabda`) in ech layer and tracks their output.\n",
    "\n",
    "In order to accomplish this, the `TrainerLambda` has three possible states, `start`, `forward` and `backward`:\n",
    "1. `start`: This state starts the initial or subsequent training epochs and performs the following:\n",
    "    - Initializes the new weights and bias for the epoch.\n",
    "    - Updates the state table with these values.\n",
    "2. `forward`: This state processes the *forward* porpogation step and launches the various hidden layer/s Neurons and supplies the necessary state information to these functions, such as:\n",
    "    - Input data location\n",
    "    - Wights and Bias.\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Activation Funciton for the Layer.\n",
    "3. `backward`: This state processes the *back* propogation/optimization step and launches the various hidden layer/s Neurons as well as supplies the necessary information for these functions, like:\n",
    "    - Gradient Parameters\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Learning Rate.\n",
    "    - Loss function calculated fromthe Forward propogation step.\n",
    "\n",
    ">**Side Note**: Ther may be a necessity later on when dealing with multiple hidden layers, to combine the `forward` and `backward` states into a `propogate` state and then have a separate `optimize` state for **Gradient Decent**.\n",
    "\n",
    "## 1 - Libraries, Global and Event Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from redis import StrictRedis as redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "# Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "cache = redis(host=endpoint, port=6379, db=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate the event variables passed from the `LaunchLambnda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event = loads(payloadbytes)\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Function Components\n",
    "### 2.1 Helper Functions\n",
    "#### `to_cache(endpoint, obj, name)`\n",
    "Serializes multiple data type to ElastiCache and returns the Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint.\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array.\n",
    "            - Python Dictionary.\n",
    "            - String.\n",
    "            - Integer.\n",
    "    name -- Name of the Key.\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `from_cache(endpoint, key)`\n",
    "De-serializes binary object from ElastiCache by reading the type of object from the name and converting it to the appropriate data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint.\n",
    "    key -- Name of the Key to retrieve the object.\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `start_epoch(epoch, layer)`\n",
    "Starts a new epoch and configures the necessary state tracking objcts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_epoch(epoch, layer):\n",
    "    \"\"\"\n",
    "    Starts a new epoch and configures the necessary state tracking objcts.\n",
    "    \n",
    "    Arguments:\n",
    "    epoch -- Integer representing the \"current\" epoch.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    #TBD\n",
    "    pass\n",
    "    \n",
    "    # Initialize the results onbject for the new epoch\n",
    "    results['epoch' + str(epoch)] = {}\n",
    "    #print(results)\n",
    "    \n",
    "    # Start forwardprop\n",
    "    #propogate(direction='forward', epoch=epoch, layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `finish_epoch()`\n",
    "Closes out the current epoch and updates the necessary information to the results object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finish_epoch(direction, epoch, layer):\n",
    "    \"\"\"\n",
    "    Closes out the current epoch and updates the necessary information to the results object.\n",
    "\n",
    "    Arguments:\n",
    "    direction -- The current direction of the propogation, either `forward` or `backward`.\n",
    "    epoch -- Integer representing the \"current\" epoch to close out.\n",
    "\n",
    "    Returns:\n",
    "    Launches `start_epoch()` to build the next epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `propogate(direction, epoch, layer)`\n",
    "Determins the amount of \"hidden\" units based on the layer and loops through launching the necessary `NeuronLambda` functions with the appropriate state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propogate(direction, epoch, layer):\n",
    "    \"\"\"\n",
    "    Determines the amount of \"hidden\" units based on the layer and loops\n",
    "    through launching the necessary `NeuronLambda` functions with the \n",
    "    appropriate state. Each `NeuronLambda` implements the cost function \n",
    "    OR the gradients depending on the direction.\n",
    "\n",
    "    Arguments:\n",
    "    direction -- The current direction of the propogation, either `forward` or `backward`.\n",
    "    epoch -- Integer representing the \"current\" epoch to close out.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################################################\n",
    "    # When launching Neuron, the following must be added      #\n",
    "    # to the payload:                                         #\n",
    "    # 1. parameter_key.                                       #\n",
    "    # 2. state/direction.                                     #\n",
    "    # 3. epoch.                                               #\n",
    "    # 4. layer.                                               #\n",
    "    # 5. final. (Is it the last neuron? True|False).          #\n",
    "    ###########################################################\n",
    "    \n",
    "    # Build the NeuronLambda payload\n",
    "    #payload = {}\n",
    "    \n",
    "    # Get the Network paramerters\n",
    "    #payload['state'] = direction\n",
    "    #payload[parameter_key] = parameter_key\n",
    "    #payload['epoch'] = epoch\n",
    "    #payload['layer'] = layer\n",
    "    \n",
    "    #TBD\n",
    "    if direction == 'forward':\n",
    "        # Launch Lambdas to propogate forward\n",
    "        pass\n",
    "    elif direction == 'backward':\n",
    "        # Launch Lambdas to propogate backward\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    Note:\n",
    "    When launching NeuronLambda with multiple hidden unit,\n",
    "    remember to assign an ID\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `optimize(epoch, layer)`\n",
    "Optimizes `w` and `b` by running Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(epoch, layer, params, grads):\n",
    "    \"\"\"\n",
    "    Optimizes `w` and `b` by running Gradient Descent to get the `cost`.\n",
    "    \n",
    "    epoch -- Integer representing the \"current\" epoch to close out.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "    params -- Dictionary containing the gradients of the weights and \n",
    "                bias.\n",
    "    grads -- Dictionary containing the gardients of the wights and\n",
    "                bias with respect to the cost function.\n",
    "    \n",
    "    Returns:\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    \n",
    "    #TBD\n",
    "    #Get the learning rate\n",
    "    #learning_rate = parameters['learning_rate']\n",
    "    \n",
    "    \"\"\"\n",
    "    Note:\n",
    "    Probably have to get the cost from the output of the NeuronLambdas\n",
    "    OR\n",
    "    Get data from the NeuronLambdas and calculate the cost here\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the grads and params\n",
    "    \n",
    "    # Perform the update rule\n",
    "    #w = w - learning_rate * grads['dw']\n",
    "    #b = b - learning_rate * grads['db']\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note**: Since this test is for a single Perceptron, and thus a single output Neuron, it should be up to the `NeuronLambda` to calculate the cost and supply it back to the `TrainerLambda`. In the case of multiple Logistic Regression or more complicated networks, there may be multiple output Neurons. How to address these situation will need to thought through later.\n",
    "\n",
    "#### `end()`\n",
    "Finishes out the epoch and starts the next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def end():\n",
    "    \"\"\"\n",
    "    Finishes out the process and launches the next state mechanisms for prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    #TBD\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lambda Handler\n",
    "#### Process `event` variables\n",
    "Processes the `event` vaiables from the various Lambda function that call it, i.e. `TrainerLambda` and `NeuronLambda`. Determines the \"current\" state and then directs the next steps.\n",
    "\n",
    ">**Note**: The following simulates the process from the point of starting an epoch, with the `state = start`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Neural Network paramaters from Elasticache\n",
    "global parameter_key\n",
    "parameter_key = event.get('parameters')\n",
    "global parameters \n",
    "parameters = from_cache(endpoint, parameter_key)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the current state from the invoking lambda\n",
    "state = event.get('state')\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute appropriate action based on the the current state\n",
    "if state == 'forward':\n",
    "    # Get important state variables\n",
    "    epoch = event.get('epoch')\n",
    "    layer = event.get('layer')\n",
    "    \n",
    "    # Determine the location within forwardprop\n",
    "    if layer > layers:\n",
    "        # Location is at the end of forwardprop\n",
    "        \n",
    "        #TBD\n",
    "        \n",
    "        # Start backprop\n",
    "        #propogate(direction='backward', layer=layer-1)\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        # Move to the next hidden layer\n",
    "        #propogate(direction='forward', layer=layer+1, activations=activations)\n",
    "        \n",
    "        pass\n",
    "elif state == 'backward':\n",
    "    # Get important state variables\n",
    "    # Determine the location within backprop\n",
    "    if epoch == epochs and layer == 0:\n",
    "        # Location is at the end of the final epoch\n",
    "        \n",
    "        # Caculate derivative?????????????????????????\n",
    "        \n",
    "        # Caclulate the absolute final weight\n",
    "        # Update the final weights and results (cost) to DynamoDB\n",
    "        \n",
    "        # Finalize the the process and clean up\n",
    "        #end()\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    elif epoch < epochs and layer == 0:\n",
    "        # Location is at the end of the current epoch and backprop is finished\n",
    "        # Calculate the derivative?????????????????????????\n",
    "        \n",
    "        # Calculate the weights for this epoch\n",
    "        \n",
    "        # Update the weights and results (cost) to ElastiCache\n",
    "        \n",
    "        # Start the next epoch\n",
    "        #epoch = epoch + 1\n",
    "        #start_epoch(epoch)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        # Move to the next hidden layer\n",
    "        #propogate(direction='backward', layer=layer-1)\n",
    "        \n",
    "        pass\n",
    "\n",
    "elif state == 'start':\n",
    "    # Start of a new run of the process\n",
    "    # Initialize the results tracking object\n",
    "    global results\n",
    "    results = {}\n",
    "    \n",
    "    # Create initial parameters\n",
    "    epoch = 1\n",
    "    layer = 1\n",
    "    start_epoch(epoch=epoch, layer=layer)\n",
    "    \n",
    "else:\n",
    "    print(\"No state informaiton has been provided.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix A: Build the Lambda Deployment Package"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
