{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "#Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations\n",
    "\n",
    "w = 0\n",
    "b = 0\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "# Retieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "data_keys = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj2cache(endpoint, obj, name):\n",
    "    def isjson(x):\n",
    "        try:\n",
    "            json_obj = json.loads(x)\n",
    "        except ValueError as e:\n",
    "            return False\n",
    "        return True\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is dict:\n",
    "        #x = json.dumps(obj)\n",
    "        #val = json.loads(x)\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "#    elif isjson(obj):\n",
    "#        key = '{0}|{1}'.format(name, 'json')\n",
    "#        val = json.dumps(obj)\n",
    "#        cache = redis(host=endpoint, port=6379, db=0)\n",
    "#        cache.set(key, val)\n",
    "#        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache2obj(endpoint, key):\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return array\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return array\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        #parsed = json.loads(data)\n",
    "        #array = json.dumps(parsed, indent=4, sort_keys=True)\n",
    "        #return array\n",
    "        return data\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        return int(data)\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main dataset\n",
    "dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "\n",
    "# Retieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "\n",
    "# Create numpy arrays from the various h5 datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "#classes = np.array(dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "# Reshape labels\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "# Preprocess inputs\n",
    "train_set_x = standardize(train_set_x_orig)\n",
    "test_set_x = standardize(test_set_x_orig)\n",
    "\n",
    "# Dump the inputs to the temporary s3 bucket for TrainerLambda\n",
    "#bucket = storage_init() # Creates a temporary bucket for the propogation steps\n",
    "data_keys = {} # Dictionary for the hask keys of the data set\n",
    "dims = {} # Dictionary of data set dimensions\n",
    "a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "a_names = [] # Placeholder for array names\n",
    "for i in range(len(a_list)):\n",
    "    # Create a lis of the names of the numpy arrays\n",
    "    a_names.append(name2str(a_list[i], globals()))\n",
    "for j in range(len(a_list)):\n",
    "    # \n",
    "    data_keys[str(a_names[j][0])] = obj2cache(endpoint, obj=a_list[j], name=a_names[j][0])\n",
    "    dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "# Initialize weights\n",
    "if w == 0: # Initialize weights to dimensions of the input data\n",
    "    dim = dims.get('train_set_x')[0]\n",
    "    weights = np.zeros((dim, 1))\n",
    "    # Store the initial weights as a column vector on S3\n",
    "    data_keys['weights'] = obj2cache(endpoint, obj=weights, name='weights')\n",
    "else:\n",
    "    #placeholder for random weight initialization\n",
    "    pass\n",
    "        \n",
    "# Initialize Bias\n",
    "if b != 0:\n",
    "    #placeholder for random bias initialization\n",
    "    #data_keys['bias'] = numpy2cache(endpoint, array=bias, name='bias')\n",
    "    pass\n",
    "else:\n",
    "    data_keys['bias'] = obj2cache(endpoint, obj=b, name='bias')\n",
    "    \n",
    "# Initialize the results tracking object\n",
    "data_keys['results'] = obj2cache(endpoint, obj='', name='results')\n",
    "        \n",
    "\n",
    "#return data_keys, [j for i in a_names for j in i], dims, #arams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 'bias|int',\n",
       " 'results': 'results|string',\n",
       " 'test_set_x': 'test_set_x|float64#12288#50',\n",
       " 'test_set_y': 'test_set_y|int64#1#50',\n",
       " 'train_set_x': 'train_set_x|float64#12288#209',\n",
       " 'train_set_y': 'train_set_y|int64#1#209',\n",
       " 'weights': 'weights|float64#12288#1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cache2obj(endpoint, key=data_keys['results'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cache2obj(endpoint, data_keys['bias'])\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the neural network parameters\n",
    "with open('/tmp/parameters.json') as parameters_file:\n",
    "    parameters = json.load(parameters_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'lnn-re-1o4ic3wm3z3t9.svpice.0001.usw2.cache.amazonaws.com',\n",
       " 'parameters': 'parameters|json',\n",
       " 'state': 'start'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build in additional parameters from neural network parameters\n",
    "parameters['epoch'] = 1\n",
    "\n",
    "# Next Layer to process\n",
    "parameters['layer'] = 1\n",
    "\n",
    "# Input data sets\n",
    "# Simulate return variables from initialize_data()\n",
    "parameters['input_data'] = [j for i in a_names for j in i]\n",
    "\n",
    "parameters['data_keys'] = data_keys\n",
    "parameters['data_dimensions'] = dims\n",
    "\n",
    "# Initialize payload to `TrainerLambda`\n",
    "payload = {}\n",
    "\n",
    "# Initialize the overall state\n",
    "payload['state'] = 'start'\n",
    "\n",
    "# Dump the parameters to ElastiCache\n",
    "payload['parameters'] = obj2cache(endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "# ElastiCache endpoint \n",
    "payload['endpoint'] = endpoint\n",
    "\n",
    "# Prepare the payload for `TrainerLambda`\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "a = dumps(payload)\n",
    "b = loads(a)\n",
    "print(type(a))\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dumps(parameters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"epochs\": 20, \"layers\": 1, \"activations\": {\"layer1\": \"sigmoid\"}, \"neurons\": {\"layer1\": 1}, \"weight\": 0, \"bias\": 0, \"learning_rate\": 0.005, \"epoch\": 1, \"layer\": 1, \"input_data\": [\"train_set_x\", \"train_set_y\", \"test_set_x\", \"test_set_y\"], \"data_keys\": {\"train_set_x\": \"train_set_x|float64#12288#209\", \"train_set_y\": \"train_set_y|int64#1#209\", \"test_set_x\": \"test_set_x|float64#12288#50\", \"test_set_y\": \"test_set_y|int64#1#50\", \"weights\": \"weights|float64#12288#1\", \"bias\": \"bias|int\", \"results\": \"results|string\"}, \"data_dimensions\": {\"train_set_x\": [12288, 209], \"train_set_y\": [1, 209], \"test_set_x\": [12288, 50], \"test_set_y\": [1, 50]}}'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "x = cache.get('parameters|json')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activations': {'layer1': 'sigmoid'},\n",
       " 'bias': 0,\n",
       " 'data_dimensions': {'test_set_x': [12288, 50],\n",
       "  'test_set_y': [1, 50],\n",
       "  'train_set_x': [12288, 209],\n",
       "  'train_set_y': [1, 209]},\n",
       " 'data_keys': {'bias': 'bias|int',\n",
       "  'results': 'results|string',\n",
       "  'test_set_x': 'test_set_x|float64#12288#50',\n",
       "  'test_set_y': 'test_set_y|int64#1#50',\n",
       "  'train_set_x': 'train_set_x|float64#12288#209',\n",
       "  'train_set_y': 'train_set_y|int64#1#209',\n",
       "  'weights': 'weights|float64#12288#1'},\n",
       " 'epoch': 1,\n",
       " 'epochs': 20,\n",
       " 'input_data': ['train_set_x', 'train_set_y', 'test_set_x', 'test_set_y'],\n",
       " 'layer': 1,\n",
       " 'layers': 1,\n",
       " 'learning_rate': 0.005,\n",
       " 'neurons': {'layer1': 1},\n",
       " 'weight': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters|json\n"
     ]
    }
   ],
   "source": [
    "key = payload['parameters']\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"epochs\": 20, \"layers\": 1, \"activations\": {\"layer1\": \"sigmoid\"}, \"neurons\": {\"layer1\": 1}, \"weight\": 0, \"bias\": 0, \"learning_rate\": 0.005, \"epoch\": 1, \"layer\": 1, \"input_data\": [\"train_set_x\", \"train_set_y\", \"test_set_x\", \"test_set_y\"], \"data_keys\": {\"train_set_x\": \"train_set_x|float64#12288#209\", \"train_set_y\": \"train_set_y|int64#1#209\", \"test_set_x\": \"test_set_x|float64#12288#50\", \"test_set_y\": \"test_set_y|int64#1#50\", \"weights\": \"weights|float64#12288#1\", \"bias\": \"bias|int\", \"results\": \"results|string\"}, \"data_dimensions\": {\"train_set_x\": [12288, 209], \"train_set_y\": [1, 209], \"test_set_x\": [12288, 50], \"test_set_y\": [1, 50]}}'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = cache2obj(endpoint, key=payload['parameters'])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
