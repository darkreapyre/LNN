{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "---\n",
    "\n",
    "**BLAH BLAH BLAH**\n",
    "\n",
    "---\n",
    "\n",
    "# Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations\n",
    "# Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "cache = redis(host=endpoint, port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_data(endpoint, w, b):\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes and then dumps the data to ElastiCache \n",
    "    for neurons to process as layer a^0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "    \n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "    \n",
    "    # Reshape labels\n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # Preprocess inputs\n",
    "    train_set_x = standardize(train_set_x_orig)\n",
    "    test_set_x = standardize(test_set_x_orig)\n",
    "\n",
    "    # Dump the inputs to the temporary s3 bucket for TrainerLambda\n",
    "    #bucket = storage_init() # Creates a temporary bucket for the propogation steps\n",
    "    data_keys = {} # Dictionary for the hask keys of the data set\n",
    "    dims = {} # Dictionary of data set dimensions\n",
    "    a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "    a_names = [] # Placeholder for array names\n",
    "    for i in range(len(a_list)):\n",
    "        # Create a lis of the names of the numpy arrays\n",
    "        a_names.append(name2str(a_list[i], locals()))\n",
    "    for j in range(len(a_list)):\n",
    "        # Dump the numpy arrays to ElastiCache\n",
    "        data_keys[str(a_names[j][0])] = to_cache(endpoint, obj=a_list[j], name=a_names[j][0])\n",
    "        # Append the array dimensions to the list\n",
    "        dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "    # Initialize weights\n",
    "    if w == 0: # Initialize weights to dimensions of the input data\n",
    "        dim = dims.get('train_set_x')[0]\n",
    "        weights = np.zeros((dim, 1))\n",
    "        # Store the initial weights as a column vector on S3\n",
    "        data_keys['weights'] = to_cache(endpoint, obj=weights, name='weights')\n",
    "    else:\n",
    "        #placeholder for random weight initialization\n",
    "        pass\n",
    "        \n",
    "    # Initialize Bias\n",
    "    if b != 0:\n",
    "        #placeholder for random bias initialization\n",
    "        #data_keys['bias'] = to_cache(endpoint, obj=bias, name='bias')\n",
    "        pass\n",
    "    else:\n",
    "        data_keys['bias'] = to_cache(endpoint, obj=b, name='bias')\n",
    "    \n",
    "    # Initialize training example size\n",
    "    m = train_set_x.shape[1]\n",
    "    data_keys['m'] = to_cache(endpoint, obj=m, name='m')\n",
    "    \n",
    "#    # Initialize the results tracking object\n",
    "#    to_cache(endpoint, dump='', name='results')\n",
    "        \n",
    "    return data_keys, [j for i in a_names for j in i], dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is dict:\n",
    "        #x = json.dumps(obj)\n",
    "        #val = json.loads(x)\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint.\n",
    "    key -- Name of the Key to retrieve the object.\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Launch Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simulate TrainerLambda ARN\n",
    "#environ[str('TrainerLambda')] = str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Neural Network Settings: \n",
      "\n",
      "{\n",
      "    \"activations\": {\n",
      "        \"layer1\": \"sigmoid\"\n",
      "    },\n",
      "    \"bias\": 0,\n",
      "    \"data_dimensions\": {\n",
      "        \"test_set_x\": [\n",
      "            12288,\n",
      "            50\n",
      "        ],\n",
      "        \"test_set_y\": [\n",
      "            1,\n",
      "            50\n",
      "        ],\n",
      "        \"train_set_x\": [\n",
      "            12288,\n",
      "            209\n",
      "        ],\n",
      "        \"train_set_y\": [\n",
      "            1,\n",
      "            209\n",
      "        ]\n",
      "    },\n",
      "    \"data_keys\": {\n",
      "        \"bias\": \"bias|int\",\n",
      "        \"m\": \"m|int\",\n",
      "        \"test_set_x\": \"test_set_x|float64#12288#50\",\n",
      "        \"test_set_y\": \"test_set_y|int64#1#50\",\n",
      "        \"train_set_x\": \"train_set_x|float64#12288#209\",\n",
      "        \"train_set_y\": \"train_set_y|int64#1#209\",\n",
      "        \"weights\": \"weights|float64#12288#1\"\n",
      "    },\n",
      "    \"epoch\": 1,\n",
      "    \"epochs\": 1,\n",
      "    \"input_data\": [\n",
      "        \"train_set_x\",\n",
      "        \"train_set_y\",\n",
      "        \"test_set_x\",\n",
      "        \"test_set_y\"\n",
      "    ],\n",
      "    \"layer\": 1,\n",
      "    \"layers\": 1,\n",
      "    \"learning_rate\": 0.005,\n",
      "    \"neurons\": {\n",
      "        \"layer1\": 1\n",
      "    },\n",
      "    \"weight\": 0\n",
      "}\n",
      "Payload to be sent to TrainerLambda: \n",
      "{\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"start\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve datasets and setting from S3\n",
    "input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "try:\n",
    "    input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "    input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(\"Error downloading input data from S3, S3 object does not exist\")\n",
    "    else:\n",
    "        raise\n",
    "    \n",
    "# Extract the neural network parameters\n",
    "with open('/tmp/parameters.json') as parameters_file:\n",
    "    parameters = json.load(parameters_file)\n",
    "    \n",
    "# Build in additional parameters from neural network parameters\n",
    "parameters['epoch'] = 1\n",
    "# Next Layer to process\n",
    "parameters['layer'] = 1\n",
    "# Input data sets and data set parameters\n",
    "parameters['data_keys'], parameters['input_data'], parameters['data_dimensions'] = initialize_data(endpoint=endpoint, w=parameters.get('weight'), b=parameters.get('bias'))\n",
    "    \n",
    "# Initialize payload to `TrainerLambda`\n",
    "payload = {}\n",
    "# Initialize the overall state\n",
    "payload['state'] = 'start'\n",
    "# Dump the parameters to ElastiCache\n",
    "payload['parameter_key'] = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "#payload['endpoint'] = endpoint\n",
    "# Prepare the payload for `TrainerLambda`\n",
    "payloadbytes = dumps(payload)\n",
    "    \n",
    "print(\"Complete Neural Network Settings: \\n\")\n",
    "print(dumps(parameters, indent=4, sort_keys=True))\n",
    "print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Trainer -> Neuron Event\n",
    "**Simulating Forward Propogation of the Neuron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event = payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global parameter_key\n",
    "parameter_key = event.get('parameter_key')\n",
    "global parameters \n",
    "parameters = from_cache(endpoint, parameter_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = from_cache(endpoint=endpoint, key=parameters['data_keys']['weights'])\n",
    "b = from_cache(endpoint=endpoint, key=parameters['data_keys']['bias'])\n",
    "X = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_x'])\n",
    "Y = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_y'])\n",
    "m = from_cache(endpoint=endpoint, key=parameters['data_keys']['m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = sigmoid(np.dot(w.T, X) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 209)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a_1|float64#1#209'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID = 1\n",
    "to_cache(endpoint=endpoint, obj=a, name='a_'+str(ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Neuron -> Trainer Event\n",
    "**Simulating processing the Cost from the Activation output/s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a_1|float64#1#209'\n",
      "b'a_2|float64#1#209'\n"
     ]
    }
   ],
   "source": [
    "for key in cache.scan_iter(match='a_*'):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_1|float64#1#209\n",
      "a_2|float64#1#209\n"
     ]
    }
   ],
   "source": [
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([])\n",
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    B = from_cache(endpoint, key)\n",
    "    C = np.append(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209,)\n"
     ]
    }
   ],
   "source": [
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = from_cache(endpoint, key='a_1|float64#1#209')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = np.append(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "        0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 209)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = C.reshape(1, B.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 209)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(C.shape == B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "**Merging two activation arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 'a_1|float64#1#209'\n",
    "A = from_cache(endpoint, key=key)\n",
    "B = from_cache(endpoint, key=key)\n",
    "assert(A.shape == B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 209)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.vstack((A, B))\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n",
       "       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "**Merging three activation arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([]).reshape(0, count)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key1 = 'a_1|float64#1#209'\n",
    "key2 = 'a_2|float64#1#209'\n",
    "a = from_cache(endpoint, key=key1)\n",
    "b = from_cache(endpoint, key=key2)\n",
    "c = from_cache(endpoint, key=key1)\n",
    "assert(a.shape == b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.concatenate((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 209)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.concatenate((A, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 209)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Process on how to the above programatically** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['a_1|float64#1#209', 'a_2|float64#1#209']\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "to_concat = []\n",
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "count = 0\n",
    "index = []\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    to_concat.append(key)\n",
    "    index.append(count)\n",
    "    count = count + 1\n",
    "print(count)\n",
    "print(to_concat)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(count):\n",
    "    index[i] = from_cache(endpoint=endpoint, key=to_concat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
       "          0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]]),\n",
       " array([[ 0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933,  0.62245933,\n",
       "          0.62245933,  0.62245933,  0.62245933,  0.62245933]])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
