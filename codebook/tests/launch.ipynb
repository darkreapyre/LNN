{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `launch.lambda_handler()`\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from decimal import Decimal, Inexact, Rounded\n",
    "from boto3.dynamodb.types import DYNAMODB_CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "dynamo_client = client('dynamodb', region_name='us-west-2') # DynamoDB access\n",
    "dynamodb = resource('dynamodb', region_name='us-west-2')\n",
    "DYNAMODB_CONTEXT.traps[Inexact] = 0\n",
    "DYNAMODB_CONTEXT.traps[Rounded] = 0\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def dict2item(raw):\n",
    "    \"\"\"\n",
    "    Converts a dictionary to the appropriate Key, Value formatting for DynamoDB\n",
    "    and ensures that `float` values are converted to `Decimal`\n",
    "    \"\"\"\n",
    "    if type(raw) is dict:\n",
    "        resp = {}\n",
    "        for k,v in raw.items():\n",
    "            if type(v) is str:\n",
    "                resp[k] = {\n",
    "                    'S': v\n",
    "                }\n",
    "            elif type(v) is int:\n",
    "                resp[k] = {\n",
    "                    'I': str(v)\n",
    "                }\n",
    "            elif type(v) is dict:\n",
    "                resp[k] = {\n",
    "                    'M': dict_to_item(v)\n",
    "                }\n",
    "            elif type(v) is list:\n",
    "                resp[k] = []\n",
    "                for i in v:\n",
    "                    resp[k].append(dict_to_item(i))\n",
    "            elif type(v) is float:\n",
    "                resp[k] = Decimal(v)\n",
    "                    \n",
    "        return resp\n",
    "    elif type(raw) is str:\n",
    "        return {\n",
    "            'S': raw\n",
    "        }\n",
    "    elif type(raw) is int:\n",
    "        return {\n",
    "            'I': str(raw)\n",
    "        }\n",
    "    elif type(raw) is float:\n",
    "        return Decimal(raw)\n",
    "\n",
    "def numpy2s3(array, name, bucket):\n",
    "    \"\"\"\n",
    "    Write a numpy array to S3 as a file, without using local copy\n",
    "    \n",
    "    Arguments:\n",
    "    array -- Numpy array to save to s3\n",
    "    name -- file of the saved Numpy array\n",
    "    \"\"\"\n",
    "    f_out = io.BytesIO()\n",
    "    np.save(f_out, array)\n",
    "    try:\n",
    "        s3_client.put_object(Key=name, Bucket=bucket, Body=f_out.getvalue(), ACL='bucket-owner-full-control')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def dynamo_init():\n",
    "    \"\"\"\n",
    "    Create a DynamoDB table to track stat across iteration/epochs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table = dynamodb.create_table(\n",
    "            TableName = 'state',\n",
    "            KeySchema = [\n",
    "                {\n",
    "                    'AttributeName': 'epoch',\n",
    "                    'KeyType': 'HASH'\n",
    "                }\n",
    "            ],\n",
    "            AttributeDefinitions = [\n",
    "                {\n",
    "                    'AttributeName': 'epoch',\n",
    "                    'AttributeType': 'N'\n",
    "                }\n",
    "            ],\n",
    "            ProvisionedThroughput = {\n",
    "                'ReadCapacityUnits': 10,\n",
    "                'WriteCapacityUnits': 10\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Wait until the table is created\n",
    "        table.meta.client.get_waiter('table_exists').wait(TableName='state')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "    return table.name\n",
    "\n",
    "def storage_init():\n",
    "    \"\"\"\n",
    "    Create temporary S3 bucket to store numpy arrays\n",
    "    \n",
    "    Return:\n",
    "    tmp_bucket -- Name of the newly created S3 bucket\n",
    "    \"\"\"\n",
    "    tmp_bucket = 'lnn-{}'.format(uuid.uuid4())\n",
    "    print('Creating new bucket with name: {}'.format(tmp_bucket))\n",
    "    try:\n",
    "        s3_client.create_bucket(Bucket=tmp_bucket, CreateBucketConfiguration={'LocationConstraint': 'us-west-2'})\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "    return tmp_bucket\n",
    "\n",
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T\n",
    "\n",
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, standardized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255\n",
    "\n",
    "def initialize_data():\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes, dumps back to S3 for neurons to process as layer a^0\n",
    "\n",
    "    Returns:\n",
    "    bucket -- name of the S3 bucket of Numpy arrays\n",
    "    a_names -- list of the Numpy array names\n",
    "    \"\"\"\n",
    "\n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "    classes = np.array(dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    # Reshape labels\n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    # Preprocess inputs\n",
    "    train_set_x = standardize(train_set_x_orig)\n",
    "    test_set_x = standardize(test_set_x_orig)\n",
    "    \n",
    "    # Dump the inputs to the temporary s3 bucket for TrainerLambda\n",
    "    bucket = storage_init() # Creates a temporary bucket for the propogation steps\n",
    "    a_list = [train_set_x, train_set_y, test_set_x, test_set_y, classes] # List of Numpy array names\n",
    "    a_names = [] # list to store the Numpy array names as strings\n",
    "    dims = {} # dictionary of input data dimensions\n",
    "    for i in range(len(a_list)):\n",
    "        # Create a lis of the names of the numpy arrays\n",
    "        a_names.append(name2str(a_list[i], locals()))\n",
    "    for j in range(len(a_list)): \n",
    "        # Save the Numpy arrays to S3\n",
    "        numpy2s3(array=a_list[j], name=a_names[j][0], bucket=bucket)\n",
    "        dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    # Return the bucket and Numpy array names\n",
    "    return bucket, [j for i in a_names for j in i], dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    # Retrieve datasets and setting from S3\n",
    "    input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "    dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "    settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "    try:\n",
    "        response = input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(e.response['Error']['Message'])\n",
    "        else:\n",
    "            raise\n",
    "    try:\n",
    "        response = input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(e.response['Error']['Message'])\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Extract the neural network parameters\n",
    "    with open('/tmp/parameters.json') as parameters_file:\n",
    "        parameters = json.load(parameters_file)\n",
    "        \n",
    "    # Create payload to send to the trainer\n",
    "    payload = {}\n",
    "    payload['epochs'] = parameters['epochs']\n",
    "    payload['epoch'] = 1\n",
    "    payload['layers'] = parameters['layers']\n",
    "    payload['layer'] = 0\n",
    "    payload['activations'] = parameters['activations']\n",
    "    payload['neurons'] = parameters['neurons']\n",
    "    payload['w'] = parameters['weight']\n",
    "    payload['b'] = parameters['bias']\n",
    "    payload['s3_bucket'], \\\n",
    "    payload['input_data'], \\\n",
    "    payload['data_dimensions'] = initialize_data() # Returns S3 Bucket of input data to process\n",
    "    payload['state_table'] = dynamo_init()\n",
    "    payload['state'] = 'start'\n",
    "    payloadbytes = dumps(payload)\n",
    "    \n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new bucket with name: lnn-fd558ff5-cd62-420c-9ac0-a3866ec4a0e8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activations': {'layer1': 'sigmoid'},\n",
       " 'b': 0,\n",
       " 'data_dimensions': {'classes': (2,),\n",
       "  'test_set_x': (12288, 50),\n",
       "  'test_set_y': (1, 50),\n",
       "  'train_set_x': (12288, 209),\n",
       "  'train_set_y': (1, 209)},\n",
       " 'epoch': 1,\n",
       " 'epochs': 20,\n",
       " 'input_data': ['train_set_x',\n",
       "  'train_set_y',\n",
       "  'test_set_x',\n",
       "  'test_set_y',\n",
       "  'classes'],\n",
       " 'layer': 0,\n",
       " 'layers': 1,\n",
       " 'neurons': {'layer1': 1},\n",
       " 's3_bucket': 'lnn-fd558ff5-cd62-420c-9ac0-a3866ec4a0e8',\n",
       " 'state': 'start',\n",
       " 'state_table': 'state',\n",
       " 'w': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake context\n",
    "context = ''\n",
    "\n",
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simulate TrainerLambda ARN\n",
    "#environ[str('TrainerLambda')] = str(None)\n",
    "\n",
    "lambda_handler(event, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
