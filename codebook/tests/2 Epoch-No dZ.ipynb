{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Layer, Single Neuron Testing\n",
    "## Architecture\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"epochs\": 3,\n",
    "    \"layers\": 1,\n",
    "    \"activations\": {\n",
    "        \"layer1\": \"sigmoid\"\n",
    "    },\n",
    "    \"neurons\": {\n",
    "        \"layer1\": 1\n",
    "    },\n",
    "    \"weight\": 0,\n",
    "    \"bias\": 0,\n",
    "    \"learning_rate\": 0.005\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## LaunchLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "import redis\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Global Variables\n",
    "#rgn = environ['Region']\n",
    "rgn = 'us-west-2'\n",
    "s3_client = client('s3', region_name=rgn) # S3 access\n",
    "s3_resource = resource('s3')\n",
    "sns_client = client('sns', region_name=rgn) # SNS\n",
    "redis_client = client('elasticache', region_name=rgn)\n",
    "#Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "lambda_client = client('lambda', region_name=rgn) # Lambda invocations\n",
    "\n",
    "# Helper Functions\n",
    "def get_arns(function_name):\n",
    "    \"\"\"\n",
    "    Return the ARN for the LNN Functions.\n",
    "    Note: This addresses circular dependency issues in CloudFormation\n",
    "    \"\"\"\n",
    "    function_list = lambda_client.list_functions()\n",
    "    function_arn = None\n",
    "    for function in function_list['Functions']:\n",
    "        if function['FunctionName'] == function_name:\n",
    "            function_arn = function['FunctionArn']\n",
    "            break\n",
    "    return function_arn\n",
    "\n",
    "def publish_sns(sns_message):\n",
    "    \"\"\"\n",
    "    Publish message to the master SNS topic.\n",
    "\n",
    "    Arguments:\n",
    "    sns_message -- the Body of the SNS Message\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Publishing message to SNS topic...\")\n",
    "    sns_client.publish(TargetArn=environ['SNSArn'], Message=sns_message)\n",
    "    return\n",
    "\n",
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        if len(obj.shape) == 0:\n",
    "            length = 0\n",
    "            width = 0\n",
    "        else:\n",
    "            length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        sns_message = \"`to_cache` Error:\\n\" + str(type(obj)) + \"is not a supported serialization type\"\n",
    "        publish_sns(sns_message)\n",
    "        print(\"The Object is not a supported serialization type\")\n",
    "        raise\n",
    "\n",
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCacheendpoint\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        if int(length) == 0:\n",
    "            obj = np.float64(np.fromstring(val))\n",
    "        else:\n",
    "            obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        sns_message = \"`from_cache` Error:\\n\" + str(type(obj)) + \"is not a supported serialization type\"\n",
    "        publish_sns(sns_message)\n",
    "        print(\"The Object is not a supported de-serialization type\")\n",
    "        raise\n",
    "\n",
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T\n",
    "\n",
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255\n",
    "\n",
    "def initialize_data(endpoint, parameters):\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes and then dumps the data to ElastiCache \n",
    "    for neurons to process as layer a^0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "    \n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "    \n",
    "    # Reshape labels\n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # Preprocess inputs\n",
    "    train_set_x = standardize(train_set_x_orig)\n",
    "    test_set_x = standardize(test_set_x_orig)\n",
    "\n",
    "    # Create necessary keys for the data in ElastiCache\n",
    "    data_keys = {} # Dictionary for the hask keys of the data set\n",
    "    dims = {} # Dictionary of data set dimensions\n",
    "    a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "    a_names = [] # Placeholder for array names\n",
    "    for i in range(len(a_list)):\n",
    "        # Create a lis of the names of the numpy arrays\n",
    "        a_names.append(name2str(a_list[i], locals()))\n",
    "    for j in range(len(a_list)):\n",
    "        # Dump the numpy arrays to ElastiCache\n",
    "        data_keys[str(a_names[j][0])] = to_cache(endpoint=endpoint, obj=a_list[j], name=a_names[j][0])\n",
    "        # Append the array dimensions to the list\n",
    "        dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "    # Initialize A0 and Y names from `train_setx`` and train_set_y`\n",
    "    data_keys['A0'] = to_cache(endpoint=endpoint, obj=train_set_x, name='A0')\n",
    "    data_keys['Y'] = to_cache(endpoint=endpoint, obj=train_set_y, name='Y')\n",
    "    \n",
    "    # Initialize weights to zero for single layer\n",
    "    dim = dims.get('train_set_x')[0]\n",
    "    weights = np.zeros((dim, 1))\n",
    "    # Store the initial weights in ElastiCache\n",
    "    data_keys['weights'] = to_cache(endpoint=endpoint, obj=weights, name='weights')\n",
    "        \n",
    "    # Initialize Bias to zero for single layer\n",
    "    bias = 0\n",
    "    # Store the bias in ElastiCache\n",
    "    data_keys['bias'] = to_cache(endpoint=endpoint, obj=bias, name='bias')\n",
    "   \n",
    "    # Initialize training example size\n",
    "    m = train_set_x.shape[1]\n",
    "    data_keys['m'] = to_cache(endpoint, obj=m, name='m')\n",
    "    \n",
    "    # Initialize the results tracking object\n",
    "    results = {}\n",
    "    data_keys['results'] = to_cache(endpoint, obj=results, name='results')\n",
    "\n",
    "    # Initialize gradient tracking object for each layer\n",
    "    grads = {}\n",
    "    for l in range(1, parameters['layers']+1):\n",
    "        layer_name = 'layer' + str(l)\n",
    "        grads[layer_name] = {}\n",
    "    data_keys['grads'] = to_cache(endpoint=endpoint, obj=grads, name='grads')\n",
    "        \n",
    "    return data_keys, [j for i in a_names for j in i], dims\n",
    "\n",
    "def launch_handler(event, context):\n",
    "    # Retrieve datasets and setting from S3\n",
    "    input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "    dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "    settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "    try:\n",
    "        input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "        input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            sns_message = \"Error downloading input data from S3, S3 object does not exist\"\n",
    "            publish_sns(sns_message)\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Extract the neural network parameters\n",
    "    with open('/tmp/parameters.json') as parameters_file:\n",
    "        parameters = json.load(parameters_file)\n",
    "    \n",
    "    # Get the ARNs for the TrainerLambda and NeuronLambda\n",
    "    parameters['ARNs'] = {\n",
    "        'TrainerLambda': get_arns('TrainerLambda'),\n",
    "        'NeuronLambda': get_arns('NeuronLambda')\n",
    "    }\n",
    "\n",
    "    # Build in additional neural network parameters\n",
    "    # Input data sets and data set parameters\n",
    "    parameters['s3_bucket'] = event['Records'][0]['s3']['bucket']['name']\n",
    "    parameters['data_keys'],\\\n",
    "    parameters['input_data'],\\\n",
    "    parameters['dims'] = initialize_data(\n",
    "        endpoint=endpoint,\n",
    "        parameters=parameters\n",
    "    )\n",
    "    \n",
    "    # Initialize payload to `TrainerLambda`\n",
    "    payload = {}\n",
    "    # Initialize the overall state\n",
    "    payload['state'] = 'start'\n",
    "    # Dump the parameters to ElastiCache\n",
    "    payload['parameter_key'] = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "    #payload['endpoint'] = endpoint\n",
    "    # Prepare the payload for `TrainerLambda`\n",
    "    payloadbytes = dumps(payload)\n",
    "    \n",
    "    # Debug Statements\n",
    "    print(\"Complete Neural Network Settings: \\n\")\n",
    "    print(dumps(parameters, indent=4, sort_keys=True))\n",
    "    print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload))\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # Invoke TrainerLambda for next layer\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=parameters['ARNs']['TrainerLambda'],\n",
    "            InvocationType='Event',\n",
    "            Payload=payloadbytes\n",
    "            )\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        sns_message = \"Errors occurred invoking TrainerLambda from LaunchLambda.\"\n",
    "        sns_message += \"\\nError:\\n\" + e\n",
    "        sns_message += \"\\nCurrent Payload:\\n\" +  dumps(payload, indent=4, sort_keys=True)\n",
    "        publish_sns(sns_message)\n",
    "        print(e)\n",
    "        raise\n",
    "    print(response)\n",
    "    \"\"\"\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = ''\n",
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Neural Network Settings: \n",
      "\n",
      "{\n",
      "    \"ARNs\": {\n",
      "        \"NeuronLambda\": \"arn:aws:lambda:us-west-2:722812380636:function:NeuronLambda\",\n",
      "        \"TrainerLambda\": \"arn:aws:lambda:us-west-2:722812380636:function:TrainerLambda\"\n",
      "    },\n",
      "    \"activations\": {\n",
      "        \"layer1\": \"sigmoid\"\n",
      "    },\n",
      "    \"bias\": 0,\n",
      "    \"data_keys\": {\n",
      "        \"A0\": \"A0|float64#12288#209\",\n",
      "        \"Y\": \"Y|int64#1#209\",\n",
      "        \"bias\": \"bias|int\",\n",
      "        \"grads\": \"grads|json\",\n",
      "        \"m\": \"m|int\",\n",
      "        \"results\": \"results|json\",\n",
      "        \"test_set_x\": \"test_set_x|float64#12288#50\",\n",
      "        \"test_set_y\": \"test_set_y|int64#1#50\",\n",
      "        \"train_set_x\": \"train_set_x|float64#12288#209\",\n",
      "        \"train_set_y\": \"train_set_y|int64#1#209\",\n",
      "        \"weights\": \"weights|float64#12288#1\"\n",
      "    },\n",
      "    \"dims\": {\n",
      "        \"test_set_x\": [\n",
      "            12288,\n",
      "            50\n",
      "        ],\n",
      "        \"test_set_y\": [\n",
      "            1,\n",
      "            50\n",
      "        ],\n",
      "        \"train_set_x\": [\n",
      "            12288,\n",
      "            209\n",
      "        ],\n",
      "        \"train_set_y\": [\n",
      "            1,\n",
      "            209\n",
      "        ]\n",
      "    },\n",
      "    \"epochs\": 3,\n",
      "    \"input_data\": [\n",
      "        \"train_set_x\",\n",
      "        \"train_set_y\",\n",
      "        \"test_set_x\",\n",
      "        \"test_set_y\"\n",
      "    ],\n",
      "    \"layers\": 1,\n",
      "    \"learning_rate\": 0.005,\n",
      "    \"neurons\": {\n",
      "        \"layer1\": 1\n",
      "    },\n",
      "    \"s3_bucket\": \"lnn\",\n",
      "    \"weight\": 0\n",
      "}\n",
      "Payload to be sent to TrainerLambda: \n",
      "{\"state\": \"start\", \"parameter_key\": \"parameters|json\"}\n"
     ]
    }
   ],
   "source": [
    "launch_handler(event, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Epoch 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "import redis\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Global Variables\n",
    "#rgn = environ['Region']\n",
    "s3_client = client('s3', region_name=rgn) # S3 access\n",
    "s3_resource = resource('s3')\n",
    "sns_client = client('sns', region_name=rgn) # SNS\n",
    "lambda_client = client('lambda', region_name=rgn) # Lambda invocations\n",
    "redis_client = client('elasticache', region_name=rgn) # ElastiCache\n",
    "# Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "\n",
    "# Helper Functions\n",
    "def publish_sns(sns_message):\n",
    "    \"\"\"\n",
    "    Publish message to the master SNS topic.\n",
    "\n",
    "    Arguments:\n",
    "    sns_message -- the Body of the SNS Message\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Publishing message to SNS topic...\")\n",
    "    sns_client.publish(TargetArn=environ['SNSArn'], Message=sns_message)\n",
    "    return\n",
    "\n",
    "def numpy2s3(array, name, bucket):\n",
    "    \"\"\"\n",
    "    Serialize a Numpy array to S3 without using local copy\n",
    "    \n",
    "    Arguments:\n",
    "    array -- Numpy array of any shape\n",
    "    name -- filename on S3\n",
    "    \"\"\"\n",
    "    f_out = io.BytesIO()\n",
    "    np.save(f_out, array)\n",
    "    try:\n",
    "        s3_client.put_object(Key=name, Bucket=bucket, Body=f_out.getvalue(), ACL='bucket-owner-full-control')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        sns_message = \"The following error occurred while running `numpy2s3`:\\n\" + e\n",
    "        publish_sns(sns_message)\n",
    "        raise\n",
    "\n",
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        if len(obj.shape) == 0:\n",
    "            length = 0\n",
    "            width = 0\n",
    "        else:\n",
    "            length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        sns_message = \"`to_cache` Error:\\n\" + str(type(obj)) + \"is not a supported serialization type\"\n",
    "        publish_sns(sns_message)\n",
    "        print(\"The Object is not a supported serialization type\")\n",
    "        raise\n",
    "\n",
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCacheendpoint\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        if int(length) == 0:\n",
    "            obj = np.float64(np.fromstring(val))\n",
    "        else:\n",
    "            obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        sns_message = \"`from_cache` Error:\\n\" + str(type(obj)) + \"is not a supported serialization type\"\n",
    "        publish_sns(sns_message)\n",
    "        print(\"The Object is not a supported de-serialization type\")\n",
    "        raise\n",
    "\n",
    "def start_epoch(epoch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Starts a new epoch and configures the necessary state tracking objcts.\n",
    "    \n",
    "    Arguments:\n",
    "    epoch -- Integer representing the \"current\" epoch.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "    \"\"\"\n",
    "    # Initialize the results object for the new epoch\n",
    "    parameters = from_cache(endpoint=endpoint, key=parameter_key)\n",
    "    \n",
    "    # Add current epoch to results\n",
    "    epoch2results = from_cache(endpoint=endpoint, key=parameters['data_keys']['results'])\n",
    "    epoch2results['epoch' + str(epoch)] = {}\n",
    "    parameters['data_keys']['results'] = to_cache(endpoint=endpoint, obj=epoch2results, name='results')\n",
    "   \n",
    "    # Update paramaters with this functions data\n",
    "    parameters['epoch'] = epoch\n",
    "    parameters['layer'] = layer\n",
    "    parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "    \n",
    "    # Start forwardprop\n",
    "    propogate(direction='forward', epoch=epoch, layer=layer+1, parameter_key=parameter_key)\n",
    "\n",
    "def end(parameter_key):\n",
    "    \"\"\"\n",
    "    Finishes the oveall training sequence and saves the \"optmized\" \n",
    "    weights and bias to S3, for the prediction aplication.\n",
    "    \n",
    "    Arguments:\n",
    "    parameter_key -- The ElastiCache key for the current set of state parameters.\n",
    "    \"\"\"\n",
    "    # Get the latest parameters\n",
    "    parameters = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameter_key\n",
    "    )\n",
    "\n",
    "    # Get the results key\n",
    "    final_results = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameters['data_keys']['results']\n",
    "    )\n",
    "    # Upload the final results to S3\n",
    "    bucket = parameters['s3_bucket']\n",
    "    results_obj = s3_resource.Object(bucket,'training_results/results.json')\n",
    "    try:\n",
    "        results_obj.put(Body=json.dumps(final_results))\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    \n",
    "    # Get the final Weights and Bias\n",
    "    weights = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameters['data_keys']['weights']\n",
    "    )\n",
    "    bias = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameters['data_keys']['bias']\n",
    "    )\n",
    "    \n",
    "    # Put the weights and bias onto S3 for prediction\n",
    "    numpy2s3(array=weights, name='predict_input/weights', bucket=bucket)\n",
    "    numpy2s3(array=bias, name='predict_input/bias', bucket=bucket)\n",
    "\n",
    "    sns_message = \"Training Completed Successfully!\\n\" + dumps(final_results, indent=4, sort_keys=True)\n",
    "\n",
    "    return\n",
    "\n",
    "def propogate(direction, epoch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Determines the amount of \"hidden\" units based on the layer and loops\n",
    "    through launching the necessary `NeuronLambda` functions with the \n",
    "    appropriate state. Each `NeuronLambda` implements the cost function \n",
    "    OR the gradients depending on the direction.\n",
    "\n",
    "    Arguments:\n",
    "    direction -- The current direction of the propogation, either `forward` or `backward`.\n",
    "    epoch -- Integer representing the \"current\" epoch to close out.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "    \"\"\"    \n",
    "    # Get the parameters for the layer\n",
    "    parameters = from_cache(endpoint=endpoint, key=parameter_key)\n",
    "    num_hidden_units = parameters['neurons']['layer' + str(layer)]\n",
    "    \n",
    "    # Build the NeuronLambda payload\n",
    "    payload = {}\n",
    "    # Add the parameters to the payload\n",
    "    payload['state'] = direction\n",
    "    payload['parameter_key'] = parameter_key\n",
    "    payload['epoch'] = epoch\n",
    "    payload['layer'] = layer\n",
    "\n",
    "    # Determine process based on direction\n",
    "    if direction == 'forward':\n",
    "        # Launch Lambdas to propogate forward\n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this function's updates\n",
    "        parameters['epoch'] = epoch\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "        print(\"Starting Forward Propogation for epoch \" + str(epoch) + \", layer \" + str(layer))\n",
    "\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            # Prepare the payload for `NeuronLambda`\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            payloadbytes = dumps(payload)\n",
    "            print(\"Payload to be sent NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "\n",
    "            \"\"\"\n",
    "            # Invoke NeuronLambdas for next layer\n",
    "            try:\n",
    "                response = lambda_client.invoke(\n",
    "                    FunctionName=parameters['ARNs']['NeuronLambda'],\n",
    "                    InvocationType='Event',\n",
    "                    Payload=payloadbytes\n",
    "                )\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                sns_message = \"Errors occurred invoking Neuron Lambda from TrainerLambda.\"\n",
    "                sns_message += \"\\nError:\\n\" + e\n",
    "                sns_message += \"\\nCurrent Payload:\\n\" +  dumps(payload, indent=4, sort_keys=True)\n",
    "                publish_sns(sns_message)\n",
    "                print(e)\n",
    "                raise\n",
    "            print(response)\n",
    "            \"\"\"\n",
    "        \n",
    "        return\n",
    "    \n",
    "    elif direction == 'backward':\n",
    "        # Launch Lambdas to propogate backward        \n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this functions updates\n",
    "        parameters['epoch'] = epoch\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "        print(\"Starting Backward Propogation for epoch \" + str(epoch) + \", layer \" + str(layer))\n",
    "\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            # Prepare the payload for `NeuronLambda`\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            payloadbytes = dumps(payload)\n",
    "            print(\"Payload to be sent to NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "\n",
    "            \"\"\"\n",
    "            # Invoke NeuronLambdas for next layer\n",
    "            try:\n",
    "                response = lambda_client.invoke(\n",
    "                    FunctionName=parameters['ARNs']['NeuronLambda'],\n",
    "                    InvocationType='Event',\n",
    "                    Payload=payloadbytes\n",
    "                )\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                sns_message = \"Errors occurred invoking Neuron Lambda from TrainerLambda.\"\n",
    "                sns_message += \"\\nError:\\n\" + e\n",
    "                sns_message += \"\\nCurrent Payload:\\n\" +  dumps(payload, indent=4, sort_keys=True)\n",
    "                publish_sns(sns_message)\n",
    "                print(e)\n",
    "                raise\n",
    "            print(response)\n",
    "            \"\"\"\n",
    "\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        sns_message = \"Errors processing `propogate()` function.\"\n",
    "        publish_sns(sns_message)\n",
    "        raise\n",
    "\n",
    "\n",
    "def trainer_handler(event, context):\n",
    "    \"\"\"\n",
    "    Processes the `event` vaiables from the various Lambda functions that call it, \n",
    "    i.e. `TrainerLambda` and `NeuronLambda`. Determines the \"current\" state and\n",
    "    then directs the next steps.\n",
    "    \"\"\"    \n",
    "    # Get the current state from the invoking lambda\n",
    "    state = event.get('state')\n",
    "    global parameters\n",
    "    parameters = from_cache(endpoint=endpoint, key=event.get('parameter_key'))\n",
    "    \n",
    "    # Execute appropriate action based on the the current state\n",
    "    if state == 'forward':\n",
    "        # Get important state variables\n",
    "        epoch = event.get('epoch')\n",
    "        layer = event.get('layer')\n",
    "\n",
    "        # First pre-process the Activations from the \"previous\" layer\n",
    "        # Use the folling Redis command to ensure a pure string is return for the key\n",
    "        r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "        key_list = []\n",
    "        # Compile a list of activations\n",
    "        for key in r.scan_iter(match='layer'+str(layer-1)+'_a_*'):\n",
    "            key_list.append(key)\n",
    "        # Create a dictionary of activation results\n",
    "        A_dict = {}\n",
    "        for i in key_list:\n",
    "            A_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "        # Number of Neuron Activations\n",
    "        num_activations = len(key_list)\n",
    "        # Create a numpy array of the results, depending on the number\n",
    "        # of hidden units\n",
    "        A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "        if num_activations == 1:\n",
    "            \"\"\"\n",
    "            Note: This assumes a single hidden unit for the last layer\n",
    "            \"\"\"\n",
    "            dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "            #debug\n",
    "            #print(\"Dimensions to reshape single hidden unit activations: \" + str(dims))\n",
    "            A = A.reshape(int(dims[0]), int(dims[1]))\n",
    "            assert(A.shape == (parameters['dims']['train_set_y'][0], parameters['dims']['train_set_y'][1]))\n",
    "        else:\n",
    "            A = np.squeeze(A)\n",
    "            assert(A.shape == (parameters['neurons']['layer'+str(layer-1)], parameters['dims']['train_set_x'][1]))\n",
    "        # Add the `A` Matrix to `data_keys` for later Neuron use\n",
    "        A_name = 'A' + str(layer-1)\n",
    "        parameters['data_keys'][A_name] = to_cache(endpoint=endpoint, obj=A, name=A_name)\n",
    "\n",
    "        # Update ElastiCache with this funciton's data\n",
    "        parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "        \n",
    "        # Determine the location within forwardprop\n",
    "        if layer > parameters['layers']:\n",
    "            # Location is at the end of forwardprop (layer 3), therefore calculate Cost\n",
    "            # Get the training examples data\n",
    "            Y = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_y'])\n",
    "            m = Y.shape[1]\n",
    "            \n",
    "            # Calculate the Cost\n",
    "            cost = -1 / m * np.sum(np.multiply(Y, np.log(A)) + np.multiply((1 - Y), np.log(1 - A)))\n",
    "            cost = np.squeeze(cost)\n",
    "            assert(cost.shape == ())\n",
    "\n",
    "            # Update results with the Cost\n",
    "            # Get the results object\n",
    "            cost2results = from_cache(endpoint=endpoint, key=parameters['data_keys']['results'])\n",
    "            # Append the cost to results object\n",
    "            cost2results['epoch' + str(epoch)]['cost'] = cost\n",
    "            # Update results key in ElastiCache\n",
    "            parameters['data_keys']['results'] = to_cache(endpoint=endpoint, obj=cost2results, name='results')\n",
    "\n",
    "            print(\"Cost after epoch {0}: {1}\".format(epoch, cost))\n",
    "\n",
    "            # Initialize backprop\n",
    "            \"\"\"\n",
    "            Not going to caclulate dZ\n",
    "            # Calculate the derivative of the Cost with respect to the last activation\n",
    "            # Ensure that `Y` is the correct shape as the last activation\n",
    "            #Y = Y.reshape(A.shape)\n",
    "            #dZ = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "            #dZ_name = 'dZ' + str(layer-1)\n",
    "            #parameters['data_keys'][dZ_name] = to_cache(endpoint=endpoint, obj=dZ, name=dZ_name)\n",
    "\n",
    "            # Update parameters from theis function in ElastiCache\n",
    "            #parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "            \"\"\"\n",
    "\n",
    "            # Start Backpropogation\n",
    "            # This should start with layer (layers = 3-1)\n",
    "            propogate(direction='backward', epoch=epoch, layer=layer-1, parameter_key=parameter_key)\n",
    "            \n",
    "        else:\n",
    "            # Move to the next hidden layer\n",
    "            #debug\n",
    "            print(\"Propogating forward onto Layer \" + str(layer))\n",
    "            propogate(direction='forward', epoch=epoch, layer=layer, parameter_key=parameter_key)\n",
    "        \n",
    "    elif state == 'backward':\n",
    "        # Get important state variables\n",
    "        epoch = event.get('epoch')\n",
    "        layer = event.get('layer')\n",
    "        \n",
    "        # Determine the location within backprop\n",
    "        if epoch == parameters['epochs']-1 and layer == 0:\n",
    "            # Location is at the end of the final epoch\n",
    "            # Retieve the \"params\"\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            w = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=parameters['data_keys']['weights']\n",
    "            )\n",
    "            b = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=parameters['data_keys']['bias']\n",
    "            )\n",
    "\n",
    "            # Retrieve the gradients\n",
    "            grads = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=parameters['data_keys']['grads']\n",
    "            )\n",
    "            dw = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=grads['layer'+ str(layer + 1)]['dw']\n",
    "            )\n",
    "            db = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=grads['layer'+ str(layer + 1)]['db']\n",
    "            )\n",
    "\n",
    "            # Run Gradient Descent\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "            # Update ElastiCache with the Weights and Bias so be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['weights'] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=w,\n",
    "                name='weights'\n",
    "            )\n",
    "            parameters['data_keys']['bias'] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='bias'\n",
    "            )\n",
    "            \n",
    "            # Update paramters for the next epoch\n",
    "            parameter_key = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=parameters,\n",
    "                name='parameters'\n",
    "            )\n",
    "                        \n",
    "            # Finalize the the process and clean up\n",
    "            end(parameter_key=parameter_key)\n",
    "            \n",
    "        elif epoch < parameters['epochs']-1 and layer == 0:\n",
    "            # Location is at the end of the current epoch and backprop is finished\n",
    "            # Retieve the \"params\"\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            w = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=parameters['data_keys']['weights']\n",
    "            )\n",
    "            b = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=parameters['data_keys']['bias']\n",
    "            )\n",
    "\n",
    "            # Retrieve the gradients\n",
    "            grads = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=parameters['data_keys']['grads']\n",
    "            )\n",
    "            dw = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=grads['layer'+ str(layer + 1)]['dw']\n",
    "            )\n",
    "            db = from_cache(\n",
    "                endpoint=endpoint,\n",
    "                key=grads['layer'+ str(layer + 1)]['db']\n",
    "            )\n",
    "\n",
    "            # Run Gradient Descent\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "            # Update ElastiCache with the Weights and Bias so be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['weights'] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=w,\n",
    "                name='weights'\n",
    "            )\n",
    "            parameters['data_keys']['bias'] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='bias'\n",
    "            )\n",
    "            \n",
    "            # Update paramters for the next epoch\n",
    "            parameter_key = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=parameters,\n",
    "                name='parameters'\n",
    "            )\n",
    "                        \n",
    "            # Start the next epoch\n",
    "            start_epoch(epoch=epoch+1, layer=0, parameter_key=parameter_key)\n",
    "            \n",
    "        else:\n",
    "            # Move to the next hidden layer\n",
    "            propogate(direction='backward', epoch=epoch, layer=layer, parameter_key=parameter_key)\n",
    "            \n",
    "    elif state == 'start':\n",
    "        # Start of a new run of the process        \n",
    "        # Create initial parameters\n",
    "        epoch = 0\n",
    "        layer = 0\n",
    "        start_epoch(epoch=epoch, layer=layer, parameter_key=event.get('parameter_key'))\n",
    "       \n",
    "    else:\n",
    "        sns_message = \"General error processing TrainerLambda handler!\"\n",
    "        publish_sns(sns_message)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Forward Propogation for epoch 0, layer 1\n",
      "Payload to be sent NeuronLambda: \n",
      "{\n",
      "    \"activation\": \"sigmoid\",\n",
      "    \"epoch\": 0,\n",
      "    \"id\": 1,\n",
      "    \"last\": \"True\",\n",
      "    \"layer\": 1,\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"forward\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "event = {\"state\": \"start\", \"parameter_key\": \"parameters|json\"}\n",
    "trainer_handler(event, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "import redis\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Global Variables\n",
    "#rgn = environ['Region']\n",
    "s3_client = client('s3', region_name=rgn) # S3 access\n",
    "s3_resource = resource('s3')\n",
    "sns_client = client('sns', region_name=rgn) # SNS\n",
    "redis_client = client('elasticache', region_name=rgn)\n",
    "lambda_client = client('lambda', region_name=rgn) # Lambda invocations\n",
    "# Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "\n",
    "# Helper Functions\n",
    "def publish_sns(sns_message):\n",
    "    \"\"\"\n",
    "    Publish message to the master SNS topic.\n",
    "\n",
    "    Arguments:\n",
    "    sns_message -- the Body of the SNS Message\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Publishing message to SNS topic...\")\n",
    "    sns_client.publish(TargetArn=environ['SNSArn'], Message=sns_message)\n",
    "    return\n",
    "\n",
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        if len(obj.shape) == 0:\n",
    "            length = 0\n",
    "            width = 0\n",
    "        else:\n",
    "            length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        sns_message = \"`to_cache` Error:\\n\" + str(type(obj)) + \"is not a supported serialization type\"\n",
    "        publish_sns(sns_message)\n",
    "        print(\"The Object is not a supported serialization type\")\n",
    "        raise\n",
    "\n",
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCacheendpoint\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        if int(length) == 0:\n",
    "            obj = np.float64(np.fromstring(val))\n",
    "        else:\n",
    "            obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        sns_message = \"`from_cache` Error:\\n\" + str(type(obj)) + \"is not a supported serialization type\"\n",
    "        publish_sns(sns_message)\n",
    "        print(\"The Object is not a supported de-serialization type\")\n",
    "        raise\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return s\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function.\n",
    "\n",
    "    Arguments:\n",
    "    z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    a -- Post-activation parameter, of the same shape as z\n",
    "    \"\"\"\n",
    "\n",
    "    a = np.maximum(0, z)\n",
    "\n",
    "    assert(A.shape == z.shape)\n",
    "\n",
    "    return a\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def neuron_handler(event, context):\n",
    "    \"\"\"\n",
    "    This Lambda Funciton simulates a single Perceptron for both \n",
    "    forward and backward propogation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the Neural Network paramaters from Elasticache\n",
    "    parameters = from_cache(endpoint, key=event.get('parameter_key'))\n",
    "       \n",
    "    # Get the current state\n",
    "    state = event.get('state')\n",
    "    epoch = event.get('epoch')\n",
    "    layer = event.get('layer')\n",
    "    ID = event.get('id') # To be used when multiple activations\n",
    "    # Determine is this is the last Neuron in the layer\n",
    "    last = event.get('last')\n",
    "\n",
    "    # Get data to process\n",
    "    #w = from_cache(endpoint=endpoint, key=parameters['data_keys']['weights'])\n",
    "    #b = from_cache(endpoint=endpoint, key=parameters['data_keys']['bias'])\n",
    "    #X = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_x'])\n",
    "    #Y = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_y'])\n",
    "    #m = from_cache(endpoint=endpoint, key=parameters['data_keys']['m'])\n",
    "\n",
    "    if state == 'forward':\n",
    "        # Forward propogation from X to Cost\n",
    "        activation = event.get('activation')\n",
    "        A = from_cache(endpoint=endpoint, key=parameters['data_keys']['A'+str(layer - 1)])\n",
    "        w = from_cache(endpoint=endpoint, key=parameters['data_keys']['weights'])\n",
    "        b = from_cache(endpoint=endpoint, key=parameters['data_keys']['bias'])\n",
    "        if activation == 'sigmoid':\n",
    "            a = sigmoid(np.dot(w.T, A) + b) # Single Neuron activation\n",
    "        else:\n",
    "            # No other functions supported on single layer at this time\n",
    "            pass\n",
    "        \n",
    "        # Upload the results to ElastiCache for `TrainerLambda` to process\n",
    "        to_cache(endpoint=endpoint, obj=a, name='layer'+str(layer)+'_a_'+str(ID))\n",
    "        \n",
    "        if last == \"True\":\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['epoch'] = epoch\n",
    "            parameters['layer'] = layer + 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'forward'\n",
    "            payload['epoch'] = epoch\n",
    "            payload['layer'] = layer + 1\n",
    "            payloadbytes = dumps(payload)\n",
    "            print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "\n",
    "            \"\"\"# Invoke TrainerLambda to process activations\n",
    "            try:\n",
    "                response = lambda_client.invoke(\n",
    "                    FunctionName=parameters['ARNs']['TrainerLambda'],\n",
    "                    InvocationType='Event',\n",
    "                    Payload=payloadbytes\n",
    "                )\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                sns_message = \"Errors occurred invoking Trainer Lambd from NeuronLambdaa.\"\n",
    "                sns_message += \"\\nError:\\n\" + e\n",
    "                sns_message += \"\\nCurrent Payload:\\n\" +  dumps(payload, indent=4, sort_keys=True)\n",
    "                publish_sns(sns_message)\n",
    "                print(e)\n",
    "                raise\n",
    "            print(response)\n",
    "            \"\"\"\n",
    "\n",
    "        return\n",
    "\n",
    "    elif state == 'backward':\n",
    "        # Backprop from Cost to X (A0)\n",
    "        activation = event.get('activation')\n",
    "        \"\"\"\n",
    "        Note: Note going to calculate dZ\n",
    "        dZ_name = 'dZ' + str(layer)\n",
    "        dZ = from_cache(\n",
    "            endpoint=endpoint,\n",
    "            key=parameters['data_keys'][dZ_name]\n",
    "        )\n",
    "        \"\"\"\n",
    "        # Get the activaion from the current layer\n",
    "        A = from_cache(\n",
    "            endpoint=endpoint,\n",
    "            key=parameters['data_keys']['A' + str(layer)]\n",
    "        )\n",
    "        X = from_cache(\n",
    "            endpoint=endpoint,\n",
    "            key=parameters['data_keys']['train_set_x']\n",
    "        )\n",
    "        m = X.shape[1]\n",
    "        Y = from_cache(\n",
    "            endpoint=endpoint,\n",
    "            key=parameters['data_keys']['train_set_y']\n",
    "        )\n",
    "        # Backward propogation to determine gradients of current layer\n",
    "        dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "        db = (1 / m) * np.sum(A - Y)\n",
    "\n",
    "        # Debug\n",
    "        w = from_cache(endpoint=endpoint, key=parameters['data_keys']['weights'])\n",
    "        assert(dw.shape == w.shape)\n",
    "       \n",
    "        # Capture gradients\n",
    "        # Load the grads object\n",
    "        grads = from_cache(endpoint, key=parameters['data_keys']['grads'])\n",
    "        # Update the grads object with the calculated derivatives\n",
    "        grads['layer' + str(layer)]['dw'] = to_cache(\n",
    "            endpoint=endpoint,\n",
    "            obj=dw,\n",
    "            name='dw'\n",
    "        )\n",
    "        grads['layer' + str(layer)]['db'] = to_cache(\n",
    "            endpoint=endpoint,\n",
    "            obj=db,\n",
    "            name='db'\n",
    "        )\n",
    "        # Update the pramaters (local)\n",
    "        parameters['data_keys']['grads'] = to_cache(\n",
    "            endpoint=endpoint,\n",
    "            obj=grads,\n",
    "            name='grads'\n",
    "        )\n",
    "\n",
    "        if last == \"True\":\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['epoch'] = epoch\n",
    "            parameters['layer'] = layer - 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'backward'\n",
    "            payload['epoch'] = epoch\n",
    "            payload['layer'] = layer - 1\n",
    "            payloadbytes = dumps(payload)\n",
    "            print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "\n",
    "            \"\"\"# Invoke NeuronLambdas for next layer\n",
    "            try:\n",
    "                response = lambda_client.invoke(\n",
    "                    FunctionName=parameters['ARNs']['TrainerLambda'], #ENSURE ARN POPULATED BY CFN\n",
    "                    InvocationType='Event',\n",
    "                    Payload=payloadbytes\n",
    "                )\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                sns_message = \"Errors occurred invoking Trainer Lambda from NauronLambda.\"\n",
    "                sns_message += \"\\nError:\\n\" + e\n",
    "                sns_message += \"\\nCurrent Payload:\\n\" +  dumps(payload, indent=4, sort_keys=True)\n",
    "                publish_sns(sns_message)\n",
    "                print(e)\n",
    "                raise\n",
    "            print(response)\n",
    "            \"\"\"\n",
    "\n",
    "        return\n",
    "\n",
    "    else:\n",
    "        sns_message = \"General error processing NeuronLambda handler.\"\n",
    "        publish_sns(sns_message)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload to be sent to TrainerLambda: \n",
      "{\n",
      "    \"epoch\": 0,\n",
      "    \"layer\": 2,\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"forward\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "event={\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"epoch\": 0,\n",
    "    \"id\": 1,\n",
    "    \"last\": \"True\",\n",
    "    \"layer\": 1,\n",
    "    \"parameter_key\": \"parameters|json\",\n",
    "    \"state\": \"forward\"\n",
    "}\n",
    "neuron_handler(event, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.6931471805599453\n",
      "Starting Backward Propogation for epoch 0, layer 1\n",
      "Payload to be sent to NeuronLambda: \n",
      "{\n",
      "    \"activation\": \"sigmoid\",\n",
      "    \"epoch\": 0,\n",
      "    \"id\": 1,\n",
      "    \"last\": \"True\",\n",
      "    \"layer\": 1,\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"backward\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "event = {\n",
    "    \"epoch\": 0,\n",
    "    \"layer\": 2,\n",
    "    \"parameter_key\": \"parameters|json\",\n",
    "    \"state\": \"forward\"\n",
    "}\n",
    "trainer_handler(event, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload to be sent to TrainerLambda: \n",
      "{\n",
      "    \"epoch\": 0,\n",
      "    \"layer\": 0,\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"backward\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "event = {\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"epoch\": 0,\n",
    "    \"id\": 1,\n",
    "    \"last\": \"True\",\n",
    "    \"layer\": 1,\n",
    "    \"parameter_key\": \"parameters|json\",\n",
    "    \"state\": \"backward\"\n",
    "}\n",
    "neuron_handler(event, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Forward Propogation for epoch 1, layer 1\n",
      "Payload to be sent NeuronLambda: \n",
      "{\n",
      "    \"activation\": \"sigmoid\",\n",
      "    \"epoch\": 1,\n",
      "    \"id\": 1,\n",
      "    \"last\": \"True\",\n",
      "    \"layer\": 1,\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"forward\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "event = {\n",
    "    \"epoch\": 0,\n",
    "    \"layer\": 0,\n",
    "    \"parameter_key\": \"parameters|json\",\n",
    "    \"state\": \"backward\"\n",
    "}\n",
    "trainer_handler(event, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Epoch 1 (Manual)\n",
    "### NeuronLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06666667,  0.76862745,  0.32156863, ...,  0.56078431,\n",
       "         0.08627451,  0.03137255],\n",
       "       [ 0.12156863,  0.75294118,  0.27843137, ...,  0.60784314,\n",
       "         0.09411765,  0.10980392],\n",
       "       [ 0.21960784,  0.74509804,  0.26666667, ...,  0.64705882,\n",
       "         0.09019608,  0.20784314],\n",
       "       ..., \n",
       "       [ 0.        ,  0.32156863,  0.54117647, ...,  0.33333333,\n",
       "         0.01568627,  0.        ],\n",
       "       [ 0.        ,  0.31372549,  0.55294118, ...,  0.41960784,\n",
       "         0.01960784,  0.        ],\n",
       "       [ 0.        ,  0.31764706,  0.55686275, ...,  0.58431373,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last = \"True\"\n",
    "epoch = 1\n",
    "ID = 1\n",
    "layer = 1\n",
    "state = 'forward'\n",
    "\n",
    "A = from_cache(endpoint=endpoint, key=parameters['data_keys']['A'+str(layer - 1)])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00023604],\n",
       "       [-0.00031499],\n",
       "       [-0.00024618],\n",
       "       ..., \n",
       "       [-0.00025373],\n",
       "       [-0.00031063],\n",
       "       [-0.00016226]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = from_cache(endpoint=endpoint, key=parameters['data_keys']['weights'])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00077751196172248804"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = from_cache(endpoint=endpoint, key=parameters['data_keys']['bias'])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27144261,  0.16639514,  0.13248549,  0.28087109,  0.27461314,\n",
       "         0.29591057,  0.27611776,  0.24298602,  0.19551452,  0.22569329,\n",
       "         0.12764666,  0.17301196,  0.23184818,  0.11594778,  0.14322922,\n",
       "         0.10592619,  0.12273193,  0.08516622,  0.14873928,  0.31005661,\n",
       "         0.16082053,  0.26238699,  0.08161255,  0.10040451,  0.13597563,\n",
       "         0.35536701,  0.18820562,  0.17195463,  0.24222168,  0.20061727,\n",
       "         0.31236899,  0.14191619,  0.17288403,  0.14902643,  0.23378567,\n",
       "         0.25391345,  0.20171301,  0.09324267,  0.42101197,  0.20425983,\n",
       "         0.19408046,  0.34390601,  0.16139733,  0.13635434,  0.17902012,\n",
       "         0.28251275,  0.17133742,  0.23124183,  0.2384457 ,  0.20163473,\n",
       "         0.10014528,  0.16141084,  0.15266961,  0.10024169,  0.2385967 ,\n",
       "         0.10757844,  0.21302844,  0.07860407,  0.33613885,  0.26416766,\n",
       "         0.25402315,  0.1922326 ,  0.13738444,  0.21277865,  0.0836154 ,\n",
       "         0.26073528,  0.12913124,  0.18987392,  0.14887659,  0.13252753,\n",
       "         0.15520007,  0.23492077,  0.31447477,  0.23596402,  0.17559356,\n",
       "         0.14015952,  0.22202713,  0.21967104,  0.23285111,  0.35771344,\n",
       "         0.18092794,  0.14578638,  0.30113728,  0.28471829,  0.16074314,\n",
       "         0.17039595,  0.13718026,  0.25816609,  0.1664884 ,  0.29518392,\n",
       "         0.14547642,  0.1953313 ,  0.21758611,  0.20109884,  0.1606474 ,\n",
       "         0.09862609,  0.11552531,  0.21059657,  0.10186284,  0.15002358,\n",
       "         0.14717797,  0.19907336,  0.14557384,  0.19726037,  0.2395638 ,\n",
       "         0.11994026,  0.12925446,  0.19096142,  0.19879251,  0.14222985,\n",
       "         0.14274767,  0.18973411,  0.28616837,  0.14336288,  0.1158658 ,\n",
       "         0.16788038,  0.18058762,  0.21090319,  0.2499734 ,  0.16795022,\n",
       "         0.19028636,  0.10807592,  0.19053086,  0.14983368,  0.17312881,\n",
       "         0.20625604,  0.185481  ,  0.24452399,  0.12257758,  0.30031867,\n",
       "         0.20724035,  0.15319453,  0.21656755,  0.25256323,  0.23076588,\n",
       "         0.10930603,  0.11495842,  0.11444984,  0.17098675,  0.24063294,\n",
       "         0.16477561,  0.15031771,  0.17806213,  0.3193831 ,  0.10863638,\n",
       "         0.19502466,  0.12134945,  0.14427406,  0.33009279,  0.14495097,\n",
       "         0.1067483 ,  0.27948344,  0.17942104,  0.15610314,  0.27710856,\n",
       "         0.16509351,  0.16845092,  0.2577705 ,  0.12433062,  0.20047114,\n",
       "         0.23329252,  0.15448514,  0.1374685 ,  0.19401536,  0.1165579 ,\n",
       "         0.26320064,  0.13755724,  0.17333636,  0.21015629,  0.12484642,\n",
       "         0.15018892,  0.18745561,  0.12368108,  0.13521015,  0.14642664,\n",
       "         0.23577053,  0.18009183,  0.19332766,  0.21941863,  0.1916846 ,\n",
       "         0.21612492,  0.11468959,  0.17319335,  0.12983148,  0.1965756 ,\n",
       "         0.17172626,  0.23468451,  0.17269887,  0.14641714,  0.17953287,\n",
       "         0.10859271,  0.11132706,  0.17285207,  0.08645172,  0.21770992,\n",
       "         0.46116505,  0.17279542,  0.18642195,  0.22222226,  0.22292108,\n",
       "         0.15162614,  0.13277835,  0.17583832,  0.1915616 ,  0.0886732 ,\n",
       "         0.406961  ,  0.15920532,  0.41594619,  0.28783577]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sigmoid(np.dot(w.T, A) + b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 209)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer1_a_1|float64#1#209'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cache(endpoint=endpoint, obj=a, name='layer'+str(layer)+'_a_'+str(ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainerLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 209)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 2\n",
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "key_list = []\n",
    "# Compile a list of activations\n",
    "for key in r.scan_iter(match='layer'+str(layer-1)+'_a_*'):\n",
    "    key_list.append(key)\n",
    "# Create a dictionary of activation results\n",
    "A_dict = {}\n",
    "for i in key_list:\n",
    "    A_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "# Number of Neuron Activations\n",
    "num_activations = len(key_list)\n",
    "# Create a numpy array of the results, depending on the number\n",
    "# of hidden units\n",
    "A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 209)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "A = A.reshape(int(dims[0]), int(dims[1]))\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the `A` Matrix to `data_keys` for later Neuron use\n",
    "A_name = 'A' + str(layer-1)\n",
    "parameters['data_keys'][A_name] = to_cache(endpoint=endpoint, obj=A, name=A_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_y'])\n",
    "m = Y.shape[1]\n",
    "            \n",
    "# Calculate the Cost\n",
    "cost = -1 / m * np.sum(np.multiply(Y, np.log(A)) + np.multiply((1 - Y), np.log(1 - A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74102941450651827"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74102941450651827"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = (-1 / m) * np.sum(Y * (np.log(A)) + ((1 - Y) * np.log(1 - A)))\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
