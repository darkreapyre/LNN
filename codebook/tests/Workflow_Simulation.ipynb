{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "---\n",
    "\n",
    "**BLAH BLAH BLAH**\n",
    "\n",
    "---\n",
    "\n",
    "# Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations\n",
    "# Retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "cache = redis(host=endpoint, port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_data(endpoint, w, b):\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes and then dumps the data to ElastiCache \n",
    "    for neurons to process as layer a^0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "    \n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "    \n",
    "    # Reshape labels\n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # Preprocess inputs\n",
    "    train_set_x = standardize(train_set_x_orig)\n",
    "    test_set_x = standardize(test_set_x_orig)\n",
    "\n",
    "    # Dump the inputs to the temporary s3 bucket for TrainerLambda\n",
    "    #bucket = storage_init() # Creates a temporary bucket for the propogation steps\n",
    "    data_keys = {} # Dictionary for the hask keys of the data set\n",
    "    dims = {} # Dictionary of data set dimensions\n",
    "    a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "    a_names = [] # Placeholder for array names\n",
    "    for i in range(len(a_list)):\n",
    "        # Create a lis of the names of the numpy arrays\n",
    "        a_names.append(name2str(a_list[i], locals()))\n",
    "    for j in range(len(a_list)):\n",
    "        # Dump the numpy arrays to ElastiCache\n",
    "        data_keys[str(a_names[j][0])] = to_cache(endpoint, obj=a_list[j], name=a_names[j][0])\n",
    "        # Append the array dimensions to the list\n",
    "        dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "    # Initialize weights\n",
    "    if w == 0: # Initialize weights to dimensions of the input data\n",
    "        dim = dims.get('train_set_x')[0]\n",
    "        weights = np.zeros((dim, 1))\n",
    "        # Store the initial weights as a column vector on S3\n",
    "        data_keys['weights'] = to_cache(endpoint, obj=weights, name='weights')\n",
    "    else:\n",
    "        #placeholder for random weight initialization\n",
    "        pass\n",
    "        \n",
    "    # Initialize Bias\n",
    "    if b != 0:\n",
    "        #placeholder for random bias initialization\n",
    "        #data_keys['bias'] = to_cache(endpoint, obj=bias, name='bias')\n",
    "        pass\n",
    "    else:\n",
    "        data_keys['bias'] = to_cache(endpoint, obj=b, name='bias')\n",
    "    \n",
    "    # Initialize training example size\n",
    "    m = train_set_x.shape[1]\n",
    "    data_keys['m'] = to_cache(endpoint, obj=m, name='m')\n",
    "    \n",
    "#    # Initialize the results tracking object\n",
    "#    to_cache(endpoint, dump='', name='results')\n",
    "        \n",
    "    return data_keys, [j for i in a_names for j in i], dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Origional version\n",
    "```python\n",
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    if 'numpy.ndarray' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    elif type(obj) is dict:\n",
    "        #x = json.dumps(obj)\n",
    "        #val = json.loads(x)\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported serialization type\")\n",
    "\n",
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint.\n",
    "    key -- Name of the Key to retrieve the object.\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        data = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported de-serialization type\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Widtch of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        if len(obj.shape) == 0:\n",
    "            length = 0\n",
    "            width = 0\n",
    "        else:\n",
    "            length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported serialization type\")\n",
    "\n",
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCacheendpoint\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        if int(length) == 0:\n",
    "            obj = np.float64(np.fromstring(val))\n",
    "        else:\n",
    "            obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=0)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported serialization type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Launch Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "epoch = 1\n",
    "layer = 1\n",
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simulate TrainerLambda ARN\n",
    "#environ[str('TrainerLambda')] = str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve datasets and setting from S3\n",
    "input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "try:\n",
    "    input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "    input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(\"Error downloading input data from S3, S3 object does not exist\")\n",
    "    else:\n",
    "        raise\n",
    "    \n",
    "# Extract the neural network parameters\n",
    "with open('/tmp/parameters.json') as parameters_file:\n",
    "    parameters = json.load(parameters_file)\n",
    "    \n",
    "# Build in additional parameters from neural network parameters\n",
    "parameters['epoch'] = 1\n",
    "# Next Layer to process\n",
    "parameters['layer'] = 1\n",
    "# Input data sets and data set parameters\n",
    "parameters['data_keys'], parameters['input_data'], parameters['data_dimensions'] = initialize_data(endpoint=endpoint, w=parameters.get('weight'), b=parameters.get('bias'))\n",
    "    \n",
    "# Initialize payload to `TrainerLambda`\n",
    "payload = {}\n",
    "# Initialize the overall state\n",
    "payload['state'] = 'start'\n",
    "# Dump the parameters to ElastiCache\n",
    "payload['parameter_key'] = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "#payload['endpoint'] = endpoint\n",
    "# Prepare the payload for `TrainerLambda`\n",
    "payloadbytes = dumps(payload)\n",
    "    \n",
    "print(\"Complete Neural Network Settings: \\n\")\n",
    "print(dumps(parameters, indent=4, sort_keys=True))\n",
    "print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Trainer -> Neuron Event for Forward Propogation\n",
    "**Simulating Forward Propogation of the Neuron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fake results object created by `TrainerLambda`\n",
    "results = {}\n",
    "results['epoch' + str(epoch)] = {}\n",
    "results_key = to_cache(endpoint=endpoint, obj=results, name='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event = payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global parameter_key\n",
    "parameter_key = event.get('parameter_key')\n",
    "global parameters \n",
    "parameters = from_cache(endpoint, parameter_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = from_cache(endpoint=endpoint, key=parameters['data_keys']['weights'])\n",
    "b = from_cache(endpoint=endpoint, key=parameters['data_keys']['bias'])\n",
    "X = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_x'])\n",
    "Y = from_cache(endpoint=endpoint, key=parameters['data_keys']['train_set_y'])\n",
    "m = from_cache(endpoint=endpoint, key=parameters['data_keys']['m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_1 = sigmoid(np.dot(w.T, X) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First clean up any existing activations in Redis for testing\n",
    "for key in cache.scan_iter(match='a_*'):\n",
    "    cache.delete(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "to_cache(endpoint=endpoint, obj=a_1, name='a_'+str(ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Neuron -> Trainer Event\n",
    "**Simulating processing the Cost from the Activation output/s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this redis command to ensure that a pure string is returned for the key\n",
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cache.scan_iter(match='a_*'):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in r.scan_iter(match='a_*'):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    key_list.append(key)\n",
    "print(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_dict = {}\n",
    "for i in key_list:\n",
    "    A_dict[i] = from_cache(endpoint=endpoint, key=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([arr.tolist() for arr in A_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_cache(endpoint=endpoint, key='a_1|float64#1#209')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = A.reshape(1, 209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A = np.squeeze(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging two activation arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_2 = sigmoid(np.dot(w.T, X) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 2\n",
    "to_cache(endpoint=endpoint, obj=a_2, name='a_'+str(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis(host=endpoint, port=6379, db=0, charset=\"utf-8\", decode_responses=True)\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    key_list.append(key)\n",
    "print(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_dict = {}\n",
    "for i in key_list:\n",
    "    A_dict[i] = from_cache(endpoint=endpoint, key=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([arr.tolist() for arr in A_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A = A.reshape(2, 209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.squeeze(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulate 3 Hidden Unit activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_3 = sigmoid(np.dot(w.T, X) + b)\n",
    "ID = 3\n",
    "to_cache(endpoint=endpoint, obj=a_3, name='a_'+str(ID))\n",
    "key_list = []\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    key_list.append(key)\n",
    "print(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dict = {}\n",
    "for i in key_list:\n",
    "    A_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "A_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "A = np.squeeze(A)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test\n",
    "**Single Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First clean up any existing activations in Redis for testing\n",
    "for key in cache.scan_iter(match='a_*'):\n",
    "    cache.delete(key)\n",
    "\n",
    "# Calculate activation\n",
    "a_1 = sigmoid(np.dot(w.T, X) + b)\n",
    "a_2 = sigmoid(np.dot(w.T, X) + b)\n",
    "a_3 = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "# Push one activation to cache\n",
    "ID = 1\n",
    "to_cache(endpoint=endpoint, obj=a_1, name='a_'+str(ID))\n",
    "\n",
    "# Run the algorithm\n",
    "key_list = []\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    key_list.append(key)\n",
    "# Create a dictionat of numpy arrays\n",
    "A_dict = {}\n",
    "for i in key_list:\n",
    "    A_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "# Create the numpy array of activations, depending on the \n",
    "# number of hidden units\n",
    "no_activations = len(key_list)\n",
    "A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "if no_activations == 1:\n",
    "    dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "    A = A.reshape(int(dims[0]), int(dims[1]))\n",
    "    print(A.shape)\n",
    "else:\n",
    "    A = np.squeeze(A)\n",
    "    print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three Activations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Push other two activations to cache\n",
    "ID = 2\n",
    "to_cache(endpoint=endpoint, obj=a_2, name='a_'+str(ID))\n",
    "ID = 3\n",
    "to_cache(endpoint=endpoint, obj=a_3, name='a_'+str(ID))\n",
    "\n",
    "# Run the algorithm\n",
    "key_list = []\n",
    "for key in r.scan_iter(match='a_*'):\n",
    "    key_list.append(key)\n",
    "# Create a dictionat of numpy arrays\n",
    "A_dict = {}\n",
    "for i in key_list:\n",
    "    A_dict[i] = from_cache(endpoint=endpoint, key=i)\n",
    "# Create the numpy array of activations, depending on the \n",
    "# number of hidden units\n",
    "no_activations = len(key_list)\n",
    "A = np.array([arr.tolist() for arr in A_dict.values()])\n",
    "if no_activations == 1:\n",
    "    dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "    A = A.reshape(int(dims[0]), int(dims[1]))\n",
    "    print(A.shape)\n",
    "else:\n",
    "    A = np.squeeze(A)\n",
    "    print(A.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost on activations\n",
    "cost = (-1 / m) * np.sum(Y * (np.log(A)) + ((1 - Y) * np.log(1 - A)))\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log A as input data to Backprop\n",
    "# Get the latest parameters \n",
    "parameters = from_cache(endpoint, key=parameter_key)\n",
    "\n",
    "# Update `A` to the local dictionary\n",
    "parameters['data_keys']['A'] = to_cache(endpoint=endpoint, obj=A, name='A')\n",
    "\n",
    "# Update paramaters in ElastiCache\n",
    "parameter_key = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "#print(parameter_key) # should be `parameters|json'\n",
    "\n",
    "# Update `results` with the Cost\n",
    "results = from_cache(endpoint=endpoint, key='results|json')\n",
    "results['epoch' + str(epoch)]['cost'] = cost\n",
    "results_key = to_cache(endpoint=endpoint, obj=results, name='results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Trainer -> Neuron Event for Backprop\n",
    "**Simulate the process of Calculating the Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fake configurations on the `TrainerLambda`\n",
    "grads = {}\n",
    "# Keys to the derivative of w and b for layer=1\n",
    "grads['layer' + str(layer)] = {}\n",
    "grads = to_cache(endpoint=endpoint, obj=grads, name='grads')\n",
    "parameters['data_keys']['grads'] = grads\n",
    "# Simulate fake payload additions\n",
    "payload['state'] = 'backward'\n",
    "payload['parameter_key'] = parameter_key\n",
    "payload['results_key'] = results_key\n",
    "payload['id'] = 1\n",
    "payload['layer'] = 1\n",
    "print(\"Complete Neural Network Settings: \\n\")\n",
    "print(dumps(parameters, indent=4, sort_keys=True))\n",
    "print(\"Payload to be sent to NeuLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propogation to determine gradients\n",
    "dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "print(\"Partial Derivatives - Weights for Neuron\" + str(ID) + \":\\n\")\n",
    "print(dw)\n",
    "db = (1 / m) * np.sum(A - Y)\n",
    "print(\"Partial Derivatives - Bias for Neuron\" + str(ID) + \":\\n\")\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are two differnt numpy array type: *ndarray* and *float64*. The `to_cache()` function can serialise a numpy array, but not a *float64*. The `cost` is also *float64* but to get around the limitations of Elasticache, it was added to results. So since the `Bias` and `Weights` are already `data_kays` -> Therefore the `to_cache()` and `from_cache()` will need to differentiate between different numpy types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"db shape: \" + str(db.shape))\n",
    "print(\"db type: \" + str(type(db)))\n",
    "print(\"dn dtrype: \" + str(db.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dw shape: \" + str(dw.shape))\n",
    "print(\"dw type: \" + str(type(dw)))\n",
    "print(\"dw dtype: \" + str(dw.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db_key = to_cache(endpoint, obj=db, name='layer'+str(layer)+'_db')\n",
    "print(test_db_key)\n",
    "test_db = from_cache(endpoint, key=test_db_key)\n",
    "print(\"test_db type: \" + str(type(test_db)))\n",
    "print(\"test_db shape: \" + str(test_db.shape))\n",
    "test_dw_key = to_cache(endpoint, obj=dw, name='layer'+str(layer)+'_dw')\n",
    "print(test_dw_key)\n",
    "test_dw = from_cache(endpoint, key=test_dw_key)\n",
    "print(\"test_dw type: \" + str(type(test_dw)))\n",
    "print(\"test_dw shape: \" + str(test_dw.shape))\n",
    "assert(test_dw.shape==dw.shape)\n",
    "assert(test_db.shape == db.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Upload Data to Cache\n",
    "The `TrainerLambda` must access the derivatives to determine the final `Weights` and `Bias` for the next epoch though *Gradient Descent*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the gradients object created by the `TrainerLambda`\n",
    "grads_key = parameters['data_keys']['grads']\n",
    "# Load the grads object\n",
    "grads = from_cache(endpoint, key=grads_key)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the grads object with the calculated derivatives\n",
    "grads['layer' + str(layer)]['dw_' + str(ID)] = to_cache(endpoint, obj=dw, name='dw_'+str(ID))\n",
    "grads['layer' + str(layer)]['db_' + str(ID)] = to_cache(endpoint, obj=db, name='db_'+str(ID))\n",
    "# Update the pramaters (local)\n",
    "parameters['data_keys']['grads'] = grads\n",
    "# Upload to ElastiCache\n",
    "parameters_key = to_cache(endpoint, obj=parameters, name='parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron -> Trainer - Optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
