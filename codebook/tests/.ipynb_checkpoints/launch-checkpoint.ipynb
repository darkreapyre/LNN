{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `launch.lambda_handler()`\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "from redis import StrictRedis as redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 access\n",
    "s3_resource = resource('s3')\n",
    "redis_client = client('elasticache', region_name='us-west-2')\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump2cache(endpoint, dump, name):\n",
    "    \"\"\"\n",
    "    Serializes a string or JSON to a binary string and stores it in ElastiCache\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint\n",
    "    dump -- JSON dump or string to be stored in ElastiCache\n",
    "    name -- name used as the hash key of the data\n",
    "    \n",
    "    Return:\n",
    "    Hash key for the data\n",
    "    \"\"\"\n",
    "    \n",
    "    key = str(name)\n",
    "    cache = redis(host=endpoint, port=6379, db=0)\n",
    "    val = dump\n",
    "    cache.set(key, val)\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache2numpy(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary string from Elasticache to a Numpy array\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint\n",
    "    key -- name of te hash key of the data to be retrieved\n",
    "    \n",
    "    Return:\n",
    "    Numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get array from Redis\n",
    "    cache = redis(host=endpoint, port=6379, db=0)\n",
    "    data = cache.get(key)\n",
    "    # De-serialize the value\n",
    "    array_dtype, length, width = key.split('|')[1].split('#')\n",
    "    array = np.fromstring(data, dtype=array_dtype).reshape(int(length), int(width))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numpy2cache(endpoint, array, name):\n",
    "    \"\"\"\n",
    "    Seializes a Numpy array to a binary string and stores it in the Redis cache\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint\n",
    "    array -- Numpy array to stored in ElastiCache\n",
    "    name -- name used as the hash key of the data\n",
    "    \n",
    "    Return:\n",
    "    Hash key for the array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get parameters for the key\n",
    "    array_dtype = str(array.dtype)\n",
    "    length, width = array.shape\n",
    "    # Convert the array to string\n",
    "    val = array.ravel().tostring()\n",
    "    # Create a key from the name and necessary parameters from the array\n",
    "    # i.e. {name}|{type}#{length}#{width} \n",
    "    key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "    # Store the binary string to Redis\n",
    "    cache = redis(host=endpoint, port=6379, db=0)\n",
    "    cache.set(key, val)\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, stndrdized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_data(endpoint, w, b):\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes and then dumps the data to ElastiCache \n",
    "    for neurons to process as layer a^0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "    \n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "    \n",
    "    # Reshape labels\n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # Preprocess inputs\n",
    "    train_set_x = standardize(train_set_x_orig)\n",
    "    test_set_x = standardize(test_set_x_orig)\n",
    "\n",
    "    # Dump the inputs to the temporary s3 bucket for TrainerLambda\n",
    "    #bucket = storage_init() # Creates a temporary bucket for the propogation steps\n",
    "    data_keys = {} # Dictionary for the hask keys of the data set\n",
    "    dims = {} # Dictionary of data set dimensions\n",
    "    a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "    a_names = [] # Placeholder for array names\n",
    "    for i in range(len(a_list)):\n",
    "        # Create a lis of the names of the numpy arrays\n",
    "        a_names.append(name2str(a_list[i], locals()))\n",
    "    for j in range(len(a_list)):\n",
    "        # Dump the numpy arrays to ElastiCache\n",
    "        data_keys[str(a_names[j][0])] = numpy2cache(endpoint, array=a_list[j], name=a_names[j][0])\n",
    "        # Append the array dimensions to the list\n",
    "        dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "    # Initialize weights\n",
    "    if w == 0: # Initialize weights to dimensions of the input data\n",
    "        dim = dims.get('train_set_x')[0]\n",
    "        weights = np.zeros((dim, 1))\n",
    "        # Store the initial weights as a column vector on S3\n",
    "        data_keys['weights'] = numpy2cache(endpoint, array=weights, name='weights')\n",
    "    else:\n",
    "        #placeholder for random weight initialization\n",
    "        pass\n",
    "        \n",
    "    # Initialize Bias\n",
    "    if b != 0:\n",
    "        #placeholder for random bias initialization\n",
    "        #data_keys['bias'] = numpy2cache(endpoint, array=bias, name='bias')\n",
    "        pass\n",
    "    else:\n",
    "        data_keys['bias'] = dump2cache(endpoint, dump=str(b), name='bias')\n",
    "        \n",
    "    return data_keys, [j for i in a_names for j in i], dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    # Retrieve datasets and setting from S3\n",
    "    input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "    dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "    settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "    try:\n",
    "        input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "        input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(\"Error downloading input data from S3, S3 object does not exist\")\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    #Retrieve the Elasticache Cluster endpoint\n",
    "    cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "    endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "    \n",
    "    # Extract the neural network parameters\n",
    "    with open('/tmp/parameters.json') as parameters_file:\n",
    "        parameters = json.load(parameters_file)\n",
    "    \n",
    "    # Build in additional parameters from neural network parameters\n",
    "    parameters['epoch'] = 1\n",
    "    # Next Layer to process\n",
    "    parameters['layer'] = 1\n",
    "    # Input data sets and data set parameters\n",
    "    parameters['data_keys'], \\\n",
    "    parameters['input_data'], \\\n",
    "    parameters['data_dimensions'] = initialize_data(\n",
    "        endpoint=endpoint, w=parameters.get('weight'), b = parameters.get('bias')\n",
    "    )\n",
    "    \n",
    "    # Initialize payload to `TrainerLambda`\n",
    "    payload = {}\n",
    "    # Initialize the overall state\n",
    "    payload['state'] = 'start'\n",
    "    # Dump the parameters to ElastiCache\n",
    "    payload['parameter_key'] = dump2cache(endpoint, dump=dumps(parameters), name='parameters')\n",
    "    payload['endpoint'] = endpoint\n",
    "    # Prepare the payload for `TrainerLambda`\n",
    "    payloadbytes = dumps(payload)\n",
    "    \n",
    "    print(\"Complete Neural Network Settings: \\n\")\n",
    "    print(dumps(parameters, indent=4, sort_keys=True))\n",
    "    print(\"\\n\" + \"Payload to be sent to TrainerLambda\")\n",
    "    print(dumps(payload, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Neural Network Settings: \n",
      "\n",
      "{\n",
      "    \"activations\": {\n",
      "        \"layer1\": \"sigmoid\"\n",
      "    },\n",
      "    \"bias\": 0,\n",
      "    \"data_dimensions\": {\n",
      "        \"test_set_x\": [\n",
      "            12288,\n",
      "            50\n",
      "        ],\n",
      "        \"test_set_y\": [\n",
      "            1,\n",
      "            50\n",
      "        ],\n",
      "        \"train_set_x\": [\n",
      "            12288,\n",
      "            209\n",
      "        ],\n",
      "        \"train_set_y\": [\n",
      "            1,\n",
      "            209\n",
      "        ]\n",
      "    },\n",
      "    \"data_keys\": {\n",
      "        \"bias\": \"bias\",\n",
      "        \"test_set_x\": \"test_set_x|float64#12288#50\",\n",
      "        \"test_set_y\": \"test_set_y|int64#1#50\",\n",
      "        \"train_set_x\": \"train_set_x|float64#12288#209\",\n",
      "        \"train_set_y\": \"train_set_y|int64#1#209\",\n",
      "        \"weights\": \"weights|float64#12288#1\"\n",
      "    },\n",
      "    \"epoch\": 1,\n",
      "    \"epochs\": 20,\n",
      "    \"input_data\": [\n",
      "        \"train_set_x\",\n",
      "        \"train_set_y\",\n",
      "        \"test_set_x\",\n",
      "        \"test_set_y\"\n",
      "    ],\n",
      "    \"layer\": 1,\n",
      "    \"layers\": 1,\n",
      "    \"learning_rate\": 0.005,\n",
      "    \"neurons\": {\n",
      "        \"layer1\": 1\n",
      "    },\n",
      "    \"weight\": 0\n",
      "}\n",
      "\n",
      "Payload to be sent to TrainerLambda\n",
      "{\n",
      "    \"endpoint\": \"lnn-re-f5up9o4wv1e6.svpice.0001.usw2.cache.amazonaws.com\",\n",
      "    \"parameter_key\": \"parameters\",\n",
      "    \"state\": \"start\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Fake context\n",
    "context = ''\n",
    "\n",
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::lnn\",\n",
    "                    \"name\": \"lnn\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simulate TrainerLambda ARN\n",
    "#environ[str('TrainerLambda')] = str(None)\n",
    "\n",
    "lambda_handler(event, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"activations\": {\n",
      "        \"layer1\": \"sigmoid\"\n",
      "    },\n",
      "    \"bias\": 0,\n",
      "    \"data_dimensions\": {\n",
      "        \"test_set_x\": [\n",
      "            12288,\n",
      "            50\n",
      "        ],\n",
      "        \"test_set_y\": [\n",
      "            1,\n",
      "            50\n",
      "        ],\n",
      "        \"train_set_x\": [\n",
      "            12288,\n",
      "            209\n",
      "        ],\n",
      "        \"train_set_y\": [\n",
      "            1,\n",
      "            209\n",
      "        ]\n",
      "    },\n",
      "    \"data_keys\": {\n",
      "        \"bias\": \"bias\",\n",
      "        \"test_set_x\": \"test_set_x|float64#12288#50\",\n",
      "        \"test_set_y\": \"test_set_y|int64#1#50\",\n",
      "        \"train_set_x\": \"train_set_x|float64#12288#209\",\n",
      "        \"train_set_y\": \"train_set_y|int64#1#209\",\n",
      "        \"weights\": \"weights|float64#12288#1\"\n",
      "    },\n",
      "    \"epoch\": 1,\n",
      "    \"epochs\": 20,\n",
      "    \"input_data\": [\n",
      "        \"train_set_x\",\n",
      "        \"train_set_y\",\n",
      "        \"test_set_x\",\n",
      "        \"test_set_y\"\n",
      "    ],\n",
      "    \"layer\": 1,\n",
      "    \"layers\": 1,\n",
      "    \"learning_rate\": 0.005,\n",
      "    \"neurons\": {\n",
      "        \"layer1\": 1\n",
      "    },\n",
      "    \"weight\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "\n",
    "\n",
    "# Get weights and bias\n",
    "cache = redis(host=endpoint, port=6379, db=0)\n",
    "data_key = 'parameters'\n",
    "data = cache.get(data_key)\n",
    "parsed = json.loads(data)\n",
    "print(dumps(parsed, indent=4, sort_keys=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
